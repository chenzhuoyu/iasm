// Code generated by "mkasm_aarch64.py", DO NOT EDIT.

package aarch64

import (
    `github.com/chenzhuoyu/iasm/asm`
)

const (
    _N_args = 4
)

// ABS instruction have 2 forms:
//
//   * ABS  <Wd>, <Wn>
//   * ABS  <Xd>, <Xn>
//
func (self *Program) ABS(v0, v1 interface{}) *Instruction {
    p := self.alloc("ABS", 2, Operands { v0, v1 })
    // ABS  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 8, sa_wn, sa_wd))
    }
    // ABS  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 8, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ABS")
}

// ADC instruction have 2 forms:
//
//   * ADC  <Wd>, <Wn>, <Wm>
//   * ADC  <Xd>, <Xn>, <Xm>
//
func (self *Program) ADC(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ADC", 3, Operands { v0, v1, v2 })
    // ADC  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(0, 0, 0, sa_wm, sa_wn, sa_wd))
    }
    // ADC  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(1, 0, 0, sa_xm, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADC")
}

// ADCS instruction have 2 forms:
//
//   * ADCS  <Wd>, <Wn>, <Wm>
//   * ADCS  <Xd>, <Xn>, <Xm>
//
func (self *Program) ADCS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ADCS", 3, Operands { v0, v1, v2 })
    // ADCS  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(0, 0, 1, sa_wm, sa_wn, sa_wd))
    }
    // ADCS  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(1, 0, 1, sa_xm, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADCS")
}

// ADD instruction have 6 forms:
//
//   * ADD  <Wd|WSP>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
//   * ADD  <Wd|WSP>, <Wn|WSP>, #<imm>{, <shift>}
//   * ADD  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ADD  <Xd|SP>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
//   * ADD  <Xd|SP>, <Xn|SP>, #<imm>{, <shift>}
//   * ADD  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) ADD(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ADD", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ADD", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ADD takes 3 or 4 operands")
    }
    // ADD  <Wd|WSP>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
    if len(vv) >= 0 && len(vv) <= 1 && isWrOrWSP(v0) && isWrOrWSP(v1) && isWr(v2) && (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_extend = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_ext(0, 0, 0, 0, sa_wm, sa_extend, sa_amount, sa_wn_wsp, sa_wd_wsp))
    }
    // ADD  <Wd|WSP>, <Wn|WSP>, #<imm>{, <shift>}
    if len(vv) >= 0 &&
       len(vv) <= 1 &&
       isWrOrWSP(v0) &&
       isWrOrWSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(0, 0, 0, sa_shift, sa_imm, sa_wn_wsp, sa_wd_wsp))
    }
    // ADD  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(0, 0, 0, sa_shift, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ADD  <Xd|SP>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
    if len(vv) >= 0 &&
       len(vv) <= 1 &&
       isXrOrSP(v0) &&
       isXrOrSP(v1) &&
       isWrOrXr(v2) &&
       (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend_1 uint32
        var sa_r [4]uint32
        var sa_r__bit_mask [4]uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        switch true {
            case isWr(v2): sa_r = [4]uint32{0b000, 0b010, 0b100, 0b110}
            case isXr(v2): sa_r = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        switch true {
            case isWr(v2): sa_r__bit_mask = [4]uint32{0b110, 0b111, 0b110, 0b111}
            case isXr(v2): sa_r__bit_mask = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        if len(vv) > 0 {
            sa_extend_1 = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        if !matchany(sa_extend_1, &sa_r__bit_mask[0], &sa_r[0], 4) {
            panic("aarch64: invalid combination of operands for ADD")
        }
        return p.setins(addsub_ext(1, 0, 0, 0, sa_m, sa_extend_1, sa_amount, sa_xn_sp, sa_xd_sp))
    }
    // ADD  <Xd|SP>, <Xn|SP>, #<imm>{, <shift>}
    if len(vv) >= 0 &&
       len(vv) <= 1 &&
       isXrOrSP(v0) &&
       isXrOrSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(1, 0, 0, sa_shift, sa_imm, sa_xn_sp, sa_xd_sp))
    }
    // ADD  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(1, 0, 0, sa_shift, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADD")
}

// ADDG instruction have one single form:
//
//   * ADDG  <Xd|SP>, <Xn|SP>, #<uimm6>, #<uimm4>
//
func (self *Program) ADDG(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("ADDG", 4, Operands { v0, v1, v2, v3 })
    if isXrOrSP(v0) && isXrOrSP(v1) && isUimm6(v2) && isUimm4(v3) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_uimm6 := asUimm6(v2)
        sa_uimm4 := asUimm4(v3)
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd_sp
        return p.setins(addsub_immtags(1, 0, 0, sa_uimm6, 0, sa_uimm4, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ADDG")
}

// ADDS instruction have 6 forms:
//
//   * ADDS  <Wd>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
//   * ADDS  <Wd>, <Wn|WSP>, #<imm>{, <shift>}
//   * ADDS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ADDS  <Xd>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
//   * ADDS  <Xd>, <Xn|SP>, #<imm>{, <shift>}
//   * ADDS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) ADDS(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ADDS", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ADDS", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ADDS takes 3 or 4 operands")
    }
    // ADDS  <Wd>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
    if len(vv) >= 0 && len(vv) <= 1 && isWr(v0) && isWrOrWSP(v1) && isWr(v2) && (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_extend = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_ext(0, 0, 1, 0, sa_wm, sa_extend, sa_amount, sa_wn_wsp, sa_wd))
    }
    // ADDS  <Wd>, <Wn|WSP>, #<imm>{, <shift>}
    if len(vv) >= 0 &&
       len(vv) <= 1 &&
       isWr(v0) &&
       isWrOrWSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(0, 0, 1, sa_shift, sa_imm, sa_wn_wsp, sa_wd))
    }
    // ADDS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(0, 0, 1, sa_shift, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ADDS  <Xd>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
    if len(vv) >= 0 && len(vv) <= 1 && isXr(v0) && isXrOrSP(v1) && isWrOrXr(v2) && (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend_1 uint32
        var sa_r [4]uint32
        var sa_r__bit_mask [4]uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        switch true {
            case isWr(v2): sa_r = [4]uint32{0b000, 0b010, 0b100, 0b110}
            case isXr(v2): sa_r = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        switch true {
            case isWr(v2): sa_r__bit_mask = [4]uint32{0b110, 0b111, 0b110, 0b111}
            case isXr(v2): sa_r__bit_mask = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        if len(vv) > 0 {
            sa_extend_1 = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        if !matchany(sa_extend_1, &sa_r__bit_mask[0], &sa_r[0], 4) {
            panic("aarch64: invalid combination of operands for ADDS")
        }
        return p.setins(addsub_ext(1, 0, 1, 0, sa_m, sa_extend_1, sa_amount, sa_xn_sp, sa_xd))
    }
    // ADDS  <Xd>, <Xn|SP>, #<imm>{, <shift>}
    if len(vv) >= 0 &&
       len(vv) <= 1 &&
       isXr(v0) &&
       isXrOrSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(1, 0, 1, sa_shift, sa_imm, sa_xn_sp, sa_xd))
    }
    // ADDS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(1, 0, 1, sa_shift, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADDS")
}

// ADR instruction have one single form:
//
//   * ADR  <Xd>, <label>
//
func (self *Program) ADR(v0, v1 interface{}) *Instruction {
    p := self.alloc("ADR", 2, Operands { v0, v1 })
    if isXr(v0) && isLabel(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 {
            delta := uint32(sa_label.RelativeTo(pc))
            return pcreladdr(0, delta & 0x3, (delta >> 2) & 0x7ffff, sa_xd)
        })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ADR")
}

// ADRP instruction have one single form:
//
//   * ADRP  <Xd>, <label>
//
func (self *Program) ADRP(v0, v1 interface{}) *Instruction {
    p := self.alloc("ADRP", 2, Operands { v0, v1 })
    if isXr(v0) && isLabel(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 {
            delta := uint32(sa_label.RelativeTo(pc))
            return pcreladdr(1, delta & 0x3, (delta >> 2) & 0x7ffff, sa_xd)
        })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ADRP")
}

// AESD instruction have one single form:
//
//   * AESD  <Vd>.16B, <Vn>.16B
//
func (self *Program) AESD(v0, v1 interface{}) *Instruction {
    p := self.alloc("AESD", 2, Operands { v0, v1 })
    if isVr(v0) && vfmt(v0) == Vec16B && isVr(v1) && vfmt(v1) == Vec16B {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(cryptoaes(0, 5, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AESD")
}

// AND instruction have 5 forms:
//
//   * AND  <Wd|WSP>, <Wn>, #<imm>
//   * AND  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * AND  <Xd|SP>, <Xn>, #<imm>
//   * AND  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//   * AND  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) AND(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("AND", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("AND", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction AND takes 3 or 4 operands")
    }
    // AND  <Wd|WSP>, <Wn>, #<imm>
    if isWrOrWSP(v0) && isWr(v1) && isMask32(v2) {
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_imm := asMaskOp(v2)
        return p.setins(log_imm(0, 0, 0, (sa_imm >> 6) & 0x3f, sa_imm & 0x3f, sa_wn, sa_wd_wsp))
    }
    // AND  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 0, sa_shift, 0, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // AND  <Xd|SP>, <Xn>, #<imm>
    if isXrOrSP(v0) && isXr(v1) && isMask64(v2) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_imm_1 := asMaskOp(v2)
        return p.setins(log_imm(1, 0, (sa_imm_1 >> 12) & 0x1, (sa_imm_1 >> 6) & 0x3f, sa_imm_1 & 0x3f, sa_xn, sa_xd_sp))
    }
    // AND  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 0, sa_shift, 0, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // AND  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       isSameSize(v0, v1) &&
       isSameSize(v1, v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t, 0, 0, sa_vm, 3, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for AND")
}

// ANDS instruction have 4 forms:
//
//   * ANDS  <Wd>, <Wn>, #<imm>
//   * ANDS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ANDS  <Xd>, <Xn>, #<imm>
//   * ANDS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) ANDS(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ANDS", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ANDS", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ANDS takes 3 or 4 operands")
    }
    // ANDS  <Wd>, <Wn>, #<imm>
    if isWr(v0) && isWr(v1) && isMask32(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_imm := asMaskOp(v2)
        return p.setins(log_imm(0, 3, 0, (sa_imm >> 6) & 0x3f, sa_imm & 0x3f, sa_wn, sa_wd))
    }
    // ANDS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 3, sa_shift, 0, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ANDS  <Xd>, <Xn>, #<imm>
    if isXr(v0) && isXr(v1) && isMask64(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_imm_1 := asMaskOp(v2)
        return p.setins(log_imm(1, 3, (sa_imm_1 >> 12) & 0x1, (sa_imm_1 >> 6) & 0x3f, sa_imm_1 & 0x3f, sa_xn, sa_xd))
    }
    // ANDS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 3, sa_shift, 0, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ANDS")
}

// ASRV instruction have 2 forms:
//
//   * ASRV  <Wd>, <Wn>, <Wm>
//   * ASRV  <Xd>, <Xn>, <Xm>
//
func (self *Program) ASRV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ASRV", 3, Operands { v0, v1, v2 })
    // ASRV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 10, sa_wn, sa_wd))
    }
    // ASRV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 10, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ASRV")
}

// AUTDA instruction have one single form:
//
//   * AUTDA  <Xd>, <Xn|SP>
//
func (self *Program) AUTDA(v0, v1 interface{}) *Instruction {
    p := self.alloc("AUTDA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 6, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTDA")
}

// AUTDB instruction have one single form:
//
//   * AUTDB  <Xd>, <Xn|SP>
//
func (self *Program) AUTDB(v0, v1 interface{}) *Instruction {
    p := self.alloc("AUTDB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 7, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTDB")
}

// AUTDZA instruction have one single form:
//
//   * AUTDZA  <Xd>
//
func (self *Program) AUTDZA(v0 interface{}) *Instruction {
    p := self.alloc("AUTDZA", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 14, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTDZA")
}

// AUTDZB instruction have one single form:
//
//   * AUTDZB  <Xd>
//
func (self *Program) AUTDZB(v0 interface{}) *Instruction {
    p := self.alloc("AUTDZB", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 15, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTDZB")
}

// AUTIA instruction have one single form:
//
//   * AUTIA  <Xd>, <Xn|SP>
//
func (self *Program) AUTIA(v0, v1 interface{}) *Instruction {
    p := self.alloc("AUTIA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 4, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTIA")
}

// AUTIA1716 instruction have one single form:
//
//   * AUTIA1716
//
func (self *Program) AUTIA1716() *Instruction {
    p := self.alloc("AUTIA1716", 0, Operands {})
    return p.setins(hints(1, 4))
}

// AUTIASP instruction have one single form:
//
//   * AUTIASP
//
func (self *Program) AUTIASP() *Instruction {
    p := self.alloc("AUTIASP", 0, Operands {})
    return p.setins(hints(3, 5))
}

// AUTIAZ instruction have one single form:
//
//   * AUTIAZ
//
func (self *Program) AUTIAZ() *Instruction {
    p := self.alloc("AUTIAZ", 0, Operands {})
    return p.setins(hints(3, 4))
}

// AUTIB instruction have one single form:
//
//   * AUTIB  <Xd>, <Xn|SP>
//
func (self *Program) AUTIB(v0, v1 interface{}) *Instruction {
    p := self.alloc("AUTIB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 5, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTIB")
}

// AUTIB1716 instruction have one single form:
//
//   * AUTIB1716
//
func (self *Program) AUTIB1716() *Instruction {
    p := self.alloc("AUTIB1716", 0, Operands {})
    return p.setins(hints(1, 6))
}

// AUTIBSP instruction have one single form:
//
//   * AUTIBSP
//
func (self *Program) AUTIBSP() *Instruction {
    p := self.alloc("AUTIBSP", 0, Operands {})
    return p.setins(hints(3, 7))
}

// AUTIBZ instruction have one single form:
//
//   * AUTIBZ
//
func (self *Program) AUTIBZ() *Instruction {
    p := self.alloc("AUTIBZ", 0, Operands {})
    return p.setins(hints(3, 6))
}

// AUTIZA instruction have one single form:
//
//   * AUTIZA  <Xd>
//
func (self *Program) AUTIZA(v0 interface{}) *Instruction {
    p := self.alloc("AUTIZA", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 12, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTIZA")
}

// AUTIZB instruction have one single form:
//
//   * AUTIZB  <Xd>
//
func (self *Program) AUTIZB(v0 interface{}) *Instruction {
    p := self.alloc("AUTIZB", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 13, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTIZB")
}

// AXFLAG instruction have one single form:
//
//   * AXFLAG
//
func (self *Program) AXFLAG() *Instruction {
    p := self.alloc("AXFLAG", 0, Operands {})
    return p.setins(pstate(0, 0, 2, 31))
}

// B instruction have one single form:
//
//   * B  <label>
//
func (self *Program) B(v0 interface{}) *Instruction {
    p := self.alloc("B", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return branch_imm(0, uint32(sa_label.RelativeTo(pc))) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for B")
}

// BFCVT instruction have one single form:
//
//   * BFCVT  <Hd>, <Sn>
//
func (self *Program) BFCVT(v0, v1 interface{}) *Instruction {
    p := self.alloc("BFCVT", 2, Operands { v0, v1 })
    if isHr(v0) && isSr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 6, sa_sn, sa_hd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BFCVT")
}

// BFM instruction have 2 forms:
//
//   * BFM  <Wd>, <Wn>, #<immr>, #<imms>
//   * BFM  <Xd>, <Xn>, #<immr>, #<imms>
//
func (self *Program) BFM(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("BFM", 4, Operands { v0, v1, v2, v3 })
    // BFM  <Wd>, <Wn>, #<immr>, #<imms>
    if isWr(v0) && isWr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_immr := asUimm6(v2)
        sa_imms := asUimm6(v3)
        return p.setins(bitfield(0, 1, 0, sa_immr, sa_imms, sa_wn, sa_wd))
    }
    // BFM  <Xd>, <Xn>, #<immr>, #<imms>
    if isXr(v0) && isXr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_immr_1 := asUimm6(v2)
        sa_imms_1 := asUimm6(v3)
        return p.setins(bitfield(1, 1, 1, sa_immr_1, sa_imms_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for BFM")
}

// BIC instruction have 2 forms:
//
//   * BIC  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * BIC  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) BIC(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("BIC", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("BIC", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction BIC takes 3 or 4 operands")
    }
    // BIC  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 0, sa_shift, 1, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // BIC  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 0, sa_shift, 1, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for BIC")
}

// BICS instruction have 2 forms:
//
//   * BICS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * BICS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) BICS(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("BICS", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("BICS", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction BICS takes 3 or 4 operands")
    }
    // BICS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 3, sa_shift, 1, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // BICS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 3, sa_shift, 1, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for BICS")
}

// BL instruction have one single form:
//
//   * BL  <label>
//
func (self *Program) BL(v0 interface{}) *Instruction {
    p := self.alloc("BL", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return branch_imm(1, uint32(sa_label.RelativeTo(pc))) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BL")
}

// BLR instruction have one single form:
//
//   * BLR  <Xn>
//
func (self *Program) BLR(v0 interface{}) *Instruction {
    p := self.alloc("BLR", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(1, 31, 0, sa_xn, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLR")
}

// BLRAA instruction have one single form:
//
//   * BLRAA  <Xn>, <Xm|SP>
//
func (self *Program) BLRAA(v0, v1 interface{}) *Instruction {
    p := self.alloc("BLRAA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm_sp := uint32(v1.(asm.Register).ID())
        op4 := uint32(0b00000)
        op4 |= sa_xm_sp
        return p.setins(branch_reg(9, 31, 2, sa_xn, op4))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLRAA")
}

// BLRAAZ instruction have one single form:
//
//   * BLRAAZ  <Xn>
//
func (self *Program) BLRAAZ(v0 interface{}) *Instruction {
    p := self.alloc("BLRAAZ", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(1, 31, 2, sa_xn, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLRAAZ")
}

// BLRAB instruction have one single form:
//
//   * BLRAB  <Xn>, <Xm|SP>
//
func (self *Program) BLRAB(v0, v1 interface{}) *Instruction {
    p := self.alloc("BLRAB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm_sp := uint32(v1.(asm.Register).ID())
        op4 := uint32(0b00000)
        op4 |= sa_xm_sp
        return p.setins(branch_reg(9, 31, 3, sa_xn, op4))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLRAB")
}

// BLRABZ instruction have one single form:
//
//   * BLRABZ  <Xn>
//
func (self *Program) BLRABZ(v0 interface{}) *Instruction {
    p := self.alloc("BLRABZ", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(1, 31, 3, sa_xn, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLRABZ")
}

// BR instruction have one single form:
//
//   * BR  <Xn>
//
func (self *Program) BR(v0 interface{}) *Instruction {
    p := self.alloc("BR", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(0, 31, 0, sa_xn, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BR")
}

// BRAA instruction have one single form:
//
//   * BRAA  <Xn>, <Xm|SP>
//
func (self *Program) BRAA(v0, v1 interface{}) *Instruction {
    p := self.alloc("BRAA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm_sp := uint32(v1.(asm.Register).ID())
        op4 := uint32(0b00000)
        op4 |= sa_xm_sp
        return p.setins(branch_reg(8, 31, 2, sa_xn, op4))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRAA")
}

// BRAAZ instruction have one single form:
//
//   * BRAAZ  <Xn>
//
func (self *Program) BRAAZ(v0 interface{}) *Instruction {
    p := self.alloc("BRAAZ", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(0, 31, 2, sa_xn, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRAAZ")
}

// BRAB instruction have one single form:
//
//   * BRAB  <Xn>, <Xm|SP>
//
func (self *Program) BRAB(v0, v1 interface{}) *Instruction {
    p := self.alloc("BRAB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm_sp := uint32(v1.(asm.Register).ID())
        op4 := uint32(0b00000)
        op4 |= sa_xm_sp
        return p.setins(branch_reg(8, 31, 3, sa_xn, op4))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRAB")
}

// BRABZ instruction have one single form:
//
//   * BRABZ  <Xn>
//
func (self *Program) BRABZ(v0 interface{}) *Instruction {
    p := self.alloc("BRABZ", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(0, 31, 3, sa_xn, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRABZ")
}

// BRK instruction have one single form:
//
//   * BRK  #<imm>
//
func (self *Program) BRK(v0 interface{}) *Instruction {
    p := self.alloc("BRK", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(1, sa_imm, 0, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRK")
}

// BTI instruction have one single form:
//
//   * BTI  {<targets>}
//
func (self *Program) BTI(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("BTI", 0, Operands {})
        case 1  : p = self.alloc("BTI", 1, Operands { vv[0] })
        default : panic("instruction BTI takes 0 or 1 operands")
    }
    if len(vv) >= 0 && len(vv) <= 1 && (len(vv) == 0 || isTargets(vv[0])) {
        sa_targets := _BrOmitted
        if len(vv) > 0 {
            sa_targets = vv[0].(BranchTarget)
        }
        op2 := uint32(0b000)
        switch sa_targets {
            case _BrOmitted: op2 |= 0b00 << 1
            case BrC: op2 |= 0b01 << 1
            case BrJ: op2 |= 0b10 << 1
            case BrJC: op2 |= 0b11 << 1
            default: panic("aarch64: invalid combination of operands for BTI")
        }
        return p.setins(hints(4, op2))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BTI")
}

// CAS instruction have 2 forms:
//
//   * CAS  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * CAS  <Xs>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) CAS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CAS", 3, Operands { v0, v1, v2 })
    // CAS  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(2, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    // CAS  <Xs>, <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(3, 0, sa_xs, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CAS")
}

// CASA instruction have 2 forms:
//
//   * CASA  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * CASA  <Xs>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASA", 3, Operands { v0, v1, v2 })
    // CASA  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(2, 1, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    // CASA  <Xs>, <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(3, 1, sa_xs, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASA")
}

// CASAB instruction have one single form:
//
//   * CASAB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(0, 1, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASAB")
}

// CASAH instruction have one single form:
//
//   * CASAH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(1, 1, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASAH")
}

// CASAL instruction have 2 forms:
//
//   * CASAL  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * CASAL  <Xs>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASAL", 3, Operands { v0, v1, v2 })
    // CASAL  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(2, 1, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    // CASAL  <Xs>, <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(3, 1, sa_xs, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASAL")
}

// CASALB instruction have one single form:
//
//   * CASALB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(0, 1, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASALB")
}

// CASALH instruction have one single form:
//
//   * CASALH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(1, 1, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASALH")
}

// CASB instruction have one single form:
//
//   * CASB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(0, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASB")
}

// CASH instruction have one single form:
//
//   * CASH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(1, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASH")
}

// CASL instruction have 2 forms:
//
//   * CASL  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * CASL  <Xs>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASL", 3, Operands { v0, v1, v2 })
    // CASL  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(2, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    // CASL  <Xs>, <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(3, 0, sa_xs, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASL")
}

// CASLB instruction have one single form:
//
//   * CASLB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(0, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASLB")
}

// CASLH instruction have one single form:
//
//   * CASLH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(1, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASLH")
}

// CBNZ instruction have 2 forms:
//
//   * CBNZ  <Wt>, <label>
//   * CBNZ  <Xt>, <label>
//
func (self *Program) CBNZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("CBNZ", 2, Operands { v0, v1 })
    // CBNZ  <Wt>, <label>
    if isWr(v0) && isLabel(v1) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return compbranch(0, 1, uint32(sa_label.RelativeTo(pc)), sa_wt) })
    }
    // CBNZ  <Xt>, <label>
    if isXr(v0) && isLabel(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return compbranch(1, 1, uint32(sa_label.RelativeTo(pc)), sa_xt) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CBNZ")
}

// CBZ instruction have 2 forms:
//
//   * CBZ  <Wt>, <label>
//   * CBZ  <Xt>, <label>
//
func (self *Program) CBZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("CBZ", 2, Operands { v0, v1 })
    // CBZ  <Wt>, <label>
    if isWr(v0) && isLabel(v1) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return compbranch(0, 0, uint32(sa_label.RelativeTo(pc)), sa_wt) })
    }
    // CBZ  <Xt>, <label>
    if isXr(v0) && isLabel(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return compbranch(1, 0, uint32(sa_label.RelativeTo(pc)), sa_xt) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CBZ")
}

// CCMN instruction have 4 forms:
//
//   * CCMN  <Wn>, #<imm>, #<nzcv>, <cond>
//   * CCMN  <Wn>, <Wm>, #<nzcv>, <cond>
//   * CCMN  <Xn>, #<imm>, #<nzcv>, <cond>
//   * CCMN  <Xn>, <Xm>, #<nzcv>, <cond>
//
func (self *Program) CCMN(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CCMN", 4, Operands { v0, v1, v2, v3 })
    // CCMN  <Wn>, #<imm>, #<nzcv>, <cond>
    if isWr(v0) && isUimm5(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_wn := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm5(v1)
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_imm(0, 0, 1, sa_imm, sa_cond, 0, sa_wn, 0, sa_nzcv))
    }
    // CCMN  <Wn>, <Wm>, #<nzcv>, <cond>
    if isWr(v0) && isWr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_wn := uint32(v0.(asm.Register).ID())
        sa_wm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_reg(0, 0, 1, sa_wm, sa_cond, 0, sa_wn, 0, sa_nzcv))
    }
    // CCMN  <Xn>, #<imm>, #<nzcv>, <cond>
    if isXr(v0) && isUimm5(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm5(v1)
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_imm(1, 0, 1, sa_imm, sa_cond, 0, sa_xn, 0, sa_nzcv))
    }
    // CCMN  <Xn>, <Xm>, #<nzcv>, <cond>
    if isXr(v0) && isXr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_reg(1, 0, 1, sa_xm, sa_cond, 0, sa_xn, 0, sa_nzcv))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CCMN")
}

// CCMP instruction have 4 forms:
//
//   * CCMP  <Wn>, #<imm>, #<nzcv>, <cond>
//   * CCMP  <Wn>, <Wm>, #<nzcv>, <cond>
//   * CCMP  <Xn>, #<imm>, #<nzcv>, <cond>
//   * CCMP  <Xn>, <Xm>, #<nzcv>, <cond>
//
func (self *Program) CCMP(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CCMP", 4, Operands { v0, v1, v2, v3 })
    // CCMP  <Wn>, #<imm>, #<nzcv>, <cond>
    if isWr(v0) && isUimm5(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_wn := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm5(v1)
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_imm(0, 1, 1, sa_imm, sa_cond, 0, sa_wn, 0, sa_nzcv))
    }
    // CCMP  <Wn>, <Wm>, #<nzcv>, <cond>
    if isWr(v0) && isWr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_wn := uint32(v0.(asm.Register).ID())
        sa_wm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_reg(0, 1, 1, sa_wm, sa_cond, 0, sa_wn, 0, sa_nzcv))
    }
    // CCMP  <Xn>, #<imm>, #<nzcv>, <cond>
    if isXr(v0) && isUimm5(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm5(v1)
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_imm(1, 1, 1, sa_imm, sa_cond, 0, sa_xn, 0, sa_nzcv))
    }
    // CCMP  <Xn>, <Xm>, #<nzcv>, <cond>
    if isXr(v0) && isXr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_reg(1, 1, 1, sa_xm, sa_cond, 0, sa_xn, 0, sa_nzcv))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CCMP")
}

// CFINV instruction have one single form:
//
//   * CFINV
//
func (self *Program) CFINV() *Instruction {
    p := self.alloc("CFINV", 0, Operands {})
    return p.setins(pstate(0, 0, 0, 31))
}

// CHKFEAT instruction have one single form:
//
//   * CHKFEAT
//
func (self *Program) CHKFEAT() *Instruction {
    p := self.alloc("CHKFEAT", 0, Operands {})
    return p.setins(hints(5, 0))
}

// CLRBHB instruction have one single form:
//
//   * CLRBHB
//
func (self *Program) CLRBHB() *Instruction {
    p := self.alloc("CLRBHB", 0, Operands {})
    return p.setins(hints(2, 6))
}

// CLREX instruction have one single form:
//
//   * CLREX  {#<imm>}
//
func (self *Program) CLREX(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("CLREX", 0, Operands {})
        case 1  : p = self.alloc("CLREX", 1, Operands { vv[0] })
        default : panic("instruction CLREX takes 0 or 1 operands")
    }
    if len(vv) >= 0 && len(vv) <= 1 && (len(vv) == 0 || isUimm4(vv[0])) {
        var sa_imm uint32
        if len(vv) > 0 {
            sa_imm = asUimm4(vv[0])
        }
        return p.setins(barriers(sa_imm, 2, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CLREX")
}

// CLS instruction have 2 forms:
//
//   * CLS  <Wd>, <Wn>
//   * CLS  <Xd>, <Xn>
//
func (self *Program) CLS(v0, v1 interface{}) *Instruction {
    p := self.alloc("CLS", 2, Operands { v0, v1 })
    // CLS  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 5, sa_wn, sa_wd))
    }
    // CLS  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 5, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CLS")
}

// CLZ instruction have 2 forms:
//
//   * CLZ  <Wd>, <Wn>
//   * CLZ  <Xd>, <Xn>
//
func (self *Program) CLZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("CLZ", 2, Operands { v0, v1 })
    // CLZ  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 4, sa_wn, sa_wd))
    }
    // CLZ  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 4, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CLZ")
}

// CNT instruction have 2 forms:
//
//   * CNT  <Wd>, <Wn>
//   * CNT  <Xd>, <Xn>
//
func (self *Program) CNT(v0, v1 interface{}) *Instruction {
    p := self.alloc("CNT", 2, Operands { v0, v1 })
    // CNT  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 7, sa_wn, sa_wd))
    }
    // CNT  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 7, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CNT")
}

// CPYE instruction have one single form:
//
//   * CPYE  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYE", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 0, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYE")
}

// CPYEN instruction have one single form:
//
//   * CPYEN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 12, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEN")
}

// CPYERN instruction have one single form:
//
//   * CPYERN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 8, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERN")
}

// CPYERT instruction have one single form:
//
//   * CPYERT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 2, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERT")
}

// CPYERTN instruction have one single form:
//
//   * CPYERTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 14, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERTN")
}

// CPYERTRN instruction have one single form:
//
//   * CPYERTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 10, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERTRN")
}

// CPYERTWN instruction have one single form:
//
//   * CPYERTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 6, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERTWN")
}

// CPYET instruction have one single form:
//
//   * CPYET  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYET", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 3, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYET")
}

// CPYETN instruction have one single form:
//
//   * CPYETN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYETN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYETN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 15, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYETN")
}

// CPYETRN instruction have one single form:
//
//   * CPYETRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYETRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYETRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 11, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYETRN")
}

// CPYETWN instruction have one single form:
//
//   * CPYETWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYETWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYETWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 7, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYETWN")
}

// CPYEWN instruction have one single form:
//
//   * CPYEWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 4, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWN")
}

// CPYEWT instruction have one single form:
//
//   * CPYEWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 1, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWT")
}

// CPYEWTN instruction have one single form:
//
//   * CPYEWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 13, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWTN")
}

// CPYEWTRN instruction have one single form:
//
//   * CPYEWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 9, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWTRN")
}

// CPYEWTWN instruction have one single form:
//
//   * CPYEWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 5, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWTWN")
}

// CPYFE instruction have one single form:
//
//   * CPYFE  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFE", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 0, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFE")
}

// CPYFEN instruction have one single form:
//
//   * CPYFEN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 12, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEN")
}

// CPYFERN instruction have one single form:
//
//   * CPYFERN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 8, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERN")
}

// CPYFERT instruction have one single form:
//
//   * CPYFERT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 2, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERT")
}

// CPYFERTN instruction have one single form:
//
//   * CPYFERTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 14, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERTN")
}

// CPYFERTRN instruction have one single form:
//
//   * CPYFERTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 10, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERTRN")
}

// CPYFERTWN instruction have one single form:
//
//   * CPYFERTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 6, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERTWN")
}

// CPYFET instruction have one single form:
//
//   * CPYFET  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFET", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 3, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFET")
}

// CPYFETN instruction have one single form:
//
//   * CPYFETN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFETN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFETN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 15, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFETN")
}

// CPYFETRN instruction have one single form:
//
//   * CPYFETRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFETRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFETRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 11, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFETRN")
}

// CPYFETWN instruction have one single form:
//
//   * CPYFETWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFETWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFETWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 7, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFETWN")
}

// CPYFEWN instruction have one single form:
//
//   * CPYFEWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 4, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWN")
}

// CPYFEWT instruction have one single form:
//
//   * CPYFEWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 1, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWT")
}

// CPYFEWTN instruction have one single form:
//
//   * CPYFEWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 13, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWTN")
}

// CPYFEWTRN instruction have one single form:
//
//   * CPYFEWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 9, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWTRN")
}

// CPYFEWTWN instruction have one single form:
//
//   * CPYFEWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 5, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWTWN")
}

// CPYFM instruction have one single form:
//
//   * CPYFM  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFM", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 0, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFM")
}

// CPYFMN instruction have one single form:
//
//   * CPYFMN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 12, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMN")
}

// CPYFMRN instruction have one single form:
//
//   * CPYFMRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 8, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRN")
}

// CPYFMRT instruction have one single form:
//
//   * CPYFMRT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 2, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRT")
}

// CPYFMRTN instruction have one single form:
//
//   * CPYFMRTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 14, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRTN")
}

// CPYFMRTRN instruction have one single form:
//
//   * CPYFMRTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 10, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRTRN")
}

// CPYFMRTWN instruction have one single form:
//
//   * CPYFMRTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 6, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRTWN")
}

// CPYFMT instruction have one single form:
//
//   * CPYFMT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 3, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMT")
}

// CPYFMTN instruction have one single form:
//
//   * CPYFMTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 15, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMTN")
}

// CPYFMTRN instruction have one single form:
//
//   * CPYFMTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 11, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMTRN")
}

// CPYFMTWN instruction have one single form:
//
//   * CPYFMTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 7, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMTWN")
}

// CPYFMWN instruction have one single form:
//
//   * CPYFMWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 4, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWN")
}

// CPYFMWT instruction have one single form:
//
//   * CPYFMWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 1, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWT")
}

// CPYFMWTN instruction have one single form:
//
//   * CPYFMWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 13, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWTN")
}

// CPYFMWTRN instruction have one single form:
//
//   * CPYFMWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 9, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWTRN")
}

// CPYFMWTWN instruction have one single form:
//
//   * CPYFMWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 5, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWTWN")
}

// CPYFP instruction have one single form:
//
//   * CPYFP  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFP", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 0, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFP")
}

// CPYFPN instruction have one single form:
//
//   * CPYFPN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 12, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPN")
}

// CPYFPRN instruction have one single form:
//
//   * CPYFPRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 8, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRN")
}

// CPYFPRT instruction have one single form:
//
//   * CPYFPRT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 2, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRT")
}

// CPYFPRTN instruction have one single form:
//
//   * CPYFPRTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 14, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRTN")
}

// CPYFPRTRN instruction have one single form:
//
//   * CPYFPRTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 10, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRTRN")
}

// CPYFPRTWN instruction have one single form:
//
//   * CPYFPRTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 6, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRTWN")
}

// CPYFPT instruction have one single form:
//
//   * CPYFPT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 3, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPT")
}

// CPYFPTN instruction have one single form:
//
//   * CPYFPTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 15, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPTN")
}

// CPYFPTRN instruction have one single form:
//
//   * CPYFPTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 11, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPTRN")
}

// CPYFPTWN instruction have one single form:
//
//   * CPYFPTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 7, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPTWN")
}

// CPYFPWN instruction have one single form:
//
//   * CPYFPWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 4, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWN")
}

// CPYFPWT instruction have one single form:
//
//   * CPYFPWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 1, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWT")
}

// CPYFPWTN instruction have one single form:
//
//   * CPYFPWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 13, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWTN")
}

// CPYFPWTRN instruction have one single form:
//
//   * CPYFPWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 9, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWTRN")
}

// CPYFPWTWN instruction have one single form:
//
//   * CPYFPWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 5, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWTWN")
}

// CPYM instruction have one single form:
//
//   * CPYM  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYM", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 0, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYM")
}

// CPYMN instruction have one single form:
//
//   * CPYMN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 12, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMN")
}

// CPYMRN instruction have one single form:
//
//   * CPYMRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 8, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRN")
}

// CPYMRT instruction have one single form:
//
//   * CPYMRT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 2, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRT")
}

// CPYMRTN instruction have one single form:
//
//   * CPYMRTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 14, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRTN")
}

// CPYMRTRN instruction have one single form:
//
//   * CPYMRTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 10, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRTRN")
}

// CPYMRTWN instruction have one single form:
//
//   * CPYMRTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 6, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRTWN")
}

// CPYMT instruction have one single form:
//
//   * CPYMT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 3, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMT")
}

// CPYMTN instruction have one single form:
//
//   * CPYMTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 15, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMTN")
}

// CPYMTRN instruction have one single form:
//
//   * CPYMTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 11, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMTRN")
}

// CPYMTWN instruction have one single form:
//
//   * CPYMTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 7, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMTWN")
}

// CPYMWN instruction have one single form:
//
//   * CPYMWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 4, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWN")
}

// CPYMWT instruction have one single form:
//
//   * CPYMWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 1, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWT")
}

// CPYMWTN instruction have one single form:
//
//   * CPYMWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 13, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWTN")
}

// CPYMWTRN instruction have one single form:
//
//   * CPYMWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 9, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWTRN")
}

// CPYMWTWN instruction have one single form:
//
//   * CPYMWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 5, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWTWN")
}

// CPYP instruction have one single form:
//
//   * CPYP  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYP", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 0, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYP")
}

// CPYPN instruction have one single form:
//
//   * CPYPN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 12, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPN")
}

// CPYPRN instruction have one single form:
//
//   * CPYPRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 8, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRN")
}

// CPYPRT instruction have one single form:
//
//   * CPYPRT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 2, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRT")
}

// CPYPRTN instruction have one single form:
//
//   * CPYPRTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 14, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRTN")
}

// CPYPRTRN instruction have one single form:
//
//   * CPYPRTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 10, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRTRN")
}

// CPYPRTWN instruction have one single form:
//
//   * CPYPRTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 6, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRTWN")
}

// CPYPT instruction have one single form:
//
//   * CPYPT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 3, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPT")
}

// CPYPTN instruction have one single form:
//
//   * CPYPTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 15, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPTN")
}

// CPYPTRN instruction have one single form:
//
//   * CPYPTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 11, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPTRN")
}

// CPYPTWN instruction have one single form:
//
//   * CPYPTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 7, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPTWN")
}

// CPYPWN instruction have one single form:
//
//   * CPYPWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 4, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWN")
}

// CPYPWT instruction have one single form:
//
//   * CPYPWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 1, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWT")
}

// CPYPWTN instruction have one single form:
//
//   * CPYPWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 13, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWTN")
}

// CPYPWTRN instruction have one single form:
//
//   * CPYPWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 9, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWTRN")
}

// CPYPWTWN instruction have one single form:
//
//   * CPYPWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 5, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWTWN")
}

// CRC32B instruction have one single form:
//
//   * CRC32B  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32B(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32B", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 16, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32B")
}

// CRC32CB instruction have one single form:
//
//   * CRC32CB  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32CB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32CB", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 20, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32CB")
}

// CRC32CH instruction have one single form:
//
//   * CRC32CH  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32CH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32CH", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 21, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32CH")
}

// CRC32CW instruction have one single form:
//
//   * CRC32CW  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32CW(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32CW", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 22, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32CW")
}

// CRC32CX instruction have one single form:
//
//   * CRC32CX  <Wd>, <Wn>, <Xm>
//
func (self *Program) CRC32CX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32CX", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isXr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 23, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32CX")
}

// CRC32H instruction have one single form:
//
//   * CRC32H  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32H(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32H", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 17, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32H")
}

// CRC32W instruction have one single form:
//
//   * CRC32W  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32W(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32W", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 18, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32W")
}

// CRC32X instruction have one single form:
//
//   * CRC32X  <Wd>, <Wn>, <Xm>
//
func (self *Program) CRC32X(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32X", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isXr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 19, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32X")
}

// CSDB instruction have one single form:
//
//   * CSDB
//
func (self *Program) CSDB() *Instruction {
    p := self.alloc("CSDB", 0, Operands {})
    return p.setins(hints(2, 4))
}

// CSEL instruction have 2 forms:
//
//   * CSEL  <Wd>, <Wn>, <Wm>, <cond>
//   * CSEL  <Xd>, <Xn>, <Xm>, <cond>
//
func (self *Program) CSEL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CSEL", 4, Operands { v0, v1, v2, v3 })
    // CSEL  <Wd>, <Wn>, <Wm>, <cond>
    if isWr(v0) && isWr(v1) && isWr(v2) && isBrCond(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(0, 0, 0, sa_wm, sa_cond, 0, sa_wn, sa_wd))
    }
    // CSEL  <Xd>, <Xn>, <Xm>, <cond>
    if isXr(v0) && isXr(v1) && isXr(v2) && isBrCond(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(1, 0, 0, sa_xm, sa_cond, 0, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CSEL")
}

// CSINC instruction have 2 forms:
//
//   * CSINC  <Wd>, <Wn>, <Wm>, <cond>
//   * CSINC  <Xd>, <Xn>, <Xm>, <cond>
//
func (self *Program) CSINC(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CSINC", 4, Operands { v0, v1, v2, v3 })
    // CSINC  <Wd>, <Wn>, <Wm>, <cond>
    if isWr(v0) && isWr(v1) && isWr(v2) && isBrCond(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(0, 0, 0, sa_wm, sa_cond, 1, sa_wn, sa_wd))
    }
    // CSINC  <Xd>, <Xn>, <Xm>, <cond>
    if isXr(v0) && isXr(v1) && isXr(v2) && isBrCond(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(1, 0, 0, sa_xm, sa_cond, 1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CSINC")
}

// CSINV instruction have 2 forms:
//
//   * CSINV  <Wd>, <Wn>, <Wm>, <cond>
//   * CSINV  <Xd>, <Xn>, <Xm>, <cond>
//
func (self *Program) CSINV(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CSINV", 4, Operands { v0, v1, v2, v3 })
    // CSINV  <Wd>, <Wn>, <Wm>, <cond>
    if isWr(v0) && isWr(v1) && isWr(v2) && isBrCond(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(0, 1, 0, sa_wm, sa_cond, 0, sa_wn, sa_wd))
    }
    // CSINV  <Xd>, <Xn>, <Xm>, <cond>
    if isXr(v0) && isXr(v1) && isXr(v2) && isBrCond(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(1, 1, 0, sa_xm, sa_cond, 0, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CSINV")
}

// CSNEG instruction have 2 forms:
//
//   * CSNEG  <Wd>, <Wn>, <Wm>, <cond>
//   * CSNEG  <Xd>, <Xn>, <Xm>, <cond>
//
func (self *Program) CSNEG(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CSNEG", 4, Operands { v0, v1, v2, v3 })
    // CSNEG  <Wd>, <Wn>, <Wm>, <cond>
    if isWr(v0) && isWr(v1) && isWr(v2) && isBrCond(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(0, 1, 0, sa_wm, sa_cond, 1, sa_wn, sa_wd))
    }
    // CSNEG  <Xd>, <Xn>, <Xm>, <cond>
    if isXr(v0) && isXr(v1) && isXr(v2) && isBrCond(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(1, 1, 0, sa_xm, sa_cond, 1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CSNEG")
}

// CTZ instruction have 2 forms:
//
//   * CTZ  <Wd>, <Wn>
//   * CTZ  <Xd>, <Xn>
//
func (self *Program) CTZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("CTZ", 2, Operands { v0, v1 })
    // CTZ  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 6, sa_wn, sa_wd))
    }
    // CTZ  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 6, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CTZ")
}

// DCPS1 instruction have one single form:
//
//   * DCPS1  {#<imm>}
//
func (self *Program) DCPS1(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("DCPS1", 0, Operands {})
        case 1  : p = self.alloc("DCPS1", 1, Operands { vv[0] })
        default : panic("instruction DCPS1 takes 0 or 1 operands")
    }
    if len(vv) >= 0 && len(vv) <= 1 && (len(vv) == 0 || isUimm16(vv[0])) {
        var sa_imm uint32
        if len(vv) > 0 {
            sa_imm = asUimm16(vv[0])
        }
        return p.setins(exception(5, sa_imm, 0, 1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for DCPS1")
}

// DCPS2 instruction have one single form:
//
//   * DCPS2  {#<imm>}
//
func (self *Program) DCPS2(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("DCPS2", 0, Operands {})
        case 1  : p = self.alloc("DCPS2", 1, Operands { vv[0] })
        default : panic("instruction DCPS2 takes 0 or 1 operands")
    }
    if len(vv) >= 0 && len(vv) <= 1 && (len(vv) == 0 || isUimm16(vv[0])) {
        var sa_imm uint32
        if len(vv) > 0 {
            sa_imm = asUimm16(vv[0])
        }
        return p.setins(exception(5, sa_imm, 0, 2))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for DCPS2")
}

// DCPS3 instruction have one single form:
//
//   * DCPS3  {#<imm>}
//
func (self *Program) DCPS3(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("DCPS3", 0, Operands {})
        case 1  : p = self.alloc("DCPS3", 1, Operands { vv[0] })
        default : panic("instruction DCPS3 takes 0 or 1 operands")
    }
    if len(vv) >= 0 && len(vv) <= 1 && (len(vv) == 0 || isUimm16(vv[0])) {
        var sa_imm uint32
        if len(vv) > 0 {
            sa_imm = asUimm16(vv[0])
        }
        return p.setins(exception(5, sa_imm, 0, 3))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for DCPS3")
}

// DGH instruction have one single form:
//
//   * DGH
//
func (self *Program) DGH() *Instruction {
    p := self.alloc("DGH", 0, Operands {})
    return p.setins(hints(0, 6))
}

// DMB instruction have one single form:
//
//   * DMB  <option>|#<imm>
//
func (self *Program) DMB(v0 interface{}) *Instruction {
    p := self.alloc("DMB", 1, Operands { v0 })
    if isOption(v0) {
        sa_option := v0.(BarrierOption)
        sa_imm := uint32(sa_option)
        return p.setins(barriers(sa_imm, 5, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for DMB")
}

// DRPS instruction have one single form:
//
//   * DRPS
//
func (self *Program) DRPS() *Instruction {
    p := self.alloc("DRPS", 0, Operands {})
    return p.setins(branch_reg(5, 31, 0, 31, 0))
}

// DSB instruction have 2 forms:
//
//   * DSB  <option>|#<imm>
//   * DSB  <option>nXS
//
func (self *Program) DSB(v0 interface{}) *Instruction {
    p := self.alloc("DSB", 1, Operands { v0 })
    // DSB  <option>|#<imm>
    if isOption(v0) {
        sa_option := v0.(BarrierOption)
        sa_imm := uint32(sa_option)
        return p.setins(barriers(sa_imm, 4, 31))
    }
    // DSB  <option>nXS
    if isOptionNXS(v0) {
        sa_option_1 := v0.(BarrierOption).nxs()
        CRm := uint32(0b0010)
        CRm |= sa_option_1 << 2
        return p.setins(barriers(CRm, 1, 31))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for DSB")
}

// EON instruction have 2 forms:
//
//   * EON  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * EON  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) EON(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("EON", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("EON", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction EON takes 3 or 4 operands")
    }
    // EON  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 2, sa_shift, 1, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // EON  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 2, sa_shift, 1, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for EON")
}

// EOR instruction have 5 forms:
//
//   * EOR  <Wd|WSP>, <Wn>, #<imm>
//   * EOR  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * EOR  <Xd|SP>, <Xn>, #<imm>
//   * EOR  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//   * EOR  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) EOR(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("EOR", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("EOR", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction EOR takes 3 or 4 operands")
    }
    // EOR  <Wd|WSP>, <Wn>, #<imm>
    if isWrOrWSP(v0) && isWr(v1) && isMask32(v2) {
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_imm := asMaskOp(v2)
        return p.setins(log_imm(0, 2, 0, (sa_imm >> 6) & 0x3f, sa_imm & 0x3f, sa_wn, sa_wd_wsp))
    }
    // EOR  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 2, sa_shift, 0, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // EOR  <Xd|SP>, <Xn>, #<imm>
    if isXrOrSP(v0) && isXr(v1) && isMask64(v2) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_imm_1 := asMaskOp(v2)
        return p.setins(log_imm(1, 2, (sa_imm_1 >> 12) & 0x1, (sa_imm_1 >> 6) & 0x3f, sa_imm_1 & 0x3f, sa_xn, sa_xd_sp))
    }
    // EOR  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 2, sa_shift, 0, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // EOR  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       isSameSize(v0, v1) &&
       isSameSize(v1, v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t, 1, 0, sa_vm, 3, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for EOR")
}

// EOR3 instruction have one single form:
//
//   * EOR3  <Vd>.16B, <Vn>.16B, <Vm>.16B, <Va>.16B
//
func (self *Program) EOR3(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("EOR3", 4, Operands { v0, v1, v2, v3 })
    if isVr(v0) &&
       vfmt(v0) == Vec16B &&
       isVr(v1) &&
       vfmt(v1) == Vec16B &&
       isVr(v2) &&
       vfmt(v2) == Vec16B &&
       isVr(v3) &&
       vfmt(v3) == Vec16B {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        sa_va := uint32(v3.(asm.Register).ID())
        return p.setins(crypto4(0, sa_vm, sa_va, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for EOR3")
}

// ERET instruction have one single form:
//
//   * ERET
//
func (self *Program) ERET() *Instruction {
    p := self.alloc("ERET", 0, Operands {})
    return p.setins(branch_reg(4, 31, 0, 31, 0))
}

// ERETAA instruction have one single form:
//
//   * ERETAA
//
func (self *Program) ERETAA() *Instruction {
    p := self.alloc("ERETAA", 0, Operands {})
    return p.setins(branch_reg(4, 31, 2, 31, 31))
}

// ERETAB instruction have one single form:
//
//   * ERETAB
//
func (self *Program) ERETAB() *Instruction {
    p := self.alloc("ERETAB", 0, Operands {})
    return p.setins(branch_reg(4, 31, 3, 31, 31))
}

// ESB instruction have one single form:
//
//   * ESB
//
func (self *Program) ESB() *Instruction {
    p := self.alloc("ESB", 0, Operands {})
    return p.setins(hints(2, 0))
}

// EXTR instruction have 2 forms:
//
//   * EXTR  <Wd>, <Wn>, <Wm>, #<lsb>
//   * EXTR  <Xd>, <Xn>, <Xm>, #<lsb>
//
func (self *Program) EXTR(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("EXTR", 4, Operands { v0, v1, v2, v3 })
    // EXTR  <Wd>, <Wn>, <Wm>, #<lsb>
    if isWr(v0) && isWr(v1) && isWr(v2) && isUimm6(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_lsb := asUimm6(v3)
        return p.setins(extract(0, 0, 0, 0, sa_wm, sa_lsb, sa_wn, sa_wd))
    }
    // EXTR  <Xd>, <Xn>, <Xm>, #<lsb>
    if isXr(v0) && isXr(v1) && isXr(v2) && isUimm6(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_lsb_1 := asUimm6(v3)
        return p.setins(extract(1, 0, 1, 0, sa_xm, sa_lsb_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for EXTR")
}

// FCMP instruction have 6 forms:
//
//   * FCMP  <Dn>, #0.0
//   * FCMP  <Dn>, <Dm>
//   * FCMP  <Hn>, #0.0
//   * FCMP  <Hn>, <Hm>
//   * FCMP  <Sn>, #0.0
//   * FCMP  <Sn>, <Sm>
//
func (self *Program) FCMP(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCMP", 2, Operands { v0, v1 })
    // FCMP  <Dn>, #0.0
    if isDr(v0) && isFloatLit(v1, 0.0) {
        sa_dn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 1, 0, 0, sa_dn, 8))
    }
    // FCMP  <Dn>, <Dm>
    if isDr(v0) && isDr(v1) {
        sa_dn_1 := uint32(v0.(asm.Register).ID())
        sa_dm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 1, sa_dm, 0, sa_dn_1, 0))
    }
    // FCMP  <Hn>, #0.0
    if isHr(v0) && isFloatLit(v1, 0.0) {
        sa_hn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 3, 0, 0, sa_hn, 8))
    }
    // FCMP  <Hn>, <Hm>
    if isHr(v0) && isHr(v1) {
        sa_hn_1 := uint32(v0.(asm.Register).ID())
        sa_hm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 3, sa_hm, 0, sa_hn_1, 0))
    }
    // FCMP  <Sn>, #0.0
    if isSr(v0) && isFloatLit(v1, 0.0) {
        sa_sn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 0, 0, 0, sa_sn, 8))
    }
    // FCMP  <Sn>, <Sm>
    if isSr(v0) && isSr(v1) {
        sa_sn_1 := uint32(v0.(asm.Register).ID())
        sa_sm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 0, sa_sm, 0, sa_sn_1, 0))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCMP")
}

// FCMPE instruction have 6 forms:
//
//   * FCMPE  <Dn>, #0.0
//   * FCMPE  <Dn>, <Dm>
//   * FCMPE  <Hn>, #0.0
//   * FCMPE  <Hn>, <Hm>
//   * FCMPE  <Sn>, #0.0
//   * FCMPE  <Sn>, <Sm>
//
func (self *Program) FCMPE(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCMPE", 2, Operands { v0, v1 })
    // FCMPE  <Dn>, #0.0
    if isDr(v0) && isFloatLit(v1, 0.0) {
        sa_dn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 1, 0, 0, sa_dn, 24))
    }
    // FCMPE  <Dn>, <Dm>
    if isDr(v0) && isDr(v1) {
        sa_dn_1 := uint32(v0.(asm.Register).ID())
        sa_dm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 1, sa_dm, 0, sa_dn_1, 16))
    }
    // FCMPE  <Hn>, #0.0
    if isHr(v0) && isFloatLit(v1, 0.0) {
        sa_hn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 3, 0, 0, sa_hn, 24))
    }
    // FCMPE  <Hn>, <Hm>
    if isHr(v0) && isHr(v1) {
        sa_hn_1 := uint32(v0.(asm.Register).ID())
        sa_hm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 3, sa_hm, 0, sa_hn_1, 16))
    }
    // FCMPE  <Sn>, #0.0
    if isSr(v0) && isFloatLit(v1, 0.0) {
        sa_sn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 0, 0, 0, sa_sn, 24))
    }
    // FCMPE  <Sn>, <Sm>
    if isSr(v0) && isSr(v1) {
        sa_sn_1 := uint32(v0.(asm.Register).ID())
        sa_sm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 0, sa_sm, 0, sa_sn_1, 16))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCMPE")
}

// FCVTZS instruction have 18 forms:
//
//   * FCVTZS  <Wd>, <Dn>, #<fbits>
//   * FCVTZS  <Wd>, <Dn>
//   * FCVTZS  <Wd>, <Hn>, #<fbits>
//   * FCVTZS  <Wd>, <Hn>
//   * FCVTZS  <Wd>, <Sn>, #<fbits>
//   * FCVTZS  <Wd>, <Sn>
//   * FCVTZS  <Xd>, <Dn>, #<fbits>
//   * FCVTZS  <Xd>, <Dn>
//   * FCVTZS  <Xd>, <Hn>, #<fbits>
//   * FCVTZS  <Xd>, <Hn>
//   * FCVTZS  <Xd>, <Sn>, #<fbits>
//   * FCVTZS  <Xd>, <Sn>
//   * FCVTZS  <Vd>.<T>, <Vn>.<T>
//   * FCVTZS  <Vd>.<T>, <Vn>.<T>
//   * FCVTZS  <Vd>.<T>, <Vn>.<T>, #<fbits>
//   * FCVTZS  <V><d>, <V><n>
//   * FCVTZS  <Hd>, <Hn>
//   * FCVTZS  <V><d>, <V><n>, #<fbits>
//
func (self *Program) FCVTZS(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("FCVTZS", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("FCVTZS", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction FCVTZS takes 2 or 3 operands")
    }
    // FCVTZS  <Wd>, <Dn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isDr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 1, 3, 0, sa_fbits, sa_dn, sa_wd))
    }
    // FCVTZS  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 3, 0, sa_dn, sa_wd))
    }
    // FCVTZS  <Wd>, <Hn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isHr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 3, 3, 0, sa_fbits, sa_hn, sa_wd))
    }
    // FCVTZS  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 3, 0, sa_hn, sa_wd))
    }
    // FCVTZS  <Wd>, <Sn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isSr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 0, 3, 0, sa_fbits, sa_sn, sa_wd))
    }
    // FCVTZS  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 3, 0, sa_sn, sa_wd))
    }
    // FCVTZS  <Xd>, <Dn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isDr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 1, 3, 0, sa_fbits_1, sa_dn, sa_xd))
    }
    // FCVTZS  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 3, 0, sa_dn, sa_xd))
    }
    // FCVTZS  <Xd>, <Hn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isHr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 3, 3, 0, sa_fbits_1, sa_hn, sa_xd))
    }
    // FCVTZS  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 3, 0, sa_hn, sa_xd))
    }
    // FCVTZS  <Xd>, <Sn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isSr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 0, 3, 0, sa_fbits_1, sa_sn, sa_xd))
    }
    // FCVTZS  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 3, 0, sa_sn, sa_xd))
    }
    // FCVTZS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isSameSize(v0, v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= (sa_t >> 1) & 0x1
        return p.setins(asimdmisc(sa_t & 0x1, 0, size, 27, sa_vn, sa_vd))
    }
    // FCVTZS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && isSameSize(v0, v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 1, 27, sa_vn, sa_vd))
    }
    // FCVTZS  <Vd>.<T>, <Vn>.<T>, #<fbits>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(vv[0]) &&
       isSameSize(v0, v1) {
        var sa_fbits uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch (sa_t >> 1) & 0xf {
            case 0b0010: sa_fbits = 32 - uint32(asLit(vv[0]))
            case 0b0100: sa_fbits = 64 - uint32(asLit(vv[0]))
            case 0b1000: sa_fbits = 128 - uint32(asLit(vv[0]))
            default: panic("aarch64: invalid operand 'sa_fbits' for FCVTZS")
        }
        if ((sa_fbits >> 3) & 0xf) & ((sa_t__bit_mask >> 1) & 0xf) != (sa_t >> 1) & 0xf {
            panic("aarch64: invalid combination of operands for FCVTZS")
        }
        return p.setins(asimdshf(sa_t & 0x1, 0, (sa_fbits >> 3) & 0xf, sa_fbits & 0x7, 31, sa_vn, sa_vd))
    }
    // FCVTZS  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SIMDRegister32: sa_v = 0b0
            case SIMDRegister64: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTZS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 27, sa_n, sa_d))
    }
    // FCVTZS  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 1, 27, sa_hn, sa_hd))
    }
    // FCVTZS  <V><d>, <V><n>, #<fbits>
    if len(vv) == 1 && isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(vv[0]) && isSameType(v0, v1) {
        var sa_fbits_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SIMDRegister16: sa_v = 0b0010
            case SIMDRegister32: sa_v = 0b0100
            case SIMDRegister64: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for FCVTZS")
        }
        switch v0.(type) {
            case SIMDRegister16: sa_v__bit_mask = 0b1110
            case SIMDRegister32: sa_v__bit_mask = 0b1100
            case SIMDRegister64: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for FCVTZS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b0010: sa_fbits_1 = 32 - uint32(asLit(vv[0]))
            case 0b0100: sa_fbits_1 = 64 - uint32(asLit(vv[0]))
            case 0b1000: sa_fbits_1 = 128 - uint32(asLit(vv[0]))
            default: panic("aarch64: invalid operand 'sa_fbits_1' for FCVTZS")
        }
        if ((sa_fbits_1 >> 3) & 0xf) & sa_v__bit_mask != sa_v {
            panic("aarch64: invalid combination of operands for FCVTZS")
        }
        return p.setins(asisdshf(0, (sa_fbits_1 >> 3) & 0xf, sa_fbits_1 & 0x7, 31, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTZS")
}

// FMOV instruction have 19 forms:
//
//   * FMOV  <Wd>, <Hn>
//   * FMOV  <Wd>, <Sn>
//   * FMOV  <Xd>, <Dn>
//   * FMOV  <Xd>, <Hn>
//   * FMOV  <Xd>, <Vn>.D[1]
//   * FMOV  <Dd>, <Xn>
//   * FMOV  <Dd>, <Dn>
//   * FMOV  <Dd>, #<imm>
//   * FMOV  <Hd>, <Wn>
//   * FMOV  <Hd>, <Xn>
//   * FMOV  <Hd>, <Hn>
//   * FMOV  <Hd>, #<imm>
//   * FMOV  <Sd>, <Wn>
//   * FMOV  <Sd>, <Sn>
//   * FMOV  <Sd>, #<imm>
//   * FMOV  <Vd>.D[1], <Xn>
//   * FMOV  <Vd>.2D, #<imm>
//   * FMOV  <Vd>.<T>, #<imm>
//   * FMOV  <Vd>.<T>, #<imm>
//
func (self *Program) FMOV(v0, v1 interface{}) *Instruction {
    p := self.alloc("FMOV", 2, Operands { v0, v1 })
    // FMOV  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 6, sa_hn, sa_wd))
    }
    // FMOV  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 6, sa_sn, sa_wd))
    }
    // FMOV  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 6, sa_dn, sa_xd))
    }
    // FMOV  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 6, sa_hn, sa_xd))
    }
    // FMOV  <Xd>, <Vn>.D[1]
    if isXr(v0) && isVri(v1) && vstrr(v1) == VecD && vidxr(v1) == 1 {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(_Indexed128r).ID())
        return p.setins(float2int(1, 0, 2, 1, 6, sa_vn, sa_xd))
    }
    // FMOV  <Dd>, <Xn>
    if isDr(v0) && isXr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 7, sa_xn, sa_dd))
    }
    // FMOV  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 0, sa_dn, sa_dd))
    }
    // FMOV  <Dd>, #<imm>
    if isDr(v0) && isFpImm8(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_imm := asFpImm8(v1)
        return p.setins(floatimm(0, 0, 1, sa_imm, 0, sa_dd))
    }
    // FMOV  <Hd>, <Wn>
    if isHr(v0) && isWr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 7, sa_wn, sa_hd))
    }
    // FMOV  <Hd>, <Xn>
    if isHr(v0) && isXr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 7, sa_xn, sa_hd))
    }
    // FMOV  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 0, sa_hn, sa_hd))
    }
    // FMOV  <Hd>, #<imm>
    if isHr(v0) && isFpImm8(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_imm := asFpImm8(v1)
        return p.setins(floatimm(0, 0, 3, sa_imm, 0, sa_hd))
    }
    // FMOV  <Sd>, <Wn>
    if isSr(v0) && isWr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 7, sa_wn, sa_sd))
    }
    // FMOV  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 0, sa_sn, sa_sd))
    }
    // FMOV  <Sd>, #<imm>
    if isSr(v0) && isFpImm8(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_imm := asFpImm8(v1)
        return p.setins(floatimm(0, 0, 0, sa_imm, 0, sa_sd))
    }
    // FMOV  <Vd>.D[1], <Xn>
    if isVri(v0) && vstrr(v0) == VecD && vidxr(v0) == 1 && isXr(v1) {
        sa_vd := uint32(v0.(_Indexed128r).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 2, 1, 7, sa_xn, sa_vd))
    }
    // FMOV  <Vd>.2D, #<imm>
    if isVr(v0) && vfmt(v0) == Vec2D && isUimm8(v1) {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm8(v1)
        return p.setins(asimdimm(
            1,
            1,
            (sa_imm >> 7) & 0x1,
            (sa_imm >> 6) & 0x1,
            (sa_imm >> 5) & 0x1,
            15,
            0,
            (sa_imm >> 4) & 0x1,
            (sa_imm >> 3) & 0x1,
            (sa_imm >> 2) & 0x1,
            (sa_imm >> 1) & 0x1,
            sa_imm & 0x1,
            sa_vd,
        ))
    }
    // FMOV  <Vd>.<T>, #<imm>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isUimm8(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm := asUimm8(v1)
        return p.setins(asimdimm(
            sa_t_1,
            0,
            (sa_imm >> 7) & 0x1,
            (sa_imm >> 6) & 0x1,
            (sa_imm >> 5) & 0x1,
            15,
            1,
            (sa_imm >> 4) & 0x1,
            (sa_imm >> 3) & 0x1,
            (sa_imm >> 2) & 0x1,
            (sa_imm >> 1) & 0x1,
            sa_imm & 0x1,
            sa_vd,
        ))
    }
    // FMOV  <Vd>.<T>, #<imm>
    if isVr(v0) && isVfmt(v0, Vec2S, Vec4S) && isUimm8(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b0
            case Vec4S: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm := asUimm8(v1)
        return p.setins(asimdimm(
            sa_t,
            0,
            (sa_imm >> 7) & 0x1,
            (sa_imm >> 6) & 0x1,
            (sa_imm >> 5) & 0x1,
            15,
            0,
            (sa_imm >> 4) & 0x1,
            (sa_imm >> 3) & 0x1,
            (sa_imm >> 2) & 0x1,
            (sa_imm >> 1) & 0x1,
            sa_imm & 0x1,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMOV")
}

// HINT instruction have one single form:
//
//   * HINT  #<imm>
//
func (self *Program) HINT(v0 interface{}) *Instruction {
    p := self.alloc("HINT", 1, Operands { v0 })
    if isUimm7(v0) {
        sa_imm := asUimm7(v0)
        return p.setins(hints((sa_imm >> 3) & 0xf, sa_imm & 0x7))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for HINT")
}

// IRG instruction have one single form:
//
//   * IRG  <Xd|SP>, <Xn|SP>{, <Xm>}
//
func (self *Program) IRG(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("IRG", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("IRG", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction IRG takes 2 or 3 operands")
    }
    if len(vv) >= 0 && len(vv) <= 1 && isXrOrSP(v0) && isXrOrSP(v1) && (len(vv) == 0 || isXr(vv[0])) {
        var sa_xm uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        if len(vv) > 0 {
            sa_xm = uint32(vv[0].(asm.Register).ID())
        }
        Rm := uint32(0b00000)
        Rm |= sa_xm
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd_sp
        return p.setins(dp_2src(1, 0, Rm, 4, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for IRG")
}

// LDNP instruction have 5 forms:
//
//   * LDNP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
//   * LDNP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * LDNP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
//   * LDNP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
//   * LDNP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
//
func (self *Program) LDNP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDNP", 3, Operands { v0, v1, v2 })
    // LDNP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(0, 0, 1, sa_imm, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDNP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(2, 0, 1, sa_imm_1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDNP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(1, 1, 1, sa_imm, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // LDNP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(2, 1, 1, sa_imm_1, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // LDNP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(0, 1, 1, sa_imm_2, sa_st2, sa_xn_sp, sa_st1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDNP")
}

// LDP instruction have 15 forms:
//
//   * LDP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <Wt1>, <Wt2>, [<Xn|SP>], #<imm>
//   * LDP  <Wt1>, <Wt2>, [<Xn|SP>, #<imm>]!
//   * LDP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
//   * LDP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
//   * LDP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <Dt1>, <Dt2>, [<Xn|SP>], #<imm>
//   * LDP  <Dt1>, <Dt2>, [<Xn|SP>, #<imm>]!
//   * LDP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <Qt1>, <Qt2>, [<Xn|SP>], #<imm>
//   * LDP  <Qt1>, <Qt2>, [<Xn|SP>, #<imm>]!
//   * LDP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <St1>, <St2>, [<Xn|SP>], #<imm>
//   * LDP  <St1>, <St2>, [<Xn|SP>, #<imm>]!
//
func (self *Program) LDP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDP", 3, Operands { v0, v1, v2 })
    // LDP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(0, 0, 1, sa_imm, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDP  <Wt1>, <Wt2>, [<Xn|SP>], #<imm>
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(0, 0, 1, sa_imm_1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDP  <Wt1>, <Wt2>, [<Xn|SP>, #<imm>]!
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(0, 0, 1, sa_imm_1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstpair_off(2, 0, 1, sa_imm_2, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_post(2, 0, 1, sa_imm_3, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(2, 0, 1, sa_imm_3, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(1, 1, 1, sa_imm, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // LDP  <Dt1>, <Dt2>, [<Xn|SP>], #<imm>
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(1, 1, 1, sa_imm_1, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // LDP  <Dt1>, <Dt2>, [<Xn|SP>, #<imm>]!
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(1, 1, 1, sa_imm_1, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // LDP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstpair_off(2, 1, 1, sa_imm_2, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // LDP  <Qt1>, <Qt2>, [<Xn|SP>], #<imm>
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_post(2, 1, 1, sa_imm_3, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // LDP  <Qt1>, <Qt2>, [<Xn|SP>, #<imm>]!
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(2, 1, 1, sa_imm_3, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // LDP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_4 := uint32(moffs(v2))
        return p.setins(ldstpair_off(0, 1, 1, sa_imm_4, sa_st2, sa_xn_sp, sa_st1))
    }
    // LDP  <St1>, <St2>, [<Xn|SP>], #<imm>
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_5 := uint32(moffs(v2))
        return p.setins(ldstpair_post(0, 1, 1, sa_imm_5, sa_st2, sa_xn_sp, sa_st1))
    }
    // LDP  <St1>, <St2>, [<Xn|SP>, #<imm>]!
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_5 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(0, 1, 1, sa_imm_5, sa_st2, sa_xn_sp, sa_st1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDP")
}

// LDPSW instruction have 3 forms:
//
//   * LDPSW  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * LDPSW  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
//   * LDPSW  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
//
func (self *Program) LDPSW(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDPSW", 3, Operands { v0, v1, v2 })
    // LDPSW  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(1, 0, 1, sa_imm, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDPSW  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(1, 0, 1, sa_imm_1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDPSW  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(1, 0, 1, sa_imm_1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDPSW")
}

// LDR instruction have 34 forms:
//
//   * LDR  <Wt>, [<Xn|SP>], #<simm>
//   * LDR  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Wt>, <label>
//   * LDR  <Xt>, [<Xn|SP>], #<simm>
//   * LDR  <Xt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Xt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Xt>, <label>
//   * LDR  <Bt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * LDR  <Bt>, [<Xn|SP>], #<simm>
//   * LDR  <Bt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Bt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Bt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * LDR  <Dt>, [<Xn|SP>], #<simm>
//   * LDR  <Dt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Dt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Dt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Dt>, <label>
//   * LDR  <Ht>, [<Xn|SP>], #<simm>
//   * LDR  <Ht>, [<Xn|SP>, #<simm>]!
//   * LDR  <Ht>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Ht>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Qt>, [<Xn|SP>], #<simm>
//   * LDR  <Qt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Qt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Qt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Qt>, <label>
//   * LDR  <St>, [<Xn|SP>], #<simm>
//   * LDR  <St>, [<Xn|SP>, #<simm>]!
//   * LDR  <St>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <St>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <St>, <label>
//
func (self *Program) LDR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDR", 2, Operands { v0, v1 })
    // LDR  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDR  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDR  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 0, 1, sa_pimm, sa_xn_sp, sa_wt))
    }
    // LDR  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 0, 1, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDR  <Wt>, <label>
    if isWr(v0) && isLabel(v1) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(0, 0, uint32(sa_label.RelativeTo(pc)), sa_wt) })
    }
    // LDR  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(3, 0, 1, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDR  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(3, 0, 1, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDR  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_1 := uint32(moffs(v1))
        return p.setins(ldst_pos(3, 0, 1, sa_pimm_1, sa_xn_sp, sa_xt))
    }
    // LDR  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_1 uint32
        var sa_extend uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount_1 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(3, 0, 1, sa_xm, sa_extend, sa_amount_1, sa_xn_sp, sa_xt))
    }
    // LDR  <Xt>, <label>
    if isXr(v0) && isLabel(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(1, 0, uint32(sa_label.RelativeTo(pc)), sa_xt) })
    }
    // LDR  <Bt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isBr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 1, 1, sa_xm, 3, sa_amount, sa_xn_sp, sa_bt))
    }
    // LDR  <Bt>, [<Xn|SP>], #<simm>
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 1, 1, sa_simm, sa_xn_sp, sa_bt))
    }
    // LDR  <Bt>, [<Xn|SP>, #<simm>]!
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 1, 1, sa_simm, sa_xn_sp, sa_bt))
    }
    // LDR  <Bt>, [<Xn|SP>{, #<pimm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 1, 1, sa_pimm, sa_xn_sp, sa_bt))
    }
    // LDR  <Bt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 1, 1, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_bt))
    }
    // LDR  <Dt>, [<Xn|SP>], #<simm>
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(3, 1, 1, sa_simm, sa_xn_sp, sa_dt))
    }
    // LDR  <Dt>, [<Xn|SP>, #<simm>]!
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(3, 1, 1, sa_simm, sa_xn_sp, sa_dt))
    }
    // LDR  <Dt>, [<Xn|SP>{, #<pimm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_1 := uint32(moffs(v1))
        return p.setins(ldst_pos(3, 1, 1, sa_pimm_1, sa_xn_sp, sa_dt))
    }
    // LDR  <Dt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isDr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_1 uint32
        var sa_extend_1 uint32
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_1 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(3, 1, 1, sa_xm, sa_extend_1, sa_amount_1, sa_xn_sp, sa_dt))
    }
    // LDR  <Dt>, <label>
    if isDr(v0) && isLabel(v1) {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(1, 1, uint32(sa_label.RelativeTo(pc)), sa_dt) })
    }
    // LDR  <Ht>, [<Xn|SP>], #<simm>
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 1, 1, sa_simm, sa_xn_sp, sa_ht))
    }
    // LDR  <Ht>, [<Xn|SP>, #<simm>]!
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 1, 1, sa_simm, sa_xn_sp, sa_ht))
    }
    // LDR  <Ht>, [<Xn|SP>{, #<pimm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_2 := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 1, 1, sa_pimm_2, sa_xn_sp, sa_ht))
    }
    // LDR  <Ht>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isHr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_2 uint32
        var sa_extend_1 uint32
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_2 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 1, 1, sa_xm, sa_extend_1, sa_amount_2, sa_xn_sp, sa_ht))
    }
    // LDR  <Qt>, [<Xn|SP>], #<simm>
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 1, 3, sa_simm, sa_xn_sp, sa_qt))
    }
    // LDR  <Qt>, [<Xn|SP>, #<simm>]!
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 1, 3, sa_simm, sa_xn_sp, sa_qt))
    }
    // LDR  <Qt>, [<Xn|SP>{, #<pimm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_3 := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 1, 3, sa_pimm_3, sa_xn_sp, sa_qt))
    }
    // LDR  <Qt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isQr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_3 uint32
        var sa_extend_1 uint32
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_3 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 1, 3, sa_xm, sa_extend_1, sa_amount_3, sa_xn_sp, sa_qt))
    }
    // LDR  <Qt>, <label>
    if isQr(v0) && isLabel(v1) {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(2, 1, uint32(sa_label.RelativeTo(pc)), sa_qt) })
    }
    // LDR  <St>, [<Xn|SP>], #<simm>
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 1, 1, sa_simm, sa_xn_sp, sa_st))
    }
    // LDR  <St>, [<Xn|SP>, #<simm>]!
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 1, 1, sa_simm, sa_xn_sp, sa_st))
    }
    // LDR  <St>, [<Xn|SP>{, #<pimm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_4 := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 1, 1, sa_pimm_4, sa_xn_sp, sa_st))
    }
    // LDR  <St>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isSr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_4 uint32
        var sa_extend_1 uint32
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_4 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 1, 1, sa_xm, sa_extend_1, sa_amount_4, sa_xn_sp, sa_st))
    }
    // LDR  <St>, <label>
    if isSr(v0) && isLabel(v1) {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(0, 1, uint32(sa_label.RelativeTo(pc)), sa_st) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDR")
}

// LDRAA instruction have 2 forms:
//
//   * LDRAA  <Xt>, [<Xn|SP>{, #<simm>}]!
//   * LDRAA  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDRAA(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRAA", 2, Operands { v0, v1 })
    // LDRAA  <Xt>, [<Xn|SP>{, #<simm>}]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_pac(3, 0, 0, (sa_simm >> 9) & 0x1, sa_simm & 0x1ff, 1, sa_xn_sp, sa_xt))
    }
    // LDRAA  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_pac(3, 0, 0, (sa_simm >> 9) & 0x1, sa_simm & 0x1ff, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRAA")
}

// LDRAB instruction have 2 forms:
//
//   * LDRAB  <Xt>, [<Xn|SP>{, #<simm>}]!
//   * LDRAB  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDRAB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRAB", 2, Operands { v0, v1 })
    // LDRAB  <Xt>, [<Xn|SP>{, #<simm>}]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_pac(3, 0, 1, (sa_simm >> 9) & 0x1, sa_simm & 0x1ff, 1, sa_xn_sp, sa_xt))
    }
    // LDRAB  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_pac(3, 0, 1, (sa_simm >> 9) & 0x1, sa_simm & 0x1ff, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRAB")
}

// LDRB instruction have 5 forms:
//
//   * LDRB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * LDRB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * LDRB  <Wt>, [<Xn|SP>], #<simm>
//   * LDRB  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDRB  <Wt>, [<Xn|SP>{, #<pimm>}]
//
func (self *Program) LDRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRB", 2, Operands { v0, v1 })
    // LDRB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 0, 1, sa_xm, 3, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 0, 1, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRB  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRB  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRB  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 0, 1, sa_pimm, sa_xn_sp, sa_wt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRB")
}

// LDRH instruction have 4 forms:
//
//   * LDRH  <Wt>, [<Xn|SP>], #<simm>
//   * LDRH  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDRH  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * LDRH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//
func (self *Program) LDRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRH", 2, Operands { v0, v1 })
    // LDRH  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRH  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRH  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 0, 1, sa_pimm, sa_xn_sp, sa_wt))
    }
    // LDRH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 0, 1, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRH")
}

// LDRSB instruction have 10 forms:
//
//   * LDRSB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * LDRSB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * LDRSB  <Wt>, [<Xn|SP>], #<simm>
//   * LDRSB  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDRSB  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * LDRSB  <Xt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * LDRSB  <Xt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * LDRSB  <Xt>, [<Xn|SP>], #<simm>
//   * LDRSB  <Xt>, [<Xn|SP>, #<simm>]!
//   * LDRSB  <Xt>, [<Xn|SP>{, #<pimm>}]
//
func (self *Program) LDRSB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRSB", 2, Operands { v0, v1 })
    // LDRSB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 0, 3, sa_xm, 3, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 0, 3, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 0, 3, sa_pimm, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Xt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 0, 2, sa_xm, 3, sa_amount, sa_xn_sp, sa_xt))
    }
    // LDRSB  <Xt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 0, 2, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_xt))
    }
    // LDRSB  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSB  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSB  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 0, 2, sa_pimm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRSB")
}

// LDRSH instruction have 8 forms:
//
//   * LDRSH  <Wt>, [<Xn|SP>], #<simm>
//   * LDRSH  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDRSH  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * LDRSH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDRSH  <Xt>, [<Xn|SP>], #<simm>
//   * LDRSH  <Xt>, [<Xn|SP>, #<simm>]!
//   * LDRSH  <Xt>, [<Xn|SP>{, #<pimm>}]
//   * LDRSH  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//
func (self *Program) LDRSH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRSH", 2, Operands { v0, v1 })
    // LDRSH  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRSH  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRSH  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 0, 3, sa_pimm, sa_xn_sp, sa_wt))
    }
    // LDRSH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 0, 3, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRSH  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSH  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSH  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 0, 2, sa_pimm, sa_xn_sp, sa_xt))
    }
    // LDRSH  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 0, 2, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRSH")
}

// LDRSW instruction have 5 forms:
//
//   * LDRSW  <Xt>, [<Xn|SP>], #<simm>
//   * LDRSW  <Xt>, [<Xn|SP>, #<simm>]!
//   * LDRSW  <Xt>, [<Xn|SP>{, #<pimm>}]
//   * LDRSW  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDRSW  <Xt>, <label>
//
func (self *Program) LDRSW(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRSW", 2, Operands { v0, v1 })
    // LDRSW  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSW  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSW  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 0, 2, sa_pimm, sa_xn_sp, sa_xt))
    }
    // LDRSW  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 0, 2, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_xt))
    }
    // LDRSW  <Xt>, <label>
    if isXr(v0) && isLabel(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(2, 0, uint32(sa_label.RelativeTo(pc)), sa_xt) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRSW")
}

// MADD instruction have 2 forms:
//
//   * MADD  <Wd>, <Wn>, <Wm>, <Wa>
//   * MADD  <Xd>, <Xn>, <Xm>, <Xa>
//
func (self *Program) MADD(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("MADD", 4, Operands { v0, v1, v2, v3 })
    // MADD  <Wd>, <Wn>, <Wm>, <Wa>
    if isWr(v0) && isWr(v1) && isWr(v2) && isWr(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_wa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(0, 0, 0, sa_wm, 0, sa_wa, sa_wn, sa_wd))
    }
    // MADD  <Xd>, <Xn>, <Xm>, <Xa>
    if isXr(v0) && isXr(v1) && isXr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 0, sa_xm, 0, sa_xa, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MADD")
}

// MRS instruction have one single form:
//
//   * MRS  <Xt>, (<systemreg>|S<op0>_<op1>_<Cn>_<Cm>_<op2>)
//
func (self *Program) MRS(v0, v1 interface{}) *Instruction {
    p := self.alloc("MRS", 2, Operands { v0, v1 })
    if isXr(v0) && isSysReg(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_systemreg := uint32(v1.(SystemRegister))
        return p.setins(systemmove(
            1,
            (sa_systemreg >> 6) & 0x1,
            (sa_systemreg >> 3) & 0x7,
            (sa_systemreg >> 7) & 0xf,
            (sa_systemreg >> 11) & 0xf,
            sa_systemreg & 0x7,
            sa_xt,
        ))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for MRS")
}

// ORN instruction have 3 forms:
//
//   * ORN  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ORN  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//   * ORN  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) ORN(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ORN", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ORN", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ORN takes 3 or 4 operands")
    }
    // ORN  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 1, sa_shift, 1, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ORN  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if len(vv) >= 0 && len(vv) <= 1 && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) > 0 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 1, sa_shift, 1, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // ORN  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       isSameSize(v0, v1) &&
       isSameSize(v1, v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t, 0, 3, sa_vm, 3, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ORN")
}

// ORR instruction have 7 forms:
//
//   * ORR  <Wd|WSP>, <Wn>, #<imm>
//   * ORR  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ORR  <Xd|SP>, <Xn>, #<imm>
//   * ORR  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//   * ORR  <Vd>.<T>, #<imm8>{, LSL #<amount>}
//   * ORR  <Vd>.<T>, #<imm8>{, LSL #<amount>}
//   * ORR  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) ORR(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ORR", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("ORR", 3, Operands { v0, v1, vv[0] })
        case 2  : p = self.alloc("ORR", 4, Operands { v0, v1, vv[0], vv[1] })
        default : panic("instruction ORR takes 2 or 3 or 4 operands")
    }
    // ORR  <Wd|WSP>, <Wn>, #<imm>
    if len(vv) == 1 && isWrOrWSP(v0) && isWr(v1) && isMask32(vv[0]) {
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_imm := asMaskOp(vv[0])
        return p.setins(log_imm(0, 1, 0, (sa_imm >> 6) & 0x3f, sa_imm & 0x3f, sa_wn, sa_wd_wsp))
    }
    // ORR  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if len(vv) >= 1 && len(vv) <= 2 && isWr(v0) && isWr(v1) && isWr(vv[0]) && (len(vv) <= 1 || isShift(vv[1])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(vv[0].(asm.Register).ID())
        if len(vv) > 1 {
            sa_shift = uint32(vv[1].(ShiftType).ShiftType())
            sa_amount = uint32(vv[1].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 1, sa_shift, 0, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ORR  <Xd|SP>, <Xn>, #<imm>
    if len(vv) == 1 && isXrOrSP(v0) && isXr(v1) && isMask64(vv[0]) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_imm_1 := asMaskOp(vv[0])
        return p.setins(log_imm(1, 1, (sa_imm_1 >> 12) & 0x1, (sa_imm_1 >> 6) & 0x3f, sa_imm_1 & 0x3f, sa_xn, sa_xd_sp))
    }
    // ORR  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if len(vv) >= 1 && len(vv) <= 2 && isXr(v0) && isXr(v1) && isXr(vv[0]) && (len(vv) <= 1 || isShift(vv[1])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(vv[0].(asm.Register).ID())
        if len(vv) > 1 {
            sa_shift = uint32(vv[1].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[1].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 1, sa_shift, 0, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // ORR  <Vd>.<T>, #<imm8>{, LSL #<amount>}
    if len(vv) >= 0 &&
       len(vv) <= 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isUimm8(v1) &&
       (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_amount uint32
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        if len(vv) > 0 {
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        cmode := uint32(0b1001)
        switch sa_amount {
            case 0: cmode |= 0b0 << 1
            case 8: cmode |= 0b1 << 1
            default: panic("aarch64: invalid combination of operands for ORR")
        }
        return p.setins(asimdimm(
            sa_t,
            0,
            (sa_imm8 >> 7) & 0x1,
            (sa_imm8 >> 6) & 0x1,
            (sa_imm8 >> 5) & 0x1,
            cmode,
            0,
            (sa_imm8 >> 4) & 0x1,
            (sa_imm8 >> 3) & 0x1,
            (sa_imm8 >> 2) & 0x1,
            (sa_imm8 >> 1) & 0x1,
            sa_imm8 & 0x1,
            sa_vd,
        ))
    }
    // ORR  <Vd>.<T>, #<imm8>{, LSL #<amount>}
    if len(vv) >= 0 &&
       len(vv) <= 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isUimm8(v1) &&
       (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_amount_1 uint32
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t_1 = 0b0
            case Vec4S: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        if len(vv) > 0 {
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        cmode := uint32(0b0001)
        switch sa_amount_1 {
            case 0: cmode |= 0b00 << 1
            case 8: cmode |= 0b01 << 1
            case 16: cmode |= 0b10 << 1
            case 24: cmode |= 0b11 << 1
            default: panic("aarch64: invalid combination of operands for ORR")
        }
        return p.setins(asimdimm(
            sa_t_1,
            0,
            (sa_imm8 >> 7) & 0x1,
            (sa_imm8 >> 6) & 0x1,
            (sa_imm8 >> 5) & 0x1,
            cmode,
            0,
            (sa_imm8 >> 4) & 0x1,
            (sa_imm8 >> 3) & 0x1,
            (sa_imm8 >> 2) & 0x1,
            (sa_imm8 >> 1) & 0x1,
            sa_imm8 & 0x1,
            sa_vd,
        ))
    }
    // ORR  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec8B, Vec16B) &&
       isSameSize(v0, v1) &&
       isSameSize(v1, vv[0]) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        return p.setins(asimdsame(sa_t, 0, 2, sa_vm, 3, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ORR")
}

// SCVTF instruction have 18 forms:
//
//   * SCVTF  <Dd>, <Wn>, #<fbits>
//   * SCVTF  <Dd>, <Wn>
//   * SCVTF  <Dd>, <Xn>, #<fbits>
//   * SCVTF  <Dd>, <Xn>
//   * SCVTF  <Hd>, <Wn>, #<fbits>
//   * SCVTF  <Hd>, <Wn>
//   * SCVTF  <Hd>, <Xn>, #<fbits>
//   * SCVTF  <Hd>, <Xn>
//   * SCVTF  <Sd>, <Wn>, #<fbits>
//   * SCVTF  <Sd>, <Wn>
//   * SCVTF  <Sd>, <Xn>, #<fbits>
//   * SCVTF  <Sd>, <Xn>
//   * SCVTF  <Vd>.<T>, <Vn>.<T>
//   * SCVTF  <Vd>.<T>, <Vn>.<T>
//   * SCVTF  <Vd>.<T>, <Vn>.<T>, #<fbits>
//   * SCVTF  <V><d>, <V><n>
//   * SCVTF  <Hd>, <Hn>
//   * SCVTF  <V><d>, <V><n>, #<fbits>
//
func (self *Program) SCVTF(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("SCVTF", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("SCVTF", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction SCVTF takes 2 or 3 operands")
    }
    // SCVTF  <Dd>, <Wn>, #<fbits>
    if len(vv) == 1 && isDr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 1, 0, 2, sa_fbits, sa_wn, sa_dd))
    }
    // SCVTF  <Dd>, <Wn>
    if isDr(v0) && isWr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 2, sa_wn, sa_dd))
    }
    // SCVTF  <Dd>, <Xn>, #<fbits>
    if len(vv) == 1 && isDr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 1, 0, 2, sa_fbits_1, sa_xn, sa_dd))
    }
    // SCVTF  <Dd>, <Xn>
    if isDr(v0) && isXr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 2, sa_xn, sa_dd))
    }
    // SCVTF  <Hd>, <Wn>, #<fbits>
    if len(vv) == 1 && isHr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 3, 0, 2, sa_fbits, sa_wn, sa_hd))
    }
    // SCVTF  <Hd>, <Wn>
    if isHr(v0) && isWr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 2, sa_wn, sa_hd))
    }
    // SCVTF  <Hd>, <Xn>, #<fbits>
    if len(vv) == 1 && isHr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 3, 0, 2, sa_fbits_1, sa_xn, sa_hd))
    }
    // SCVTF  <Hd>, <Xn>
    if isHr(v0) && isXr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 2, sa_xn, sa_hd))
    }
    // SCVTF  <Sd>, <Wn>, #<fbits>
    if len(vv) == 1 && isSr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 0, 0, 2, sa_fbits, sa_wn, sa_sd))
    }
    // SCVTF  <Sd>, <Wn>
    if isSr(v0) && isWr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 2, sa_wn, sa_sd))
    }
    // SCVTF  <Sd>, <Xn>, #<fbits>
    if len(vv) == 1 && isSr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 0, 0, 2, sa_fbits_1, sa_xn, sa_sd))
    }
    // SCVTF  <Sd>, <Xn>
    if isSr(v0) && isXr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 2, sa_xn, sa_sd))
    }
    // SCVTF  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isSameSize(v0, v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= (sa_t >> 1) & 0x1
        return p.setins(asimdmisc(sa_t & 0x1, 0, size, 29, sa_vn, sa_vd))
    }
    // SCVTF  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && isSameSize(v0, v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 0, 29, sa_vn, sa_vd))
    }
    // SCVTF  <Vd>.<T>, <Vn>.<T>, #<fbits>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(vv[0]) &&
       isSameSize(v0, v1) {
        var sa_fbits uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch (sa_t >> 1) & 0xf {
            case 0b0010: sa_fbits = 32 - uint32(asLit(vv[0]))
            case 0b0100: sa_fbits = 64 - uint32(asLit(vv[0]))
            case 0b1000: sa_fbits = 128 - uint32(asLit(vv[0]))
            default: panic("aarch64: invalid operand 'sa_fbits' for SCVTF")
        }
        if ((sa_fbits >> 3) & 0xf) & ((sa_t__bit_mask >> 1) & 0xf) != (sa_t >> 1) & 0xf {
            panic("aarch64: invalid combination of operands for SCVTF")
        }
        return p.setins(asimdshf(sa_t & 0x1, 0, (sa_fbits >> 3) & 0xf, sa_fbits & 0x7, 28, sa_vn, sa_vd))
    }
    // SCVTF  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SIMDRegister32: sa_v = 0b0
            case SIMDRegister64: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for SCVTF")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 29, sa_n, sa_d))
    }
    // SCVTF  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 0, 29, sa_hn, sa_hd))
    }
    // SCVTF  <V><d>, <V><n>, #<fbits>
    if len(vv) == 1 && isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(vv[0]) && isSameType(v0, v1) {
        var sa_fbits_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SIMDRegister16: sa_v = 0b0010
            case SIMDRegister32: sa_v = 0b0100
            case SIMDRegister64: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SCVTF")
        }
        switch v0.(type) {
            case SIMDRegister16: sa_v__bit_mask = 0b1110
            case SIMDRegister32: sa_v__bit_mask = 0b1100
            case SIMDRegister64: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SCVTF")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b0010: sa_fbits_1 = 32 - uint32(asLit(vv[0]))
            case 0b0100: sa_fbits_1 = 64 - uint32(asLit(vv[0]))
            case 0b1000: sa_fbits_1 = 128 - uint32(asLit(vv[0]))
            default: panic("aarch64: invalid operand 'sa_fbits_1' for SCVTF")
        }
        if ((sa_fbits_1 >> 3) & 0xf) & sa_v__bit_mask != sa_v {
            panic("aarch64: invalid combination of operands for SCVTF")
        }
        return p.setins(asisdshf(0, (sa_fbits_1 >> 3) & 0xf, sa_fbits_1 & 0x7, 28, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SCVTF")
}

// SHL instruction have 2 forms:
//
//   * SHL  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * SHL  <V><d>, <V><n>, #<shift>
//
func (self *Program) SHL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHL", 3, Operands { v0, v1, v2 })
    // SHL  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       isSameSize(v0, v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch (sa_t >> 1) & 0xf {
            case 0b0001: sa_shift = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift = uint32(asLit(v2)) + 32
            case 0b1000: sa_shift = uint32(asLit(v2)) + 64
            default: panic("aarch64: invalid operand 'sa_shift' for SHL")
        }
        if ((sa_shift >> 3) & 0xf) & ((sa_t__bit_mask >> 1) & 0xf) != (sa_t >> 1) & 0xf {
            panic("aarch64: invalid combination of operands for SHL")
        }
        return p.setins(asimdshf(sa_t & 0x1, 0, (sa_shift >> 3) & 0xf, sa_shift & 0x7, 10, sa_vn, sa_vd))
    }
    // SHL  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SIMDRegister64: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SHL")
        }
        switch v0.(type) {
            case SIMDRegister64: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = uint32(asLit(v2)) + 64
            default: panic("aarch64: invalid operand 'sa_shift_1' for SHL")
        }
        if ((sa_shift_1 >> 3) & 0xf) & sa_v__bit_mask != sa_v {
            panic("aarch64: invalid combination of operands for SHL")
        }
        return p.setins(asisdshf(0, (sa_shift_1 >> 3) & 0xf, sa_shift_1 & 0x7, 10, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SHL")
}

// SHLL instruction have one single form:
//
//   * SHLL{2}  <Vd>.<Ta>, <Vn>.<Tb>, #<shift>
//
func (self *Program) SHLL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHLL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isIntLit(v2, 8, 16, 32) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        switch asLit(v2) {
            case 8: sa_shift = 0b00
            case 16: sa_shift = 0b01
            case 32: sa_shift = 0b10
            default: panic("aarch64: invalid operand 'sa_shift' for SHLL")
        }
        if sa_shift != sa_ta || sa_ta != (sa_tb >> 1) & 0x3 {
            panic("aarch64: invalid combination of operands for SHLL")
        }
        return p.setins(asimdmisc(sa_tb & 0x1, 1, sa_shift, 19, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHLL")
}

// SHRN instruction have one single form:
//
//   * SHRN{2}  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) SHRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHRN", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch (sa_tb >> 1) & 0xf {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SHRN")
        }
        if ((sa_shift >> 3) & 0xf) & sa_ta__bit_mask != sa_ta || sa_ta & ((sa_tb__bit_mask >> 1) & 0xf) != (sa_tb >> 1) & 0xf {
            panic("aarch64: invalid combination of operands for SHRN")
        }
        return p.setins(asimdshf(sa_tb & 0x1, 0, (sa_shift >> 3) & 0xf, sa_shift & 0x7, 16, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHRN")
}

// SSHL instruction have 2 forms:
//
//   * SSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * SSHL  <V><d>, <V><n>, <V><m>
//
func (self *Program) SSHL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSHL", 3, Operands { v0, v1, v2 })
    // SSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isSameSize(v0, v1) &&
       isSameSize(v1, v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 0x1, 0, (sa_t >> 1) & 0x3, sa_vm, 8, sa_vn, sa_vd))
    }
    // SSHL  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SIMDRegister64: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for SSHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(0, sa_v, sa_m, 8, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SSHL")
}

// SSHLL instruction have one single form:
//
//   * SSHLL{2}  <Vd>.<Ta>, <Vn>.<Tb>, #<shift>
//
func (self *Program) SSHLL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSHLL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        switch sa_ta {
            case 0b0001: sa_shift = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift = uint32(asLit(v2)) + 32
            default: panic("aarch64: invalid operand 'sa_shift' for SSHLL")
        }
        if ((sa_shift >> 3) & 0xf) & sa_ta__bit_mask != sa_ta || sa_ta & ((sa_tb__bit_mask >> 1) & 0xf) != (sa_tb >> 1) & 0xf {
            panic("aarch64: invalid combination of operands for SSHLL")
        }
        return p.setins(asimdshf(sa_tb & 0x1, 0, (sa_shift >> 3) & 0xf, sa_shift & 0x7, 20, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SSHLL")
}

// SSHR instruction have 2 forms:
//
//   * SSHR  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * SSHR  <V><d>, <V><n>, #<shift>
//
func (self *Program) SSHR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSHR", 3, Operands { v0, v1, v2 })
    // SSHR  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       isSameSize(v0, v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch (sa_t >> 1) & 0xf {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            case 0b1000: sa_shift = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SSHR")
        }
        if ((sa_shift >> 3) & 0xf) & ((sa_t__bit_mask >> 1) & 0xf) != (sa_t >> 1) & 0xf {
            panic("aarch64: invalid combination of operands for SSHR")
        }
        return p.setins(asimdshf(sa_t & 0x1, 0, (sa_shift >> 3) & 0xf, sa_shift & 0x7, 0, sa_vn, sa_vd))
    }
    // SSHR  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SIMDRegister64: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SSHR")
        }
        switch v0.(type) {
            case SIMDRegister64: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SSHR")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for SSHR")
        }
        if ((sa_shift_1 >> 3) & 0xf) & sa_v__bit_mask != sa_v {
            panic("aarch64: invalid combination of operands for SSHR")
        }
        return p.setins(asisdshf(0, (sa_shift_1 >> 3) & 0xf, sa_shift_1 & 0x7, 0, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SSHR")
}

// SSRA instruction have 2 forms:
//
//   * SSRA  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * SSRA  <V><d>, <V><n>, #<shift>
//
func (self *Program) SSRA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSRA", 3, Operands { v0, v1, v2 })
    // SSRA  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       isSameSize(v0, v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch (sa_t >> 1) & 0xf {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            case 0b1000: sa_shift = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SSRA")
        }
        if ((sa_shift >> 3) & 0xf) & ((sa_t__bit_mask >> 1) & 0xf) != (sa_t >> 1) & 0xf {
            panic("aarch64: invalid combination of operands for SSRA")
        }
        return p.setins(asimdshf(sa_t & 0x1, 0, (sa_shift >> 3) & 0xf, sa_shift & 0x7, 2, sa_vn, sa_vd))
    }
    // SSRA  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SIMDRegister64: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SSRA")
        }
        switch v0.(type) {
            case SIMDRegister64: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SSRA")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for SSRA")
        }
        if ((sa_shift_1 >> 3) & 0xf) & sa_v__bit_mask != sa_v {
            panic("aarch64: invalid combination of operands for SSRA")
        }
        return p.setins(asisdshf(0, (sa_shift_1 >> 3) & 0xf, sa_shift_1 & 0x7, 2, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SSRA")
}

// UQXTN instruction have 2 forms:
//
//   * UQXTN{2}  <Vd>.<Tb>, <Vn>.<Ta>
//   * UQXTN  <Vb><d>, <Va><n>
//
func (self *Program) UQXTN(v0, v1 interface{}) *Instruction {
    p := self.alloc("UQXTN", 2, Operands { v0, v1 })
    // UQXTN{2}  <Vd>.<Tb>, <Vn>.<Ta>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        if sa_ta != (sa_tb >> 1) & 0x3 {
            panic("aarch64: invalid combination of operands for UQXTN")
        }
        return p.setins(asimdmisc(sa_tb & 0x1, 1, sa_ta, 20, sa_vn, sa_vd))
    }
    // UQXTN  <Vb><d>, <Va><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) {
        var sa_va uint32
        var sa_vb uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SIMDRegister8: sa_vb = 0b00
            case SIMDRegister16: sa_vb = 0b01
            case SIMDRegister32: sa_vb = 0b10
            default: panic("aarch64: invalid scalar operand size for UQXTN")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case SIMDRegister16: sa_va = 0b00
            case SIMDRegister32: sa_va = 0b01
            case SIMDRegister64: sa_va = 0b10
            default: panic("aarch64: invalid scalar operand size for UQXTN")
        }
        if sa_va != sa_vb {
            panic("aarch64: invalid combination of operands for UQXTN")
        }
        return p.setins(asisdmisc(1, sa_va, 20, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UQXTN")
}

// USRA instruction have 2 forms:
//
//   * USRA  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * USRA  <V><d>, <V><n>, #<shift>
//
func (self *Program) USRA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("USRA", 3, Operands { v0, v1, v2 })
    // USRA  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       isSameSize(v0, v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch (sa_t >> 1) & 0xf {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            case 0b1000: sa_shift = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for USRA")
        }
        if ((sa_shift >> 3) & 0xf) & ((sa_t__bit_mask >> 1) & 0xf) != (sa_t >> 1) & 0xf {
            panic("aarch64: invalid combination of operands for USRA")
        }
        return p.setins(asimdshf(sa_t & 0x1, 1, (sa_shift >> 3) & 0xf, sa_shift & 0x7, 2, sa_vn, sa_vd))
    }
    // USRA  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SIMDRegister64: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for USRA")
        }
        switch v0.(type) {
            case SIMDRegister64: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for USRA")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for USRA")
        }
        if ((sa_shift_1 >> 3) & 0xf) & sa_v__bit_mask != sa_v {
            panic("aarch64: invalid combination of operands for USRA")
        }
        return p.setins(asisdshf(1, (sa_shift_1 >> 3) & 0xf, sa_shift_1 & 0x7, 2, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for USRA")
}
