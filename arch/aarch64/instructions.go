// Code generated by "mkasm_aarch64.py", DO NOT EDIT.

package aarch64

import (
    `github.com/chenzhuoyu/iasm/asm`
)

const (
    _N_args = 6
)

// ABS instruction have 4 forms:
//
//   * ABS  <Wd>, <Wn>
//   * ABS  <Xd>, <Xn>
//   * ABS  <Vd>.<T>, <Vn>.<T>
//   * ABS  <V><d>, <V><n>
//
func (self *Program) ABS(v0, v1 interface{}) *Instruction {
    p := self.alloc("ABS", 2, Operands { v0, v1 })
    // ABS  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 8, sa_wn, sa_wd))
    }
    // ABS  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 8, sa_xn, sa_xd))
    }
    // ABS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 0, maskp(sa_t, 1, 2), 11, sa_vn, sa_vd))
    }
    // ABS  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for ABS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmisc(0, sa_v, 11, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ABS")
}

// ADC instruction have 2 forms:
//
//   * ADC  <Wd>, <Wn>, <Wm>
//   * ADC  <Xd>, <Xn>, <Xm>
//
func (self *Program) ADC(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ADC", 3, Operands { v0, v1, v2 })
    // ADC  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(0, 0, 0, sa_wm, sa_wn, sa_wd))
    }
    // ADC  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(1, 0, 0, sa_xm, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADC")
}

// ADCS instruction have 2 forms:
//
//   * ADCS  <Wd>, <Wn>, <Wm>
//   * ADCS  <Xd>, <Xn>, <Xm>
//
func (self *Program) ADCS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ADCS", 3, Operands { v0, v1, v2 })
    // ADCS  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(0, 0, 1, sa_wm, sa_wn, sa_wd))
    }
    // ADCS  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(1, 0, 1, sa_xm, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADCS")
}

// ADD instruction have 8 forms:
//
//   * ADD  <Wd|WSP>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
//   * ADD  <Wd|WSP>, <Wn|WSP>, #<imm>{, <shift>}
//   * ADD  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ADD  <Xd|SP>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
//   * ADD  <Xd|SP>, <Xn|SP>, #<imm>{, <shift>}
//   * ADD  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//   * ADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * ADD  <V><d>, <V><n>, <V><m>
//
func (self *Program) ADD(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ADD", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ADD", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ADD takes 3 or 4 operands")
    }
    // ADD  <Wd|WSP>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) &&
       isWrOrWSP(v0) &&
       isWrOrWSP(v1) &&
       isWr(v2) &&
       (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_extend = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_ext(0, 0, 0, 0, sa_wm, sa_extend, sa_amount, sa_wn_wsp, sa_wd_wsp))
    }
    // ADD  <Wd|WSP>, <Wn|WSP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isWrOrWSP(v0) &&
       isWrOrWSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(0, 0, 0, sa_shift, sa_imm, sa_wn_wsp, sa_wd_wsp))
    }
    // ADD  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(0, 0, 0, sa_shift, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ADD  <Xd|SP>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) &&
       isXrOrSP(v0) &&
       isXrOrSP(v1) &&
       isWrOrXr(v2) &&
       (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend_1 uint32
        var sa_r [4]uint32
        var sa_r__bit_mask [4]uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        switch true {
            case isWr(v2): sa_r = [4]uint32{0b000, 0b010, 0b100, 0b110}
            case isXr(v2): sa_r = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        switch true {
            case isWr(v2): sa_r__bit_mask = [4]uint32{0b110, 0b111, 0b110, 0b111}
            case isXr(v2): sa_r__bit_mask = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        if len(vv) == 1 {
            sa_extend_1 = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        if !matchany(sa_extend_1, &sa_r[0], &sa_r__bit_mask[0], 4) {
            panic("aarch64: invalid combination of operands for ADD")
        }
        return p.setins(addsub_ext(1, 0, 0, 0, sa_m, sa_extend_1, sa_amount, sa_xn_sp, sa_xd_sp))
    }
    // ADD  <Xd|SP>, <Xn|SP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isXrOrSP(v0) &&
       isXrOrSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(1, 0, 0, sa_shift, sa_imm, sa_xn_sp, sa_xd_sp))
    }
    // ADD  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(1, 0, 0, sa_shift, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // ADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 16, sa_vn, sa_vd))
    }
    // ADD  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for ADD")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(0, sa_v, sa_m, 16, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADD")
}

// ADDG instruction have one single form:
//
//   * ADDG  <Xd|SP>, <Xn|SP>, #<uimm6>, #<uimm4>
//
func (self *Program) ADDG(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("ADDG", 4, Operands { v0, v1, v2, v3 })
    if isXrOrSP(v0) && isXrOrSP(v1) && isUimm6(v2) && isUimm4(v3) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_uimm6 := asUimm6(v2)
        sa_uimm4 := asUimm4(v3)
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd_sp
        return p.setins(addsub_immtags(1, 0, 0, sa_uimm6, 0, sa_uimm4, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ADDG")
}

// ADDHN instruction have one single form:
//
//   * ADDHN  <Vd>.<Tb>, <Vn>.<Ta>, <Vm>.<Ta>
//
func (self *Program) ADDHN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ADDHN", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8H, Vec4S, Vec2D) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for ADDHN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for ADDHN")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 4, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ADDHN")
}

// ADDHN2 instruction have one single form:
//
//   * ADDHN2  <Vd>.<Tb>, <Vn>.<Ta>, <Vm>.<Ta>
//
func (self *Program) ADDHN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ADDHN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8H, Vec4S, Vec2D) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for ADDHN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for ADDHN2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 4, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ADDHN2")
}

// ADDP instruction have 2 forms:
//
//   * ADDP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * ADDP  <V><d>, <Vn>.<T>
//
func (self *Program) ADDP(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ADDP", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("ADDP", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction ADDP takes 2 or 3 operands")
    }
    // ADDP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 23, sa_vn, sa_vd))
    }
    // ADDP  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && vfmt(v1) == Vec2D {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for ADDP")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        if sa_t != sa_v {
            panic("aarch64: invalid combination of operands for ADDP")
        }
        return p.setins(asisdpair(0, sa_t, 27, sa_vn, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADDP")
}

// ADDS instruction have 6 forms:
//
//   * ADDS  <Wd>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
//   * ADDS  <Wd>, <Wn|WSP>, #<imm>{, <shift>}
//   * ADDS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ADDS  <Xd>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
//   * ADDS  <Xd>, <Xn|SP>, #<imm>{, <shift>}
//   * ADDS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) ADDS(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ADDS", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ADDS", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ADDS takes 3 or 4 operands")
    }
    // ADDS  <Wd>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWrOrWSP(v1) && isWr(v2) && (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_extend = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_ext(0, 0, 1, 0, sa_wm, sa_extend, sa_amount, sa_wn_wsp, sa_wd))
    }
    // ADDS  <Wd>, <Wn|WSP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isWr(v0) &&
       isWrOrWSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(0, 0, 1, sa_shift, sa_imm, sa_wn_wsp, sa_wd))
    }
    // ADDS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(0, 0, 1, sa_shift, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ADDS  <Xd>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXrOrSP(v1) && isWrOrXr(v2) && (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend_1 uint32
        var sa_r [4]uint32
        var sa_r__bit_mask [4]uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        switch true {
            case isWr(v2): sa_r = [4]uint32{0b000, 0b010, 0b100, 0b110}
            case isXr(v2): sa_r = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        switch true {
            case isWr(v2): sa_r__bit_mask = [4]uint32{0b110, 0b111, 0b110, 0b111}
            case isXr(v2): sa_r__bit_mask = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        if len(vv) == 1 {
            sa_extend_1 = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        if !matchany(sa_extend_1, &sa_r[0], &sa_r__bit_mask[0], 4) {
            panic("aarch64: invalid combination of operands for ADDS")
        }
        return p.setins(addsub_ext(1, 0, 1, 0, sa_m, sa_extend_1, sa_amount, sa_xn_sp, sa_xd))
    }
    // ADDS  <Xd>, <Xn|SP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isXr(v0) &&
       isXrOrSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(1, 0, 1, sa_shift, sa_imm, sa_xn_sp, sa_xd))
    }
    // ADDS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(1, 0, 1, sa_shift, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADDS")
}

// ADDV instruction have one single form:
//
//   * ADDV  <V><d>, <Vn>.<T>
//
func (self *Program) ADDV(v0, v1 interface{}) *Instruction {
    p := self.alloc("ADDV", 2, Operands { v0, v1 })
    if isAdvSIMD(v0) && isVr(v1) && isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec4S) {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for ADDV")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        if maskp(sa_t, 1, 2) != sa_v {
            panic("aarch64: invalid combination of operands for ADDV")
        }
        return p.setins(asimdall(sa_t & 1, 0, maskp(sa_t, 1, 2), 27, sa_vn, sa_d))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ADDV")
}

// ADR instruction have one single form:
//
//   * ADR  <Xd>, <label>
//
func (self *Program) ADR(v0, v1 interface{}) *Instruction {
    p := self.alloc("ADR", 2, Operands { v0, v1 })
    if isXr(v0) && isLabel(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 {
            delta := uint32(sa_label.RelativeTo(pc))
            return pcreladdr(0, delta & 3, maskp(delta, 2, 19), sa_xd)
        })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ADR")
}

// ADRP instruction have one single form:
//
//   * ADRP  <Xd>, <label>
//
func (self *Program) ADRP(v0, v1 interface{}) *Instruction {
    p := self.alloc("ADRP", 2, Operands { v0, v1 })
    if isXr(v0) && isLabel(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 {
            delta := uint32(sa_label.RelativeTo(pc))
            return pcreladdr(1, delta & 3, maskp(delta, 2, 19), sa_xd)
        })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ADRP")
}

// AESD instruction have one single form:
//
//   * AESD  <Vd>.16B, <Vn>.16B
//
func (self *Program) AESD(v0, v1 interface{}) *Instruction {
    p := self.alloc("AESD", 2, Operands { v0, v1 })
    if isVr(v0) && vfmt(v0) == Vec16B && isVr(v1) && vfmt(v1) == Vec16B {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(cryptoaes(0, 5, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AESD")
}

// AESE instruction have one single form:
//
//   * AESE  <Vd>.16B, <Vn>.16B
//
func (self *Program) AESE(v0, v1 interface{}) *Instruction {
    p := self.alloc("AESE", 2, Operands { v0, v1 })
    if isVr(v0) && vfmt(v0) == Vec16B && isVr(v1) && vfmt(v1) == Vec16B {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(cryptoaes(0, 4, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AESE")
}

// AESIMC instruction have one single form:
//
//   * AESIMC  <Vd>.16B, <Vn>.16B
//
func (self *Program) AESIMC(v0, v1 interface{}) *Instruction {
    p := self.alloc("AESIMC", 2, Operands { v0, v1 })
    if isVr(v0) && vfmt(v0) == Vec16B && isVr(v1) && vfmt(v1) == Vec16B {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(cryptoaes(0, 7, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AESIMC")
}

// AESMC instruction have one single form:
//
//   * AESMC  <Vd>.16B, <Vn>.16B
//
func (self *Program) AESMC(v0, v1 interface{}) *Instruction {
    p := self.alloc("AESMC", 2, Operands { v0, v1 })
    if isVr(v0) && vfmt(v0) == Vec16B && isVr(v1) && vfmt(v1) == Vec16B {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(cryptoaes(0, 6, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AESMC")
}

// AND instruction have 5 forms:
//
//   * AND  <Wd|WSP>, <Wn>, #<imm>
//   * AND  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * AND  <Xd|SP>, <Xn>, #<imm>
//   * AND  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//   * AND  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) AND(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("AND", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("AND", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction AND takes 3 or 4 operands")
    }
    // AND  <Wd|WSP>, <Wn>, #<imm>
    if isWrOrWSP(v0) && isWr(v1) && isMask32(v2) {
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_imm := asMaskOp(v2)
        return p.setins(log_imm(0, 0, 0, maskp(sa_imm, 6, 6), mask(sa_imm, 6), sa_wn, sa_wd_wsp))
    }
    // AND  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 0, sa_shift, 0, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // AND  <Xd|SP>, <Xn>, #<imm>
    if isXrOrSP(v0) && isXr(v1) && isMask64(v2) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_imm_1 := asMaskOp(v2)
        return p.setins(log_imm(1, 0, maskp(sa_imm_1, 12, 1), maskp(sa_imm_1, 6, 6), mask(sa_imm_1, 6), sa_xn, sa_xd_sp))
    }
    // AND  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 0, sa_shift, 0, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // AND  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t, 0, 0, sa_vm, 3, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for AND")
}

// ANDS instruction have 4 forms:
//
//   * ANDS  <Wd>, <Wn>, #<imm>
//   * ANDS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ANDS  <Xd>, <Xn>, #<imm>
//   * ANDS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) ANDS(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ANDS", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ANDS", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ANDS takes 3 or 4 operands")
    }
    // ANDS  <Wd>, <Wn>, #<imm>
    if isWr(v0) && isWr(v1) && isMask32(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_imm := asMaskOp(v2)
        return p.setins(log_imm(0, 3, 0, maskp(sa_imm, 6, 6), mask(sa_imm, 6), sa_wn, sa_wd))
    }
    // ANDS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 3, sa_shift, 0, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ANDS  <Xd>, <Xn>, #<imm>
    if isXr(v0) && isXr(v1) && isMask64(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_imm_1 := asMaskOp(v2)
        return p.setins(log_imm(1, 3, maskp(sa_imm_1, 12, 1), maskp(sa_imm_1, 6, 6), mask(sa_imm_1, 6), sa_xn, sa_xd))
    }
    // ANDS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 3, sa_shift, 0, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ANDS")
}

// ASRV instruction have 2 forms:
//
//   * ASRV  <Wd>, <Wn>, <Wm>
//   * ASRV  <Xd>, <Xn>, <Xm>
//
func (self *Program) ASRV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ASRV", 3, Operands { v0, v1, v2 })
    // ASRV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 10, sa_wn, sa_wd))
    }
    // ASRV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 10, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ASRV")
}

// AUTDA instruction have one single form:
//
//   * AUTDA  <Xd>, <Xn|SP>
//
func (self *Program) AUTDA(v0, v1 interface{}) *Instruction {
    p := self.alloc("AUTDA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 6, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTDA")
}

// AUTDB instruction have one single form:
//
//   * AUTDB  <Xd>, <Xn|SP>
//
func (self *Program) AUTDB(v0, v1 interface{}) *Instruction {
    p := self.alloc("AUTDB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 7, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTDB")
}

// AUTDZA instruction have one single form:
//
//   * AUTDZA  <Xd>
//
func (self *Program) AUTDZA(v0 interface{}) *Instruction {
    p := self.alloc("AUTDZA", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 14, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTDZA")
}

// AUTDZB instruction have one single form:
//
//   * AUTDZB  <Xd>
//
func (self *Program) AUTDZB(v0 interface{}) *Instruction {
    p := self.alloc("AUTDZB", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 15, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTDZB")
}

// AUTIA instruction have one single form:
//
//   * AUTIA  <Xd>, <Xn|SP>
//
func (self *Program) AUTIA(v0, v1 interface{}) *Instruction {
    p := self.alloc("AUTIA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 4, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTIA")
}

// AUTIA1716 instruction have one single form:
//
//   * AUTIA1716
//
func (self *Program) AUTIA1716() *Instruction {
    p := self.alloc("AUTIA1716", 0, Operands {})
    return p.setins(hints(1, 4))
}

// AUTIASP instruction have one single form:
//
//   * AUTIASP
//
func (self *Program) AUTIASP() *Instruction {
    p := self.alloc("AUTIASP", 0, Operands {})
    return p.setins(hints(3, 5))
}

// AUTIAZ instruction have one single form:
//
//   * AUTIAZ
//
func (self *Program) AUTIAZ() *Instruction {
    p := self.alloc("AUTIAZ", 0, Operands {})
    return p.setins(hints(3, 4))
}

// AUTIB instruction have one single form:
//
//   * AUTIB  <Xd>, <Xn|SP>
//
func (self *Program) AUTIB(v0, v1 interface{}) *Instruction {
    p := self.alloc("AUTIB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 5, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTIB")
}

// AUTIB1716 instruction have one single form:
//
//   * AUTIB1716
//
func (self *Program) AUTIB1716() *Instruction {
    p := self.alloc("AUTIB1716", 0, Operands {})
    return p.setins(hints(1, 6))
}

// AUTIBSP instruction have one single form:
//
//   * AUTIBSP
//
func (self *Program) AUTIBSP() *Instruction {
    p := self.alloc("AUTIBSP", 0, Operands {})
    return p.setins(hints(3, 7))
}

// AUTIBZ instruction have one single form:
//
//   * AUTIBZ
//
func (self *Program) AUTIBZ() *Instruction {
    p := self.alloc("AUTIBZ", 0, Operands {})
    return p.setins(hints(3, 6))
}

// AUTIZA instruction have one single form:
//
//   * AUTIZA  <Xd>
//
func (self *Program) AUTIZA(v0 interface{}) *Instruction {
    p := self.alloc("AUTIZA", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 12, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTIZA")
}

// AUTIZB instruction have one single form:
//
//   * AUTIZB  <Xd>
//
func (self *Program) AUTIZB(v0 interface{}) *Instruction {
    p := self.alloc("AUTIZB", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 13, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTIZB")
}

// AXFLAG instruction have one single form:
//
//   * AXFLAG
//
func (self *Program) AXFLAG() *Instruction {
    p := self.alloc("AXFLAG", 0, Operands {})
    return p.setins(pstate(0, 0, 2, 31))
}

// B instruction have one single form:
//
//   * B  <label>
//
func (self *Program) B(v0 interface{}) *Instruction {
    p := self.alloc("B", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return branch_imm(0, uint32(sa_label.RelativeTo(pc))) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for B")
}

// BEQ instruction have one single form:
//
//   * B.eq  <label>
//
func (self *Program) BEQ(v0 interface{}) *Instruction {
    p := self.alloc("BEQ", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 0) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BEQ")
}

// BNE instruction have one single form:
//
//   * B.ne  <label>
//
func (self *Program) BNE(v0 interface{}) *Instruction {
    p := self.alloc("BNE", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 1) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BNE")
}

// BCS instruction have one single form:
//
//   * B.cs  <label>
//
func (self *Program) BCS(v0 interface{}) *Instruction {
    p := self.alloc("BCS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 2) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCS")
}

// BHS instruction have one single form:
//
//   * B.hs  <label>
//
func (self *Program) BHS(v0 interface{}) *Instruction {
    p := self.alloc("BHS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 2) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BHS")
}

// BCC instruction have one single form:
//
//   * B.cc  <label>
//
func (self *Program) BCC(v0 interface{}) *Instruction {
    p := self.alloc("BCC", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 3) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCC")
}

// BLO instruction have one single form:
//
//   * B.lo  <label>
//
func (self *Program) BLO(v0 interface{}) *Instruction {
    p := self.alloc("BLO", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 3) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLO")
}

// BMI instruction have one single form:
//
//   * B.mi  <label>
//
func (self *Program) BMI(v0 interface{}) *Instruction {
    p := self.alloc("BMI", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 4) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BMI")
}

// BPL instruction have one single form:
//
//   * B.pl  <label>
//
func (self *Program) BPL(v0 interface{}) *Instruction {
    p := self.alloc("BPL", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 5) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BPL")
}

// BVS instruction have one single form:
//
//   * B.vs  <label>
//
func (self *Program) BVS(v0 interface{}) *Instruction {
    p := self.alloc("BVS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 6) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BVS")
}

// BVC instruction have one single form:
//
//   * B.vc  <label>
//
func (self *Program) BVC(v0 interface{}) *Instruction {
    p := self.alloc("BVC", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 7) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BVC")
}

// BHI instruction have one single form:
//
//   * B.hi  <label>
//
func (self *Program) BHI(v0 interface{}) *Instruction {
    p := self.alloc("BHI", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 8) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BHI")
}

// BLS instruction have one single form:
//
//   * B.ls  <label>
//
func (self *Program) BLS(v0 interface{}) *Instruction {
    p := self.alloc("BLS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 9) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLS")
}

// BGE instruction have one single form:
//
//   * B.ge  <label>
//
func (self *Program) BGE(v0 interface{}) *Instruction {
    p := self.alloc("BGE", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 10) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BGE")
}

// BLT instruction have one single form:
//
//   * B.lt  <label>
//
func (self *Program) BLT(v0 interface{}) *Instruction {
    p := self.alloc("BLT", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 11) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLT")
}

// BGT instruction have one single form:
//
//   * B.gt  <label>
//
func (self *Program) BGT(v0 interface{}) *Instruction {
    p := self.alloc("BGT", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 12) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BGT")
}

// BLE instruction have one single form:
//
//   * B.le  <label>
//
func (self *Program) BLE(v0 interface{}) *Instruction {
    p := self.alloc("BLE", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 13) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLE")
}

// BAL instruction have one single form:
//
//   * B.al  <label>
//
func (self *Program) BAL(v0 interface{}) *Instruction {
    p := self.alloc("BAL", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 14) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BAL")
}

// BCEQ instruction have one single form:
//
//   * BC.eq  <label>
//
func (self *Program) BCEQ(v0 interface{}) *Instruction {
    p := self.alloc("BCEQ", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 0) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCEQ")
}

// BCNE instruction have one single form:
//
//   * BC.ne  <label>
//
func (self *Program) BCNE(v0 interface{}) *Instruction {
    p := self.alloc("BCNE", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 1) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCNE")
}

// BCCS instruction have one single form:
//
//   * BC.cs  <label>
//
func (self *Program) BCCS(v0 interface{}) *Instruction {
    p := self.alloc("BCCS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 2) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCCS")
}

// BCHS instruction have one single form:
//
//   * BC.hs  <label>
//
func (self *Program) BCHS(v0 interface{}) *Instruction {
    p := self.alloc("BCHS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 2) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCHS")
}

// BCCC instruction have one single form:
//
//   * BC.cc  <label>
//
func (self *Program) BCCC(v0 interface{}) *Instruction {
    p := self.alloc("BCCC", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 3) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCCC")
}

// BCLO instruction have one single form:
//
//   * BC.lo  <label>
//
func (self *Program) BCLO(v0 interface{}) *Instruction {
    p := self.alloc("BCLO", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 3) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCLO")
}

// BCMI instruction have one single form:
//
//   * BC.mi  <label>
//
func (self *Program) BCMI(v0 interface{}) *Instruction {
    p := self.alloc("BCMI", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 4) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCMI")
}

// BCPL instruction have one single form:
//
//   * BC.pl  <label>
//
func (self *Program) BCPL(v0 interface{}) *Instruction {
    p := self.alloc("BCPL", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 5) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCPL")
}

// BCVS instruction have one single form:
//
//   * BC.vs  <label>
//
func (self *Program) BCVS(v0 interface{}) *Instruction {
    p := self.alloc("BCVS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 6) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCVS")
}

// BCVC instruction have one single form:
//
//   * BC.vc  <label>
//
func (self *Program) BCVC(v0 interface{}) *Instruction {
    p := self.alloc("BCVC", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 7) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCVC")
}

// BCHI instruction have one single form:
//
//   * BC.hi  <label>
//
func (self *Program) BCHI(v0 interface{}) *Instruction {
    p := self.alloc("BCHI", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 8) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCHI")
}

// BCLS instruction have one single form:
//
//   * BC.ls  <label>
//
func (self *Program) BCLS(v0 interface{}) *Instruction {
    p := self.alloc("BCLS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 9) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCLS")
}

// BCGE instruction have one single form:
//
//   * BC.ge  <label>
//
func (self *Program) BCGE(v0 interface{}) *Instruction {
    p := self.alloc("BCGE", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 10) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCGE")
}

// BCLT instruction have one single form:
//
//   * BC.lt  <label>
//
func (self *Program) BCLT(v0 interface{}) *Instruction {
    p := self.alloc("BCLT", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 11) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCLT")
}

// BCGT instruction have one single form:
//
//   * BC.gt  <label>
//
func (self *Program) BCGT(v0 interface{}) *Instruction {
    p := self.alloc("BCGT", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 12) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCGT")
}

// BCLE instruction have one single form:
//
//   * BC.le  <label>
//
func (self *Program) BCLE(v0 interface{}) *Instruction {
    p := self.alloc("BCLE", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 13) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCLE")
}

// BCAL instruction have one single form:
//
//   * BC.al  <label>
//
func (self *Program) BCAL(v0 interface{}) *Instruction {
    p := self.alloc("BCAL", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 14) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCAL")
}

// BCAX instruction have one single form:
//
//   * BCAX  <Vd>.16B, <Vn>.16B, <Vm>.16B, <Va>.16B
//
func (self *Program) BCAX(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("BCAX", 4, Operands { v0, v1, v2, v3 })
    if isVr(v0) &&
       vfmt(v0) == Vec16B &&
       isVr(v1) &&
       vfmt(v1) == Vec16B &&
       isVr(v2) &&
       vfmt(v2) == Vec16B &&
       isVr(v3) &&
       vfmt(v3) == Vec16B {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        sa_va := uint32(v3.(asm.Register).ID())
        return p.setins(crypto4(1, sa_vm, sa_va, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCAX")
}

// BFCVT instruction have one single form:
//
//   * BFCVT  <Hd>, <Sn>
//
func (self *Program) BFCVT(v0, v1 interface{}) *Instruction {
    p := self.alloc("BFCVT", 2, Operands { v0, v1 })
    if isHr(v0) && isSr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 6, sa_sn, sa_hd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BFCVT")
}

// BFCVTN instruction have one single form:
//
//   * BFCVTN  <Vd>.<Ta>, <Vn>.4S
//
func (self *Program) BFCVTN(v0, v1 interface{}) *Instruction {
    p := self.alloc("BFCVTN", 2, Operands { v0, v1 })
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && vfmt(v1) == Vec4S {
        var sa_ta uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_ta = 0b0
            case Vec8H: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        if sa_ta != 0 {
            panic("aarch64: invalid combination of operands for BFCVTN")
        }
        return p.setins(asimdmisc(0, 0, 2, 22, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BFCVTN")
}

// BFCVTN2 instruction have one single form:
//
//   * BFCVTN2  <Vd>.<Ta>, <Vn>.4S
//
func (self *Program) BFCVTN2(v0, v1 interface{}) *Instruction {
    p := self.alloc("BFCVTN2", 2, Operands { v0, v1 })
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && vfmt(v1) == Vec4S {
        var sa_ta uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_ta = 0b0
            case Vec8H: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        if sa_ta != 1 {
            panic("aarch64: invalid combination of operands for BFCVTN2")
        }
        return p.setins(asimdmisc(1, 0, 2, 22, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BFCVTN2")
}

// BFDOT instruction have 2 forms:
//
//   * BFDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.2H[<index>]
//   * BFDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) BFDOT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("BFDOT", 3, Operands { v0, v1, v2 })
    // BFDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.2H[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVri(v2) &&
       vmoder(v2) == Mode2H {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b0
            case Vec8H: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for BFDOT")
        }
        return p.setins(asimdelem(
            sa_ta,
            0,
            1,
            sa_index & 1,
            maskp(sa_vm, 4, 1),
            mask(sa_vm, 4),
            15,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // BFDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b0
            case Vec8H: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for BFDOT")
        }
        return p.setins(asimdsame2(sa_ta, 1, 1, sa_vm, 15, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for BFDOT")
}

// BFM instruction have 2 forms:
//
//   * BFM  <Wd>, <Wn>, #<immr>, #<imms>
//   * BFM  <Xd>, <Xn>, #<immr>, #<imms>
//
func (self *Program) BFM(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("BFM", 4, Operands { v0, v1, v2, v3 })
    // BFM  <Wd>, <Wn>, #<immr>, #<imms>
    if isWr(v0) && isWr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_immr := asUimm6(v2)
        sa_imms := asUimm6(v3)
        return p.setins(bitfield(0, 1, 0, sa_immr, sa_imms, sa_wn, sa_wd))
    }
    // BFM  <Xd>, <Xn>, #<immr>, #<imms>
    if isXr(v0) && isXr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_immr_1 := asUimm6(v2)
        sa_imms_1 := asUimm6(v3)
        return p.setins(bitfield(1, 1, 1, sa_immr_1, sa_imms_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for BFM")
}

// BFMLALB instruction have 2 forms:
//
//   * BFMLALB  <Vd>.4S, <Vn>.8H, <Vm>.H[<index>]
//   * BFMLALB  <Vd>.4S, <Vn>.8H, <Vm>.8H
//
func (self *Program) BFMLALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("BFMLALB", 3, Operands { v0, v1, v2 })
    // BFMLALB  <Vd>.4S, <Vn>.8H, <Vm>.H[<index>]
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec8H && isVri(v2) && vmoder(v2) == ModeH {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        return p.setins(asimdelem(
            0,
            0,
            3,
            maskp(sa_index, 1, 1),
            sa_index & 1,
            sa_vm,
            15,
            maskp(sa_index, 2, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // BFMLALB  <Vd>.4S, <Vn>.8H, <Vm>.8H
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec8H && isVr(v2) && vfmt(v2) == Vec8H {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame2(0, 1, 3, sa_vm, 15, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for BFMLALB")
}

// BFMLALT instruction have 2 forms:
//
//   * BFMLALT  <Vd>.4S, <Vn>.8H, <Vm>.H[<index>]
//   * BFMLALT  <Vd>.4S, <Vn>.8H, <Vm>.8H
//
func (self *Program) BFMLALT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("BFMLALT", 3, Operands { v0, v1, v2 })
    // BFMLALT  <Vd>.4S, <Vn>.8H, <Vm>.H[<index>]
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec8H && isVri(v2) && vmoder(v2) == ModeH {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        return p.setins(asimdelem(
            1,
            0,
            3,
            maskp(sa_index, 1, 1),
            sa_index & 1,
            sa_vm,
            15,
            maskp(sa_index, 2, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // BFMLALT  <Vd>.4S, <Vn>.8H, <Vm>.8H
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec8H && isVr(v2) && vfmt(v2) == Vec8H {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame2(1, 1, 3, sa_vm, 15, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for BFMLALT")
}

// BFMMLA instruction have one single form:
//
//   * BFMMLA  <Vd>.4S, <Vn>.8H, <Vm>.8H
//
func (self *Program) BFMMLA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("BFMMLA", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec8H && isVr(v2) && vfmt(v2) == Vec8H {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame2(1, 1, 1, sa_vm, 13, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BFMMLA")
}

// BIC instruction have 5 forms:
//
//   * BIC  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * BIC  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//   * BIC  <Vd>.<T>, #<imm8>{, LSL #<amount>}
//   * BIC  <Vd>.<T>, #<imm8>{, LSL #<amount>}
//   * BIC  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) BIC(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("BIC", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("BIC", 3, Operands { v0, v1, vv[0] })
        case 2  : p = self.alloc("BIC", 4, Operands { v0, v1, vv[0], vv[1] })
        default : panic("instruction BIC takes 2 or 3 or 4 operands")
    }
    // BIC  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 1 || len(vv) == 2) && isWr(v0) && isWr(v1) && isWr(vv[0]) && (len(vv) == 0 || isShift(vv[1])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(vv[0].(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[1].(ShiftType).ShiftType())
            sa_amount = uint32(vv[1].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 0, sa_shift, 1, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // BIC  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 1 || len(vv) == 2) && isXr(v0) && isXr(v1) && isXr(vv[0]) && (len(vv) == 0 || isShift(vv[1])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(vv[0].(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[1].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[1].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 0, sa_shift, 1, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // BIC  <Vd>.<T>, #<imm8>{, LSL #<amount>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isUimm8(v1) &&
       (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_amount uint32
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        if len(vv) == 1 {
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        cmode := uint32(0b1001)
        switch sa_amount {
            case 0: cmode |= 0b0 << 1
            case 8: cmode |= 0b1 << 1
            default: panic("aarch64: invalid combination of operands for BIC")
        }
        return p.setins(asimdimm(
            sa_t,
            1,
            maskp(sa_imm8, 7, 1),
            maskp(sa_imm8, 6, 1),
            maskp(sa_imm8, 5, 1),
            cmode,
            0,
            maskp(sa_imm8, 4, 1),
            maskp(sa_imm8, 3, 1),
            maskp(sa_imm8, 2, 1),
            maskp(sa_imm8, 1, 1),
            sa_imm8 & 1,
            sa_vd,
        ))
    }
    // BIC  <Vd>.<T>, #<imm8>{, LSL #<amount>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isUimm8(v1) &&
       (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_amount_1 uint32
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t_1 = 0b0
            case Vec4S: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        if len(vv) == 1 {
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        cmode := uint32(0b0001)
        switch sa_amount_1 {
            case 0: cmode |= 0b00 << 1
            case 8: cmode |= 0b01 << 1
            case 16: cmode |= 0b10 << 1
            case 24: cmode |= 0b11 << 1
            default: panic("aarch64: invalid combination of operands for BIC")
        }
        return p.setins(asimdimm(
            sa_t_1,
            1,
            maskp(sa_imm8, 7, 1),
            maskp(sa_imm8, 6, 1),
            maskp(sa_imm8, 5, 1),
            cmode,
            0,
            maskp(sa_imm8, 4, 1),
            maskp(sa_imm8, 3, 1),
            maskp(sa_imm8, 2, 1),
            maskp(sa_imm8, 1, 1),
            sa_imm8 & 1,
            sa_vd,
        ))
    }
    // BIC  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        return p.setins(asimdsame(sa_t, 0, 1, sa_vm, 3, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for BIC")
}

// BICS instruction have 2 forms:
//
//   * BICS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * BICS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) BICS(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("BICS", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("BICS", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction BICS takes 3 or 4 operands")
    }
    // BICS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 3, sa_shift, 1, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // BICS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 3, sa_shift, 1, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for BICS")
}

// BIF instruction have one single form:
//
//   * BIF  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) BIF(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("BIF", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t, 1, 3, sa_vm, 3, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BIF")
}

// BIT instruction have one single form:
//
//   * BIT  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) BIT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("BIT", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t, 1, 2, sa_vm, 3, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BIT")
}

// BL instruction have one single form:
//
//   * BL  <label>
//
func (self *Program) BL(v0 interface{}) *Instruction {
    p := self.alloc("BL", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return branch_imm(1, uint32(sa_label.RelativeTo(pc))) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BL")
}

// BLR instruction have one single form:
//
//   * BLR  <Xn>
//
func (self *Program) BLR(v0 interface{}) *Instruction {
    p := self.alloc("BLR", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(1, 31, 0, sa_xn, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLR")
}

// BLRAA instruction have one single form:
//
//   * BLRAA  <Xn>, <Xm|SP>
//
func (self *Program) BLRAA(v0, v1 interface{}) *Instruction {
    p := self.alloc("BLRAA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm_sp := uint32(v1.(asm.Register).ID())
        op4 := uint32(0b00000)
        op4 |= sa_xm_sp
        return p.setins(branch_reg(9, 31, 2, sa_xn, op4))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLRAA")
}

// BLRAAZ instruction have one single form:
//
//   * BLRAAZ  <Xn>
//
func (self *Program) BLRAAZ(v0 interface{}) *Instruction {
    p := self.alloc("BLRAAZ", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(1, 31, 2, sa_xn, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLRAAZ")
}

// BLRAB instruction have one single form:
//
//   * BLRAB  <Xn>, <Xm|SP>
//
func (self *Program) BLRAB(v0, v1 interface{}) *Instruction {
    p := self.alloc("BLRAB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm_sp := uint32(v1.(asm.Register).ID())
        op4 := uint32(0b00000)
        op4 |= sa_xm_sp
        return p.setins(branch_reg(9, 31, 3, sa_xn, op4))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLRAB")
}

// BLRABZ instruction have one single form:
//
//   * BLRABZ  <Xn>
//
func (self *Program) BLRABZ(v0 interface{}) *Instruction {
    p := self.alloc("BLRABZ", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(1, 31, 3, sa_xn, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLRABZ")
}

// BR instruction have one single form:
//
//   * BR  <Xn>
//
func (self *Program) BR(v0 interface{}) *Instruction {
    p := self.alloc("BR", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(0, 31, 0, sa_xn, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BR")
}

// BRAA instruction have one single form:
//
//   * BRAA  <Xn>, <Xm|SP>
//
func (self *Program) BRAA(v0, v1 interface{}) *Instruction {
    p := self.alloc("BRAA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm_sp := uint32(v1.(asm.Register).ID())
        op4 := uint32(0b00000)
        op4 |= sa_xm_sp
        return p.setins(branch_reg(8, 31, 2, sa_xn, op4))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRAA")
}

// BRAAZ instruction have one single form:
//
//   * BRAAZ  <Xn>
//
func (self *Program) BRAAZ(v0 interface{}) *Instruction {
    p := self.alloc("BRAAZ", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(0, 31, 2, sa_xn, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRAAZ")
}

// BRAB instruction have one single form:
//
//   * BRAB  <Xn>, <Xm|SP>
//
func (self *Program) BRAB(v0, v1 interface{}) *Instruction {
    p := self.alloc("BRAB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm_sp := uint32(v1.(asm.Register).ID())
        op4 := uint32(0b00000)
        op4 |= sa_xm_sp
        return p.setins(branch_reg(8, 31, 3, sa_xn, op4))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRAB")
}

// BRABZ instruction have one single form:
//
//   * BRABZ  <Xn>
//
func (self *Program) BRABZ(v0 interface{}) *Instruction {
    p := self.alloc("BRABZ", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(0, 31, 3, sa_xn, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRABZ")
}

// BRK instruction have one single form:
//
//   * BRK  #<imm>
//
func (self *Program) BRK(v0 interface{}) *Instruction {
    p := self.alloc("BRK", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(1, sa_imm, 0, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRK")
}

// BSL instruction have one single form:
//
//   * BSL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) BSL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("BSL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t, 1, 1, sa_vm, 3, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BSL")
}

// BTI instruction have one single form:
//
//   * BTI  {<targets>}
//
func (self *Program) BTI(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("BTI", 0, Operands {})
        case 1  : p = self.alloc("BTI", 1, Operands { vv[0] })
        default : panic("instruction BTI takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isTargets(vv[0])) {
        sa_targets := _BrOmitted
        if len(vv) == 1 {
            sa_targets = vv[0].(BranchTarget)
        }
        op2 := uint32(0b000)
        switch sa_targets {
            case _BrOmitted: op2 |= 0b00 << 1
            case BrC: op2 |= 0b01 << 1
            case BrJ: op2 |= 0b10 << 1
            case BrJC: op2 |= 0b11 << 1
            default: panic("aarch64: invalid combination of operands for BTI")
        }
        return p.setins(hints(4, op2))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BTI")
}

// CAS instruction have 2 forms:
//
//   * CAS  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * CAS  <Xs>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) CAS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CAS", 3, Operands { v0, v1, v2 })
    // CAS  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(2, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    // CAS  <Xs>, <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(3, 0, sa_xs, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CAS")
}

// CASA instruction have 2 forms:
//
//   * CASA  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * CASA  <Xs>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASA", 3, Operands { v0, v1, v2 })
    // CASA  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(2, 1, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    // CASA  <Xs>, <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(3, 1, sa_xs, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASA")
}

// CASAB instruction have one single form:
//
//   * CASAB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(0, 1, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASAB")
}

// CASAH instruction have one single form:
//
//   * CASAH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(1, 1, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASAH")
}

// CASAL instruction have 2 forms:
//
//   * CASAL  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * CASAL  <Xs>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASAL", 3, Operands { v0, v1, v2 })
    // CASAL  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(2, 1, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    // CASAL  <Xs>, <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(3, 1, sa_xs, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASAL")
}

// CASALB instruction have one single form:
//
//   * CASALB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(0, 1, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASALB")
}

// CASALH instruction have one single form:
//
//   * CASALH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(1, 1, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASALH")
}

// CASB instruction have one single form:
//
//   * CASB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(0, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASB")
}

// CASH instruction have one single form:
//
//   * CASH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(1, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASH")
}

// CASL instruction have 2 forms:
//
//   * CASL  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * CASL  <Xs>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASL", 3, Operands { v0, v1, v2 })
    // CASL  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(2, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    // CASL  <Xs>, <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(3, 0, sa_xs, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASL")
}

// CASLB instruction have one single form:
//
//   * CASLB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(0, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASLB")
}

// CASLH instruction have one single form:
//
//   * CASLH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(1, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASLH")
}

// CASP instruction have 2 forms:
//
//   * CASP  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
//   * CASP  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
//
func (self *Program) CASP(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("CASP", 5, Operands { v0, v1, v2, v3, v4 })
    // CASP  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isNextReg(v1, v0, 1) &&
       isWr(v2) &&
       isWr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(0, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    // CASP  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(1, 0, sa_xs, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASP")
}

// CASPA instruction have 2 forms:
//
//   * CASPA  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
//   * CASPA  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
//
func (self *Program) CASPA(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("CASPA", 5, Operands { v0, v1, v2, v3, v4 })
    // CASPA  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isNextReg(v1, v0, 1) &&
       isWr(v2) &&
       isWr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(0, 1, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    // CASPA  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(1, 1, sa_xs, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASPA")
}

// CASPAL instruction have 2 forms:
//
//   * CASPAL  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
//   * CASPAL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
//
func (self *Program) CASPAL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("CASPAL", 5, Operands { v0, v1, v2, v3, v4 })
    // CASPAL  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isNextReg(v1, v0, 1) &&
       isWr(v2) &&
       isWr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(0, 1, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    // CASPAL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(1, 1, sa_xs, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASPAL")
}

// CASPL instruction have 2 forms:
//
//   * CASPL  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
//   * CASPL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
//
func (self *Program) CASPL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("CASPL", 5, Operands { v0, v1, v2, v3, v4 })
    // CASPL  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isNextReg(v1, v0, 1) &&
       isWr(v2) &&
       isWr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(0, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    // CASPL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(1, 0, sa_xs, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASPL")
}

// CBNZ instruction have 2 forms:
//
//   * CBNZ  <Wt>, <label>
//   * CBNZ  <Xt>, <label>
//
func (self *Program) CBNZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("CBNZ", 2, Operands { v0, v1 })
    // CBNZ  <Wt>, <label>
    if isWr(v0) && isLabel(v1) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return compbranch(0, 1, uint32(sa_label.RelativeTo(pc)), sa_wt) })
    }
    // CBNZ  <Xt>, <label>
    if isXr(v0) && isLabel(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return compbranch(1, 1, uint32(sa_label.RelativeTo(pc)), sa_xt) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CBNZ")
}

// CBZ instruction have 2 forms:
//
//   * CBZ  <Wt>, <label>
//   * CBZ  <Xt>, <label>
//
func (self *Program) CBZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("CBZ", 2, Operands { v0, v1 })
    // CBZ  <Wt>, <label>
    if isWr(v0) && isLabel(v1) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return compbranch(0, 0, uint32(sa_label.RelativeTo(pc)), sa_wt) })
    }
    // CBZ  <Xt>, <label>
    if isXr(v0) && isLabel(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return compbranch(1, 0, uint32(sa_label.RelativeTo(pc)), sa_xt) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CBZ")
}

// CCMN instruction have 4 forms:
//
//   * CCMN  <Wn>, #<imm>, #<nzcv>, <cond>
//   * CCMN  <Wn>, <Wm>, #<nzcv>, <cond>
//   * CCMN  <Xn>, #<imm>, #<nzcv>, <cond>
//   * CCMN  <Xn>, <Xm>, #<nzcv>, <cond>
//
func (self *Program) CCMN(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CCMN", 4, Operands { v0, v1, v2, v3 })
    // CCMN  <Wn>, #<imm>, #<nzcv>, <cond>
    if isWr(v0) && isUimm5(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_wn := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm5(v1)
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_imm(0, 0, 1, sa_imm, sa_cond, 0, sa_wn, 0, sa_nzcv))
    }
    // CCMN  <Wn>, <Wm>, #<nzcv>, <cond>
    if isWr(v0) && isWr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_wn := uint32(v0.(asm.Register).ID())
        sa_wm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_reg(0, 0, 1, sa_wm, sa_cond, 0, sa_wn, 0, sa_nzcv))
    }
    // CCMN  <Xn>, #<imm>, #<nzcv>, <cond>
    if isXr(v0) && isUimm5(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm5(v1)
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_imm(1, 0, 1, sa_imm, sa_cond, 0, sa_xn, 0, sa_nzcv))
    }
    // CCMN  <Xn>, <Xm>, #<nzcv>, <cond>
    if isXr(v0) && isXr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_reg(1, 0, 1, sa_xm, sa_cond, 0, sa_xn, 0, sa_nzcv))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CCMN")
}

// CCMP instruction have 4 forms:
//
//   * CCMP  <Wn>, #<imm>, #<nzcv>, <cond>
//   * CCMP  <Wn>, <Wm>, #<nzcv>, <cond>
//   * CCMP  <Xn>, #<imm>, #<nzcv>, <cond>
//   * CCMP  <Xn>, <Xm>, #<nzcv>, <cond>
//
func (self *Program) CCMP(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CCMP", 4, Operands { v0, v1, v2, v3 })
    // CCMP  <Wn>, #<imm>, #<nzcv>, <cond>
    if isWr(v0) && isUimm5(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_wn := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm5(v1)
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_imm(0, 1, 1, sa_imm, sa_cond, 0, sa_wn, 0, sa_nzcv))
    }
    // CCMP  <Wn>, <Wm>, #<nzcv>, <cond>
    if isWr(v0) && isWr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_wn := uint32(v0.(asm.Register).ID())
        sa_wm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_reg(0, 1, 1, sa_wm, sa_cond, 0, sa_wn, 0, sa_nzcv))
    }
    // CCMP  <Xn>, #<imm>, #<nzcv>, <cond>
    if isXr(v0) && isUimm5(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm5(v1)
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_imm(1, 1, 1, sa_imm, sa_cond, 0, sa_xn, 0, sa_nzcv))
    }
    // CCMP  <Xn>, <Xm>, #<nzcv>, <cond>
    if isXr(v0) && isXr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_reg(1, 1, 1, sa_xm, sa_cond, 0, sa_xn, 0, sa_nzcv))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CCMP")
}

// CFINV instruction have one single form:
//
//   * CFINV
//
func (self *Program) CFINV() *Instruction {
    p := self.alloc("CFINV", 0, Operands {})
    return p.setins(pstate(0, 0, 0, 31))
}

// CHKFEAT instruction have one single form:
//
//   * CHKFEAT
//
func (self *Program) CHKFEAT() *Instruction {
    p := self.alloc("CHKFEAT", 0, Operands {})
    return p.setins(hints(5, 0))
}

// CLRBHB instruction have one single form:
//
//   * CLRBHB
//
func (self *Program) CLRBHB() *Instruction {
    p := self.alloc("CLRBHB", 0, Operands {})
    return p.setins(hints(2, 6))
}

// CLREX instruction have one single form:
//
//   * CLREX  {#<imm>}
//
func (self *Program) CLREX(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("CLREX", 0, Operands {})
        case 1  : p = self.alloc("CLREX", 1, Operands { vv[0] })
        default : panic("instruction CLREX takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isUimm4(vv[0])) {
        var sa_imm uint32
        if len(vv) == 1 {
            sa_imm = asUimm4(vv[0])
        }
        return p.setins(barriers(sa_imm, 2, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CLREX")
}

// CLS instruction have 3 forms:
//
//   * CLS  <Wd>, <Wn>
//   * CLS  <Xd>, <Xn>
//   * CLS  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) CLS(v0, v1 interface{}) *Instruction {
    p := self.alloc("CLS", 2, Operands { v0, v1 })
    // CLS  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 5, sa_wn, sa_wd))
    }
    // CLS  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 5, sa_xn, sa_xd))
    }
    // CLS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 0, maskp(sa_t, 1, 2), 4, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CLS")
}

// CLZ instruction have 3 forms:
//
//   * CLZ  <Wd>, <Wn>
//   * CLZ  <Xd>, <Xn>
//   * CLZ  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) CLZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("CLZ", 2, Operands { v0, v1 })
    // CLZ  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 4, sa_wn, sa_wd))
    }
    // CLZ  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 4, sa_xn, sa_xd))
    }
    // CLZ  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 1, maskp(sa_t, 1, 2), 4, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CLZ")
}

// CMEQ instruction have 4 forms:
//
//   * CMEQ  <Vd>.<T>, <Vn>.<T>, #0
//   * CMEQ  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * CMEQ  <V><d>, <V><n>, #0
//   * CMEQ  <V><d>, <V><n>, <V><m>
//
func (self *Program) CMEQ(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CMEQ", 3, Operands { v0, v1, v2 })
    // CMEQ  <Vd>.<T>, <Vn>.<T>, #0
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isIntLit(v2, 0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 0, maskp(sa_t, 1, 2), 9, sa_vn, sa_vd))
    }
    // CMEQ  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 17, sa_vn, sa_vd))
    }
    // CMEQ  <V><d>, <V><n>, #0
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isIntLit(v2, 0) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for CMEQ")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmisc(0, sa_v, 9, sa_n, sa_d))
    }
    // CMEQ  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for CMEQ")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(1, sa_v, sa_m, 17, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CMEQ")
}

// CMGE instruction have 4 forms:
//
//   * CMGE  <Vd>.<T>, <Vn>.<T>, #0
//   * CMGE  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * CMGE  <V><d>, <V><n>, #0
//   * CMGE  <V><d>, <V><n>, <V><m>
//
func (self *Program) CMGE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CMGE", 3, Operands { v0, v1, v2 })
    // CMGE  <Vd>.<T>, <Vn>.<T>, #0
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isIntLit(v2, 0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 1, maskp(sa_t, 1, 2), 8, sa_vn, sa_vd))
    }
    // CMGE  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 7, sa_vn, sa_vd))
    }
    // CMGE  <V><d>, <V><n>, #0
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isIntLit(v2, 0) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for CMGE")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmisc(1, sa_v, 8, sa_n, sa_d))
    }
    // CMGE  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for CMGE")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(0, sa_v, sa_m, 7, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CMGE")
}

// CMGT instruction have 4 forms:
//
//   * CMGT  <Vd>.<T>, <Vn>.<T>, #0
//   * CMGT  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * CMGT  <V><d>, <V><n>, #0
//   * CMGT  <V><d>, <V><n>, <V><m>
//
func (self *Program) CMGT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CMGT", 3, Operands { v0, v1, v2 })
    // CMGT  <Vd>.<T>, <Vn>.<T>, #0
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isIntLit(v2, 0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 0, maskp(sa_t, 1, 2), 8, sa_vn, sa_vd))
    }
    // CMGT  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 6, sa_vn, sa_vd))
    }
    // CMGT  <V><d>, <V><n>, #0
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isIntLit(v2, 0) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for CMGT")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmisc(0, sa_v, 8, sa_n, sa_d))
    }
    // CMGT  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for CMGT")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(0, sa_v, sa_m, 6, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CMGT")
}

// CMHI instruction have 2 forms:
//
//   * CMHI  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * CMHI  <V><d>, <V><n>, <V><m>
//
func (self *Program) CMHI(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CMHI", 3, Operands { v0, v1, v2 })
    // CMHI  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 6, sa_vn, sa_vd))
    }
    // CMHI  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for CMHI")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(1, sa_v, sa_m, 6, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CMHI")
}

// CMHS instruction have 2 forms:
//
//   * CMHS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * CMHS  <V><d>, <V><n>, <V><m>
//
func (self *Program) CMHS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CMHS", 3, Operands { v0, v1, v2 })
    // CMHS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 7, sa_vn, sa_vd))
    }
    // CMHS  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for CMHS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(1, sa_v, sa_m, 7, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CMHS")
}

// CMLE instruction have 2 forms:
//
//   * CMLE  <Vd>.<T>, <Vn>.<T>, #0
//   * CMLE  <V><d>, <V><n>, #0
//
func (self *Program) CMLE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CMLE", 3, Operands { v0, v1, v2 })
    // CMLE  <Vd>.<T>, <Vn>.<T>, #0
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isIntLit(v2, 0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 1, maskp(sa_t, 1, 2), 9, sa_vn, sa_vd))
    }
    // CMLE  <V><d>, <V><n>, #0
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isIntLit(v2, 0) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for CMLE")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmisc(1, sa_v, 9, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CMLE")
}

// CMLT instruction have 2 forms:
//
//   * CMLT  <Vd>.<T>, <Vn>.<T>, #0
//   * CMLT  <V><d>, <V><n>, #0
//
func (self *Program) CMLT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CMLT", 3, Operands { v0, v1, v2 })
    // CMLT  <Vd>.<T>, <Vn>.<T>, #0
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isIntLit(v2, 0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 0, maskp(sa_t, 1, 2), 10, sa_vn, sa_vd))
    }
    // CMLT  <V><d>, <V><n>, #0
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isIntLit(v2, 0) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for CMLT")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmisc(0, sa_v, 10, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CMLT")
}

// CMTST instruction have 2 forms:
//
//   * CMTST  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * CMTST  <V><d>, <V><n>, <V><m>
//
func (self *Program) CMTST(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CMTST", 3, Operands { v0, v1, v2 })
    // CMTST  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 17, sa_vn, sa_vd))
    }
    // CMTST  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for CMTST")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(0, sa_v, sa_m, 17, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CMTST")
}

// CNT instruction have 3 forms:
//
//   * CNT  <Wd>, <Wn>
//   * CNT  <Xd>, <Xn>
//   * CNT  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) CNT(v0, v1 interface{}) *Instruction {
    p := self.alloc("CNT", 2, Operands { v0, v1 })
    // CNT  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 7, sa_wn, sa_wd))
    }
    // CNT  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 7, sa_xn, sa_xd))
    }
    // CNT  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec8B, Vec16B) && isVr(v1) && isVfmt(v1, Vec8B, Vec16B) && vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 0, maskp(sa_t, 1, 2), 5, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CNT")
}

// CPYE instruction have one single form:
//
//   * CPYE  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYE", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 0, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYE")
}

// CPYEN instruction have one single form:
//
//   * CPYEN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 12, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEN")
}

// CPYERN instruction have one single form:
//
//   * CPYERN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 8, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERN")
}

// CPYERT instruction have one single form:
//
//   * CPYERT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 2, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERT")
}

// CPYERTN instruction have one single form:
//
//   * CPYERTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 14, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERTN")
}

// CPYERTRN instruction have one single form:
//
//   * CPYERTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 10, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERTRN")
}

// CPYERTWN instruction have one single form:
//
//   * CPYERTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 6, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERTWN")
}

// CPYET instruction have one single form:
//
//   * CPYET  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYET", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 3, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYET")
}

// CPYETN instruction have one single form:
//
//   * CPYETN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYETN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYETN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 15, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYETN")
}

// CPYETRN instruction have one single form:
//
//   * CPYETRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYETRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYETRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 11, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYETRN")
}

// CPYETWN instruction have one single form:
//
//   * CPYETWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYETWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYETWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 7, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYETWN")
}

// CPYEWN instruction have one single form:
//
//   * CPYEWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 4, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWN")
}

// CPYEWT instruction have one single form:
//
//   * CPYEWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 1, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWT")
}

// CPYEWTN instruction have one single form:
//
//   * CPYEWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 13, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWTN")
}

// CPYEWTRN instruction have one single form:
//
//   * CPYEWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 9, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWTRN")
}

// CPYEWTWN instruction have one single form:
//
//   * CPYEWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 5, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWTWN")
}

// CPYFE instruction have one single form:
//
//   * CPYFE  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFE", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 0, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFE")
}

// CPYFEN instruction have one single form:
//
//   * CPYFEN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 12, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEN")
}

// CPYFERN instruction have one single form:
//
//   * CPYFERN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 8, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERN")
}

// CPYFERT instruction have one single form:
//
//   * CPYFERT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 2, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERT")
}

// CPYFERTN instruction have one single form:
//
//   * CPYFERTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 14, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERTN")
}

// CPYFERTRN instruction have one single form:
//
//   * CPYFERTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 10, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERTRN")
}

// CPYFERTWN instruction have one single form:
//
//   * CPYFERTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 6, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERTWN")
}

// CPYFET instruction have one single form:
//
//   * CPYFET  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFET", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 3, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFET")
}

// CPYFETN instruction have one single form:
//
//   * CPYFETN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFETN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFETN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 15, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFETN")
}

// CPYFETRN instruction have one single form:
//
//   * CPYFETRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFETRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFETRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 11, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFETRN")
}

// CPYFETWN instruction have one single form:
//
//   * CPYFETWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFETWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFETWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 7, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFETWN")
}

// CPYFEWN instruction have one single form:
//
//   * CPYFEWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 4, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWN")
}

// CPYFEWT instruction have one single form:
//
//   * CPYFEWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 1, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWT")
}

// CPYFEWTN instruction have one single form:
//
//   * CPYFEWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 13, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWTN")
}

// CPYFEWTRN instruction have one single form:
//
//   * CPYFEWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 9, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWTRN")
}

// CPYFEWTWN instruction have one single form:
//
//   * CPYFEWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 5, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWTWN")
}

// CPYFM instruction have one single form:
//
//   * CPYFM  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFM", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 0, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFM")
}

// CPYFMN instruction have one single form:
//
//   * CPYFMN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 12, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMN")
}

// CPYFMRN instruction have one single form:
//
//   * CPYFMRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 8, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRN")
}

// CPYFMRT instruction have one single form:
//
//   * CPYFMRT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 2, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRT")
}

// CPYFMRTN instruction have one single form:
//
//   * CPYFMRTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 14, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRTN")
}

// CPYFMRTRN instruction have one single form:
//
//   * CPYFMRTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 10, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRTRN")
}

// CPYFMRTWN instruction have one single form:
//
//   * CPYFMRTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 6, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRTWN")
}

// CPYFMT instruction have one single form:
//
//   * CPYFMT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 3, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMT")
}

// CPYFMTN instruction have one single form:
//
//   * CPYFMTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 15, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMTN")
}

// CPYFMTRN instruction have one single form:
//
//   * CPYFMTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 11, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMTRN")
}

// CPYFMTWN instruction have one single form:
//
//   * CPYFMTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 7, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMTWN")
}

// CPYFMWN instruction have one single form:
//
//   * CPYFMWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 4, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWN")
}

// CPYFMWT instruction have one single form:
//
//   * CPYFMWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 1, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWT")
}

// CPYFMWTN instruction have one single form:
//
//   * CPYFMWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 13, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWTN")
}

// CPYFMWTRN instruction have one single form:
//
//   * CPYFMWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 9, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWTRN")
}

// CPYFMWTWN instruction have one single form:
//
//   * CPYFMWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 5, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWTWN")
}

// CPYFP instruction have one single form:
//
//   * CPYFP  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFP", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 0, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFP")
}

// CPYFPN instruction have one single form:
//
//   * CPYFPN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 12, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPN")
}

// CPYFPRN instruction have one single form:
//
//   * CPYFPRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 8, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRN")
}

// CPYFPRT instruction have one single form:
//
//   * CPYFPRT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 2, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRT")
}

// CPYFPRTN instruction have one single form:
//
//   * CPYFPRTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 14, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRTN")
}

// CPYFPRTRN instruction have one single form:
//
//   * CPYFPRTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 10, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRTRN")
}

// CPYFPRTWN instruction have one single form:
//
//   * CPYFPRTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 6, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRTWN")
}

// CPYFPT instruction have one single form:
//
//   * CPYFPT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 3, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPT")
}

// CPYFPTN instruction have one single form:
//
//   * CPYFPTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 15, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPTN")
}

// CPYFPTRN instruction have one single form:
//
//   * CPYFPTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 11, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPTRN")
}

// CPYFPTWN instruction have one single form:
//
//   * CPYFPTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 7, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPTWN")
}

// CPYFPWN instruction have one single form:
//
//   * CPYFPWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 4, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWN")
}

// CPYFPWT instruction have one single form:
//
//   * CPYFPWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 1, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWT")
}

// CPYFPWTN instruction have one single form:
//
//   * CPYFPWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 13, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWTN")
}

// CPYFPWTRN instruction have one single form:
//
//   * CPYFPWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 9, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWTRN")
}

// CPYFPWTWN instruction have one single form:
//
//   * CPYFPWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 5, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWTWN")
}

// CPYM instruction have one single form:
//
//   * CPYM  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYM", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 0, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYM")
}

// CPYMN instruction have one single form:
//
//   * CPYMN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 12, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMN")
}

// CPYMRN instruction have one single form:
//
//   * CPYMRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 8, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRN")
}

// CPYMRT instruction have one single form:
//
//   * CPYMRT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 2, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRT")
}

// CPYMRTN instruction have one single form:
//
//   * CPYMRTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 14, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRTN")
}

// CPYMRTRN instruction have one single form:
//
//   * CPYMRTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 10, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRTRN")
}

// CPYMRTWN instruction have one single form:
//
//   * CPYMRTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 6, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRTWN")
}

// CPYMT instruction have one single form:
//
//   * CPYMT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 3, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMT")
}

// CPYMTN instruction have one single form:
//
//   * CPYMTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 15, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMTN")
}

// CPYMTRN instruction have one single form:
//
//   * CPYMTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 11, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMTRN")
}

// CPYMTWN instruction have one single form:
//
//   * CPYMTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 7, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMTWN")
}

// CPYMWN instruction have one single form:
//
//   * CPYMWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 4, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWN")
}

// CPYMWT instruction have one single form:
//
//   * CPYMWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 1, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWT")
}

// CPYMWTN instruction have one single form:
//
//   * CPYMWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 13, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWTN")
}

// CPYMWTRN instruction have one single form:
//
//   * CPYMWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 9, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWTRN")
}

// CPYMWTWN instruction have one single form:
//
//   * CPYMWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 5, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWTWN")
}

// CPYP instruction have one single form:
//
//   * CPYP  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYP", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 0, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYP")
}

// CPYPN instruction have one single form:
//
//   * CPYPN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 12, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPN")
}

// CPYPRN instruction have one single form:
//
//   * CPYPRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 8, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRN")
}

// CPYPRT instruction have one single form:
//
//   * CPYPRT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 2, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRT")
}

// CPYPRTN instruction have one single form:
//
//   * CPYPRTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 14, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRTN")
}

// CPYPRTRN instruction have one single form:
//
//   * CPYPRTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 10, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRTRN")
}

// CPYPRTWN instruction have one single form:
//
//   * CPYPRTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 6, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRTWN")
}

// CPYPT instruction have one single form:
//
//   * CPYPT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 3, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPT")
}

// CPYPTN instruction have one single form:
//
//   * CPYPTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 15, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPTN")
}

// CPYPTRN instruction have one single form:
//
//   * CPYPTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 11, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPTRN")
}

// CPYPTWN instruction have one single form:
//
//   * CPYPTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 7, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPTWN")
}

// CPYPWN instruction have one single form:
//
//   * CPYPWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 4, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWN")
}

// CPYPWT instruction have one single form:
//
//   * CPYPWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 1, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWT")
}

// CPYPWTN instruction have one single form:
//
//   * CPYPWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 13, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWTN")
}

// CPYPWTRN instruction have one single form:
//
//   * CPYPWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 9, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWTRN")
}

// CPYPWTWN instruction have one single form:
//
//   * CPYPWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 5, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWTWN")
}

// CRC32B instruction have one single form:
//
//   * CRC32B  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32B(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32B", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 16, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32B")
}

// CRC32CB instruction have one single form:
//
//   * CRC32CB  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32CB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32CB", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 20, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32CB")
}

// CRC32CH instruction have one single form:
//
//   * CRC32CH  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32CH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32CH", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 21, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32CH")
}

// CRC32CW instruction have one single form:
//
//   * CRC32CW  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32CW(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32CW", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 22, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32CW")
}

// CRC32CX instruction have one single form:
//
//   * CRC32CX  <Wd>, <Wn>, <Xm>
//
func (self *Program) CRC32CX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32CX", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isXr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 23, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32CX")
}

// CRC32H instruction have one single form:
//
//   * CRC32H  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32H(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32H", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 17, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32H")
}

// CRC32W instruction have one single form:
//
//   * CRC32W  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32W(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32W", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 18, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32W")
}

// CRC32X instruction have one single form:
//
//   * CRC32X  <Wd>, <Wn>, <Xm>
//
func (self *Program) CRC32X(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32X", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isXr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 19, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32X")
}

// CSDB instruction have one single form:
//
//   * CSDB
//
func (self *Program) CSDB() *Instruction {
    p := self.alloc("CSDB", 0, Operands {})
    return p.setins(hints(2, 4))
}

// CSEL instruction have 2 forms:
//
//   * CSEL  <Wd>, <Wn>, <Wm>, <cond>
//   * CSEL  <Xd>, <Xn>, <Xm>, <cond>
//
func (self *Program) CSEL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CSEL", 4, Operands { v0, v1, v2, v3 })
    // CSEL  <Wd>, <Wn>, <Wm>, <cond>
    if isWr(v0) && isWr(v1) && isWr(v2) && isBrCond(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(0, 0, 0, sa_wm, sa_cond, 0, sa_wn, sa_wd))
    }
    // CSEL  <Xd>, <Xn>, <Xm>, <cond>
    if isXr(v0) && isXr(v1) && isXr(v2) && isBrCond(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(1, 0, 0, sa_xm, sa_cond, 0, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CSEL")
}

// CSINC instruction have 2 forms:
//
//   * CSINC  <Wd>, <Wn>, <Wm>, <cond>
//   * CSINC  <Xd>, <Xn>, <Xm>, <cond>
//
func (self *Program) CSINC(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CSINC", 4, Operands { v0, v1, v2, v3 })
    // CSINC  <Wd>, <Wn>, <Wm>, <cond>
    if isWr(v0) && isWr(v1) && isWr(v2) && isBrCond(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(0, 0, 0, sa_wm, sa_cond, 1, sa_wn, sa_wd))
    }
    // CSINC  <Xd>, <Xn>, <Xm>, <cond>
    if isXr(v0) && isXr(v1) && isXr(v2) && isBrCond(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(1, 0, 0, sa_xm, sa_cond, 1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CSINC")
}

// CSINV instruction have 2 forms:
//
//   * CSINV  <Wd>, <Wn>, <Wm>, <cond>
//   * CSINV  <Xd>, <Xn>, <Xm>, <cond>
//
func (self *Program) CSINV(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CSINV", 4, Operands { v0, v1, v2, v3 })
    // CSINV  <Wd>, <Wn>, <Wm>, <cond>
    if isWr(v0) && isWr(v1) && isWr(v2) && isBrCond(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(0, 1, 0, sa_wm, sa_cond, 0, sa_wn, sa_wd))
    }
    // CSINV  <Xd>, <Xn>, <Xm>, <cond>
    if isXr(v0) && isXr(v1) && isXr(v2) && isBrCond(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(1, 1, 0, sa_xm, sa_cond, 0, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CSINV")
}

// CSNEG instruction have 2 forms:
//
//   * CSNEG  <Wd>, <Wn>, <Wm>, <cond>
//   * CSNEG  <Xd>, <Xn>, <Xm>, <cond>
//
func (self *Program) CSNEG(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CSNEG", 4, Operands { v0, v1, v2, v3 })
    // CSNEG  <Wd>, <Wn>, <Wm>, <cond>
    if isWr(v0) && isWr(v1) && isWr(v2) && isBrCond(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(0, 1, 0, sa_wm, sa_cond, 1, sa_wn, sa_wd))
    }
    // CSNEG  <Xd>, <Xn>, <Xm>, <cond>
    if isXr(v0) && isXr(v1) && isXr(v2) && isBrCond(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(1, 1, 0, sa_xm, sa_cond, 1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CSNEG")
}

// CTZ instruction have 2 forms:
//
//   * CTZ  <Wd>, <Wn>
//   * CTZ  <Xd>, <Xn>
//
func (self *Program) CTZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("CTZ", 2, Operands { v0, v1 })
    // CTZ  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 6, sa_wn, sa_wd))
    }
    // CTZ  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 6, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CTZ")
}

// DCPS1 instruction have one single form:
//
//   * DCPS1  {#<imm>}
//
func (self *Program) DCPS1(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("DCPS1", 0, Operands {})
        case 1  : p = self.alloc("DCPS1", 1, Operands { vv[0] })
        default : panic("instruction DCPS1 takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isUimm16(vv[0])) {
        var sa_imm uint32
        if len(vv) == 1 {
            sa_imm = asUimm16(vv[0])
        }
        return p.setins(exception(5, sa_imm, 0, 1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for DCPS1")
}

// DCPS2 instruction have one single form:
//
//   * DCPS2  {#<imm>}
//
func (self *Program) DCPS2(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("DCPS2", 0, Operands {})
        case 1  : p = self.alloc("DCPS2", 1, Operands { vv[0] })
        default : panic("instruction DCPS2 takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isUimm16(vv[0])) {
        var sa_imm uint32
        if len(vv) == 1 {
            sa_imm = asUimm16(vv[0])
        }
        return p.setins(exception(5, sa_imm, 0, 2))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for DCPS2")
}

// DCPS3 instruction have one single form:
//
//   * DCPS3  {#<imm>}
//
func (self *Program) DCPS3(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("DCPS3", 0, Operands {})
        case 1  : p = self.alloc("DCPS3", 1, Operands { vv[0] })
        default : panic("instruction DCPS3 takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isUimm16(vv[0])) {
        var sa_imm uint32
        if len(vv) == 1 {
            sa_imm = asUimm16(vv[0])
        }
        return p.setins(exception(5, sa_imm, 0, 3))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for DCPS3")
}

// DGH instruction have one single form:
//
//   * DGH
//
func (self *Program) DGH() *Instruction {
    p := self.alloc("DGH", 0, Operands {})
    return p.setins(hints(0, 6))
}

// DMB instruction have one single form:
//
//   * DMB  <option>|#<imm>
//
func (self *Program) DMB(v0 interface{}) *Instruction {
    p := self.alloc("DMB", 1, Operands { v0 })
    if isOption(v0) {
        sa_option := v0.(BarrierOption)
        sa_imm := uint32(sa_option)
        return p.setins(barriers(sa_imm, 5, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for DMB")
}

// DRPS instruction have one single form:
//
//   * DRPS
//
func (self *Program) DRPS() *Instruction {
    p := self.alloc("DRPS", 0, Operands {})
    return p.setins(branch_reg(5, 31, 0, 31, 0))
}

// DSB instruction have 2 forms:
//
//   * DSB  <option>|#<imm>
//   * DSB  <option>nXS
//
func (self *Program) DSB(v0 interface{}) *Instruction {
    p := self.alloc("DSB", 1, Operands { v0 })
    // DSB  <option>|#<imm>
    if isOption(v0) {
        sa_option := v0.(BarrierOption)
        sa_imm := uint32(sa_option)
        return p.setins(barriers(sa_imm, 4, 31))
    }
    // DSB  <option>nXS
    if isOptionNXS(v0) {
        sa_option_1 := v0.(BarrierOption).nxs()
        CRm := uint32(0b0010)
        CRm |= sa_option_1 << 2
        return p.setins(barriers(CRm, 1, 31))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for DSB")
}

// DUP instruction have 3 forms:
//
//   * DUP  <Vd>.<T>, <R><n>
//   * DUP  <Vd>.<T>, <Vn>.<Ts>[<index>]
//   * DUP  <V><d>, <Vn>.<T>[<index>]
//
func (self *Program) DUP(v0, v1 interface{}) *Instruction {
    p := self.alloc("DUP", 2, Operands { v0, v1 })
    // DUP  <Vd>.<T>, <R><n>
    if isVr(v0) && isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) && isWrOrXr(v1) {
        var sa_r [3]uint32
        var sa_r__bit_mask [3]uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000010
            case Vec16B: sa_t = 0b000011
            case Vec4H: sa_t = 0b000100
            case Vec8H: sa_t = 0b000101
            case Vec2S: sa_t = 0b001000
            case Vec4S: sa_t = 0b001001
            case Vec2D: sa_t = 0b010001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b000011
            case Vec16B: sa_t__bit_mask = 0b000011
            case Vec4H: sa_t__bit_mask = 0b000111
            case Vec8H: sa_t__bit_mask = 0b000111
            case Vec2S: sa_t__bit_mask = 0b001111
            case Vec4S: sa_t__bit_mask = 0b001111
            case Vec2D: sa_t__bit_mask = 0b011111
            default: panic("aarch64: unreachable")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch true {
            case isWr(v1): sa_r = [3]uint32{0b00100, 0b00010, 0b00001}
            case isXr(v1): sa_r = [3]uint32{0b01000}
            default: panic("aarch64: unreachable")
        }
        switch true {
            case isWr(v1): sa_r__bit_mask = [3]uint32{0b00111, 0b00011, 0b00001}
            case isXr(v1): sa_r__bit_mask = [3]uint32{0b01111}
            default: panic("aarch64: unreachable")
        }
        if !matchany(maskp(sa_t, 1, 5) & maskp(sa_t__bit_mask, 1, 5), &sa_r[0], &sa_r__bit_mask[0], 3) {
            panic("aarch64: invalid combination of operands for DUP")
        }
        return p.setins(asimdins(sa_t & 1, 0, maskp(sa_t, 1, 5), 1, sa_n, sa_vd))
    }
    // DUP  <Vd>.<T>, <Vn>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) && isVri(v1) {
        var sa_t uint32
        var sa_t__bit_mask uint32
        var sa_ts uint32
        var sa_ts__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000010
            case Vec16B: sa_t = 0b000011
            case Vec4H: sa_t = 0b000100
            case Vec8H: sa_t = 0b000101
            case Vec2S: sa_t = 0b001000
            case Vec4S: sa_t = 0b001001
            case Vec2D: sa_t = 0b010001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b000011
            case Vec16B: sa_t__bit_mask = 0b000011
            case Vec4H: sa_t__bit_mask = 0b000111
            case Vec8H: sa_t__bit_mask = 0b000111
            case Vec2S: sa_t__bit_mask = 0b001111
            case Vec4S: sa_t__bit_mask = 0b001111
            case Vec2D: sa_t__bit_mask = 0b011111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(VidxRegister).ID())
        switch vmoder(v1) {
            case ModeB: sa_ts = 0b00001
            case ModeH: sa_ts = 0b00010
            case ModeS: sa_ts = 0b00100
            case ModeD: sa_ts = 0b01000
            default: panic("aarch64: unreachable")
        }
        switch vmoder(v1) {
            case ModeB: sa_ts__bit_mask = 0b00001
            case ModeH: sa_ts__bit_mask = 0b00011
            case ModeS: sa_ts__bit_mask = 0b00111
            case ModeD: sa_ts__bit_mask = 0b01111
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v1))
        if sa_index != maskp(sa_t, 1, 5) & maskp(sa_t__bit_mask, 1, 5) ||
           maskp(sa_t, 1, 5) & maskp(sa_t__bit_mask, 1, 5) != sa_ts & sa_ts__bit_mask {
            panic("aarch64: invalid combination of operands for DUP")
        }
        return p.setins(asimdins(sa_t & 1, 0, sa_index, 0, sa_vn, sa_vd))
    }
    // DUP  <V><d>, <Vn>.<T>[<index>]
    if isAdvSIMD(v0) && isVri(v1) {
        var sa_t_1 uint32
        var sa_t_1__bit_mask uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00001
            case HRegister: sa_v = 0b00010
            case SRegister: sa_v = 0b00100
            case DRegister: sa_v = 0b01000
            default: panic("aarch64: invalid scalar operand size for DUP")
        }
        switch v0.(type) {
            case BRegister: sa_v__bit_mask = 0b00001
            case HRegister: sa_v__bit_mask = 0b00011
            case SRegister: sa_v__bit_mask = 0b00111
            case DRegister: sa_v__bit_mask = 0b01111
            default: panic("aarch64: invalid scalar operand size for DUP")
        }
        sa_vn := uint32(v1.(VidxRegister).ID())
        switch vmoder(v1) {
            case ModeB: sa_t_1 = 0b00001
            case ModeH: sa_t_1 = 0b00010
            case ModeS: sa_t_1 = 0b00100
            case ModeD: sa_t_1 = 0b01000
            default: panic("aarch64: unreachable")
        }
        switch vmoder(v1) {
            case ModeB: sa_t_1__bit_mask = 0b00001
            case ModeH: sa_t_1__bit_mask = 0b00011
            case ModeS: sa_t_1__bit_mask = 0b00111
            case ModeD: sa_t_1__bit_mask = 0b01111
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v1))
        if sa_index != sa_t_1 & sa_t_1__bit_mask || sa_t_1 & sa_t_1__bit_mask != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for DUP")
        }
        return p.setins(asisdone(0, sa_index, 0, sa_vn, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for DUP")
}

// EON instruction have 2 forms:
//
//   * EON  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * EON  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) EON(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("EON", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("EON", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction EON takes 3 or 4 operands")
    }
    // EON  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 2, sa_shift, 1, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // EON  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 2, sa_shift, 1, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for EON")
}

// EOR instruction have 5 forms:
//
//   * EOR  <Wd|WSP>, <Wn>, #<imm>
//   * EOR  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * EOR  <Xd|SP>, <Xn>, #<imm>
//   * EOR  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//   * EOR  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) EOR(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("EOR", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("EOR", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction EOR takes 3 or 4 operands")
    }
    // EOR  <Wd|WSP>, <Wn>, #<imm>
    if isWrOrWSP(v0) && isWr(v1) && isMask32(v2) {
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_imm := asMaskOp(v2)
        return p.setins(log_imm(0, 2, 0, maskp(sa_imm, 6, 6), mask(sa_imm, 6), sa_wn, sa_wd_wsp))
    }
    // EOR  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 2, sa_shift, 0, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // EOR  <Xd|SP>, <Xn>, #<imm>
    if isXrOrSP(v0) && isXr(v1) && isMask64(v2) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_imm_1 := asMaskOp(v2)
        return p.setins(log_imm(1, 2, maskp(sa_imm_1, 12, 1), maskp(sa_imm_1, 6, 6), mask(sa_imm_1, 6), sa_xn, sa_xd_sp))
    }
    // EOR  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 2, sa_shift, 0, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // EOR  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t, 1, 0, sa_vm, 3, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for EOR")
}

// EOR3 instruction have one single form:
//
//   * EOR3  <Vd>.16B, <Vn>.16B, <Vm>.16B, <Va>.16B
//
func (self *Program) EOR3(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("EOR3", 4, Operands { v0, v1, v2, v3 })
    if isVr(v0) &&
       vfmt(v0) == Vec16B &&
       isVr(v1) &&
       vfmt(v1) == Vec16B &&
       isVr(v2) &&
       vfmt(v2) == Vec16B &&
       isVr(v3) &&
       vfmt(v3) == Vec16B {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        sa_va := uint32(v3.(asm.Register).ID())
        return p.setins(crypto4(0, sa_vm, sa_va, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for EOR3")
}

// ERET instruction have one single form:
//
//   * ERET
//
func (self *Program) ERET() *Instruction {
    p := self.alloc("ERET", 0, Operands {})
    return p.setins(branch_reg(4, 31, 0, 31, 0))
}

// ERETAA instruction have one single form:
//
//   * ERETAA
//
func (self *Program) ERETAA() *Instruction {
    p := self.alloc("ERETAA", 0, Operands {})
    return p.setins(branch_reg(4, 31, 2, 31, 31))
}

// ERETAB instruction have one single form:
//
//   * ERETAB
//
func (self *Program) ERETAB() *Instruction {
    p := self.alloc("ERETAB", 0, Operands {})
    return p.setins(branch_reg(4, 31, 3, 31, 31))
}

// ESB instruction have one single form:
//
//   * ESB
//
func (self *Program) ESB() *Instruction {
    p := self.alloc("ESB", 0, Operands {})
    return p.setins(hints(2, 0))
}

// EXT instruction have one single form:
//
//   * EXT  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>, #<index>
//
func (self *Program) EXT(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("EXT", 4, Operands { v0, v1, v2, v3 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       isExtIndex(v0, v3) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        sa_index := asExtIndex(v0, v3)
        if maskp(sa_index, 4, 1) != sa_t {
            panic("aarch64: invalid combination of operands for EXT")
        }
        return p.setins(asimdext(maskp(sa_index, 4, 1), 0, sa_vm, mask(sa_index, 4), sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for EXT")
}

// EXTR instruction have 2 forms:
//
//   * EXTR  <Wd>, <Wn>, <Wm>, #<lsb>
//   * EXTR  <Xd>, <Xn>, <Xm>, #<lsb>
//
func (self *Program) EXTR(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("EXTR", 4, Operands { v0, v1, v2, v3 })
    // EXTR  <Wd>, <Wn>, <Wm>, #<lsb>
    if isWr(v0) && isWr(v1) && isWr(v2) && isUimm6(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_lsb := asUimm6(v3)
        return p.setins(extract(0, 0, 0, 0, sa_wm, sa_lsb, sa_wn, sa_wd))
    }
    // EXTR  <Xd>, <Xn>, <Xm>, #<lsb>
    if isXr(v0) && isXr(v1) && isXr(v2) && isUimm6(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_lsb_1 := asUimm6(v3)
        return p.setins(extract(1, 0, 1, 0, sa_xm, sa_lsb_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for EXTR")
}

// FABD instruction have 4 forms:
//
//   * FABD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FABD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FABD  <V><d>, <V><n>, <V><m>
//   * FABD  <Hd>, <Hn>, <Hm>
//
func (self *Program) FABD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FABD", 3, Operands { v0, v1, v2 })
    // FABD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 1, size, sa_vm, 26, sa_vn, sa_vd))
    }
    // FABD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 1, 1, sa_vm, 2, sa_vn, sa_vd))
    }
    // FABD  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FABD")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdsame(1, size, sa_m, 26, sa_n, sa_d))
    }
    // FABD  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsamefp16(1, 1, sa_hm, 2, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FABD")
}

// FABS instruction have 5 forms:
//
//   * FABS  <Dd>, <Dn>
//   * FABS  <Hd>, <Hn>
//   * FABS  <Sd>, <Sn>
//   * FABS  <Vd>.<T>, <Vn>.<T>
//   * FABS  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FABS(v0, v1 interface{}) *Instruction {
    p := self.alloc("FABS", 2, Operands { v0, v1 })
    // FABS  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 1, sa_dn, sa_dd))
    }
    // FABS  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 1, sa_hn, sa_hd))
    }
    // FABS  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 1, sa_sn, sa_sd))
    }
    // FABS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 15, sa_vn, sa_vd))
    }
    // FABS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 1, 15, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FABS")
}

// FACGE instruction have 4 forms:
//
//   * FACGE  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FACGE  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FACGE  <V><d>, <V><n>, <V><m>
//   * FACGE  <Hd>, <Hn>, <Hm>
//
func (self *Program) FACGE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FACGE", 3, Operands { v0, v1, v2 })
    // FACGE  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 1, size, sa_vm, 29, sa_vn, sa_vd))
    }
    // FACGE  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 1, 0, sa_vm, 5, sa_vn, sa_vd))
    }
    // FACGE  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FACGE")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdsame(1, size, sa_m, 29, sa_n, sa_d))
    }
    // FACGE  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsamefp16(1, 0, sa_hm, 5, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FACGE")
}

// FACGT instruction have 4 forms:
//
//   * FACGT  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FACGT  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FACGT  <V><d>, <V><n>, <V><m>
//   * FACGT  <Hd>, <Hn>, <Hm>
//
func (self *Program) FACGT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FACGT", 3, Operands { v0, v1, v2 })
    // FACGT  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 1, size, sa_vm, 29, sa_vn, sa_vd))
    }
    // FACGT  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 1, 1, sa_vm, 5, sa_vn, sa_vd))
    }
    // FACGT  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FACGT")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdsame(1, size, sa_m, 29, sa_n, sa_d))
    }
    // FACGT  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsamefp16(1, 1, sa_hm, 5, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FACGT")
}

// FADD instruction have 5 forms:
//
//   * FADD  <Dd>, <Dn>, <Dm>
//   * FADD  <Hd>, <Hn>, <Hm>
//   * FADD  <Sd>, <Sn>, <Sm>
//   * FADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) FADD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FADD", 3, Operands { v0, v1, v2 })
    // FADD  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 2, sa_dn, sa_dd))
    }
    // FADD  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 2, sa_hn, sa_hd))
    }
    // FADD  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 2, sa_sn, sa_sd))
    }
    // FADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 0, size, sa_vm, 26, sa_vn, sa_vd))
    }
    // FADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 0, 0, sa_vm, 2, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FADD")
}

// FADDP instruction have 4 forms:
//
//   * FADDP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FADDP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FADDP  <V><d>, <Vn>.<T>
//   * FADDP  <V><d>, <Vn>.<T>
//
func (self *Program) FADDP(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("FADDP", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("FADDP", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction FADDP takes 2 or 3 operands")
    }
    // FADDP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 1, size, sa_vm, 26, sa_vn, sa_vd))
    }
    // FADDP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 1, 0, sa_vm, 2, sa_vn, sa_vd))
    }
    // FADDP  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && vfmt(v1) == Vec2H {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b0
            default: panic("aarch64: invalid scalar operand size for FADDP")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_t = 0b0
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= sa_v
        if sa_v != sa_t {
            panic("aarch64: invalid combination of operands for FADDP")
        }
        return p.setins(asisdpair(0, size, 13, sa_vn, sa_d))
    }
    // FADDP  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && isVfmt(v1, Vec2S, Vec2D) {
        var sa_t_1 uint32
        var sa_v_1 uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v_1 = 0b0
            case DRegister: sa_v_1 = 0b1
            default: panic("aarch64: invalid scalar operand size for FADDP")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2S: sa_t_1 = 0b0
            case Vec2D: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= sa_v_1
        if sa_v_1 != sa_t_1 {
            panic("aarch64: invalid combination of operands for FADDP")
        }
        return p.setins(asisdpair(1, size, 13, sa_vn, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FADDP")
}

// FCADD instruction have one single form:
//
//   * FCADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>, #<rotate>
//
func (self *Program) FCADD(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FCADD", 4, Operands { v0, v1, v2, v3 })
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isIntLit(v3, 90, 270) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_rotate uint32
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        switch asLit(v3) {
            case 90: sa_rotate = 0b0
            case 270: sa_rotate = 0b1
            default: panic("aarch64: invalid operand 'sa_rotate' for FCADD")
        }
        opcode := uint32(0b1100)
        opcode |= sa_rotate << 1
        return p.setins(asimdsame2(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, opcode, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for FCADD")
}

// FCCMP instruction have 3 forms:
//
//   * FCCMP  <Dn>, <Dm>, #<nzcv>, <cond>
//   * FCCMP  <Hn>, <Hm>, #<nzcv>, <cond>
//   * FCCMP  <Sn>, <Sm>, #<nzcv>, <cond>
//
func (self *Program) FCCMP(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FCCMP", 4, Operands { v0, v1, v2, v3 })
    // FCCMP  <Dn>, <Dm>, #<nzcv>, <cond>
    if isDr(v0) && isDr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_dn := uint32(v0.(asm.Register).ID())
        sa_dm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatccmp(0, 0, 1, sa_dm, sa_cond, sa_dn, 0, sa_nzcv))
    }
    // FCCMP  <Hn>, <Hm>, #<nzcv>, <cond>
    if isHr(v0) && isHr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_hn := uint32(v0.(asm.Register).ID())
        sa_hm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatccmp(0, 0, 3, sa_hm, sa_cond, sa_hn, 0, sa_nzcv))
    }
    // FCCMP  <Sn>, <Sm>, #<nzcv>, <cond>
    if isSr(v0) && isSr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_sn := uint32(v0.(asm.Register).ID())
        sa_sm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatccmp(0, 0, 0, sa_sm, sa_cond, sa_sn, 0, sa_nzcv))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCCMP")
}

// FCCMPE instruction have 3 forms:
//
//   * FCCMPE  <Dn>, <Dm>, #<nzcv>, <cond>
//   * FCCMPE  <Hn>, <Hm>, #<nzcv>, <cond>
//   * FCCMPE  <Sn>, <Sm>, #<nzcv>, <cond>
//
func (self *Program) FCCMPE(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FCCMPE", 4, Operands { v0, v1, v2, v3 })
    // FCCMPE  <Dn>, <Dm>, #<nzcv>, <cond>
    if isDr(v0) && isDr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_dn := uint32(v0.(asm.Register).ID())
        sa_dm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatccmp(0, 0, 1, sa_dm, sa_cond, sa_dn, 1, sa_nzcv))
    }
    // FCCMPE  <Hn>, <Hm>, #<nzcv>, <cond>
    if isHr(v0) && isHr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_hn := uint32(v0.(asm.Register).ID())
        sa_hm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatccmp(0, 0, 3, sa_hm, sa_cond, sa_hn, 1, sa_nzcv))
    }
    // FCCMPE  <Sn>, <Sm>, #<nzcv>, <cond>
    if isSr(v0) && isSr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_sn := uint32(v0.(asm.Register).ID())
        sa_sm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatccmp(0, 0, 0, sa_sm, sa_cond, sa_sn, 1, sa_nzcv))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCCMPE")
}

// FCMEQ instruction have 8 forms:
//
//   * FCMEQ  <Vd>.<T>, <Vn>.<T>, #0.0
//   * FCMEQ  <Vd>.<T>, <Vn>.<T>, #0.0
//   * FCMEQ  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FCMEQ  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FCMEQ  <V><d>, <V><n>, #0.0
//   * FCMEQ  <Hd>, <Hn>, #0.0
//   * FCMEQ  <V><d>, <V><n>, <V><m>
//   * FCMEQ  <Hd>, <Hn>, <Hm>
//
func (self *Program) FCMEQ(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FCMEQ", 3, Operands { v0, v1, v2 })
    // FCMEQ  <Vd>.<T>, <Vn>.<T>, #0.0
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isFloatLit(v2, 0.0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 13, sa_vn, sa_vd))
    }
    // FCMEQ  <Vd>.<T>, <Vn>.<T>, #0.0
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isFloatLit(v2, 0.0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 1, 13, sa_vn, sa_vd))
    }
    // FCMEQ  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 0, size, sa_vm, 28, sa_vn, sa_vd))
    }
    // FCMEQ  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 0, 0, sa_vm, 4, sa_vn, sa_vd))
    }
    // FCMEQ  <V><d>, <V><n>, #0.0
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFloatLit(v2, 0.0) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCMEQ")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 13, sa_n, sa_d))
    }
    // FCMEQ  <Hd>, <Hn>, #0.0
    if isHr(v0) && isHr(v1) && isFloatLit(v2, 0.0) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 1, 13, sa_hn, sa_hd))
    }
    // FCMEQ  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCMEQ")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdsame(0, size, sa_m, 28, sa_n, sa_d))
    }
    // FCMEQ  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsamefp16(0, 0, sa_hm, 4, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCMEQ")
}

// FCMGE instruction have 8 forms:
//
//   * FCMGE  <Vd>.<T>, <Vn>.<T>, #0.0
//   * FCMGE  <Vd>.<T>, <Vn>.<T>, #0.0
//   * FCMGE  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FCMGE  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FCMGE  <V><d>, <V><n>, #0.0
//   * FCMGE  <Hd>, <Hn>, #0.0
//   * FCMGE  <V><d>, <V><n>, <V><m>
//   * FCMGE  <Hd>, <Hn>, <Hm>
//
func (self *Program) FCMGE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FCMGE", 3, Operands { v0, v1, v2 })
    // FCMGE  <Vd>.<T>, <Vn>.<T>, #0.0
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isFloatLit(v2, 0.0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 12, sa_vn, sa_vd))
    }
    // FCMGE  <Vd>.<T>, <Vn>.<T>, #0.0
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isFloatLit(v2, 0.0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 1, 12, sa_vn, sa_vd))
    }
    // FCMGE  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 1, size, sa_vm, 28, sa_vn, sa_vd))
    }
    // FCMGE  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 1, 0, sa_vm, 4, sa_vn, sa_vd))
    }
    // FCMGE  <V><d>, <V><n>, #0.0
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFloatLit(v2, 0.0) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCMGE")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(1, size, 12, sa_n, sa_d))
    }
    // FCMGE  <Hd>, <Hn>, #0.0
    if isHr(v0) && isHr(v1) && isFloatLit(v2, 0.0) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(1, 1, 12, sa_hn, sa_hd))
    }
    // FCMGE  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCMGE")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdsame(1, size, sa_m, 28, sa_n, sa_d))
    }
    // FCMGE  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsamefp16(1, 0, sa_hm, 4, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCMGE")
}

// FCMGT instruction have 8 forms:
//
//   * FCMGT  <Vd>.<T>, <Vn>.<T>, #0.0
//   * FCMGT  <Vd>.<T>, <Vn>.<T>, #0.0
//   * FCMGT  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FCMGT  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FCMGT  <V><d>, <V><n>, #0.0
//   * FCMGT  <Hd>, <Hn>, #0.0
//   * FCMGT  <V><d>, <V><n>, <V><m>
//   * FCMGT  <Hd>, <Hn>, <Hm>
//
func (self *Program) FCMGT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FCMGT", 3, Operands { v0, v1, v2 })
    // FCMGT  <Vd>.<T>, <Vn>.<T>, #0.0
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isFloatLit(v2, 0.0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 12, sa_vn, sa_vd))
    }
    // FCMGT  <Vd>.<T>, <Vn>.<T>, #0.0
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isFloatLit(v2, 0.0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 1, 12, sa_vn, sa_vd))
    }
    // FCMGT  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 1, size, sa_vm, 28, sa_vn, sa_vd))
    }
    // FCMGT  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 1, 1, sa_vm, 4, sa_vn, sa_vd))
    }
    // FCMGT  <V><d>, <V><n>, #0.0
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFloatLit(v2, 0.0) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCMGT")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 12, sa_n, sa_d))
    }
    // FCMGT  <Hd>, <Hn>, #0.0
    if isHr(v0) && isHr(v1) && isFloatLit(v2, 0.0) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 1, 12, sa_hn, sa_hd))
    }
    // FCMGT  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCMGT")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdsame(1, size, sa_m, 28, sa_n, sa_d))
    }
    // FCMGT  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsamefp16(1, 1, sa_hm, 4, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCMGT")
}

// FCMLA instruction have 3 forms:
//
//   * FCMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>], #<rotate>
//   * FCMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>], #<rotate>
//   * FCMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>, #<rotate>
//
func (self *Program) FCMLA(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FCMLA", 4, Operands { v0, v1, v2, v3 })
    // FCMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>], #<rotate>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec4S) &&
       isVri(v2) &&
       isIntLit(v3, 0, 90, 180, 270) &&
       vfmt(v0) == vfmt(v1) {
        var sa_rotate uint32
        var sa_t uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        switch asLit(v3) {
            case 0: sa_rotate = 0b00
            case 90: sa_rotate = 0b01
            case 180: sa_rotate = 0b10
            case 270: sa_rotate = 0b11
            default: panic("aarch64: invalid operand 'sa_rotate' for FCMLA")
        }
        opcode := uint32(0b0001)
        opcode |= sa_rotate << 1
        if maskp(sa_t, 1, 2) != sa_ts || sa_ts != maskp(sa_index, 2, 2) {
            panic("aarch64: invalid combination of operands for FCMLA")
        }
        return p.setins(asimdelem(
            sa_t & 1,
            1,
            1,
            sa_index & 1,
            maskp(sa_vm, 4, 1),
            mask(sa_vm, 4),
            opcode,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // FCMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>], #<rotate>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec4S) &&
       isVri(v2) &&
       isIntLit(v3, 0, 90, 180, 270) &&
       vfmt(v0) == vfmt(v1) {
        var sa_rotate uint32
        var sa_t uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        switch asLit(v3) {
            case 0: sa_rotate = 0b00
            case 90: sa_rotate = 0b01
            case 180: sa_rotate = 0b10
            case 270: sa_rotate = 0b11
            default: panic("aarch64: invalid operand 'sa_rotate' for FCMLA")
        }
        opcode := uint32(0b0001)
        opcode |= sa_rotate << 1
        if maskp(sa_t, 1, 2) != sa_ts || sa_ts != maskp(sa_index, 2, 2) {
            panic("aarch64: invalid combination of operands for FCMLA")
        }
        return p.setins(asimdelem(
            sa_t & 1,
            1,
            2,
            sa_index & 1,
            maskp(sa_vm, 4, 1),
            mask(sa_vm, 4),
            opcode,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // FCMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>, #<rotate>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isIntLit(v3, 0, 90, 180, 270) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_rotate uint32
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        switch asLit(v3) {
            case 0: sa_rotate = 0b00
            case 90: sa_rotate = 0b01
            case 180: sa_rotate = 0b10
            case 270: sa_rotate = 0b11
            default: panic("aarch64: invalid operand 'sa_rotate' for FCMLA")
        }
        opcode := uint32(0b1000)
        opcode |= sa_rotate
        return p.setins(asimdsame2(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, opcode, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCMLA")
}

// FCMLE instruction have 4 forms:
//
//   * FCMLE  <Vd>.<T>, <Vn>.<T>, #0.0
//   * FCMLE  <Vd>.<T>, <Vn>.<T>, #0.0
//   * FCMLE  <V><d>, <V><n>, #0.0
//   * FCMLE  <Hd>, <Hn>, #0.0
//
func (self *Program) FCMLE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FCMLE", 3, Operands { v0, v1, v2 })
    // FCMLE  <Vd>.<T>, <Vn>.<T>, #0.0
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isFloatLit(v2, 0.0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 13, sa_vn, sa_vd))
    }
    // FCMLE  <Vd>.<T>, <Vn>.<T>, #0.0
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isFloatLit(v2, 0.0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 1, 13, sa_vn, sa_vd))
    }
    // FCMLE  <V><d>, <V><n>, #0.0
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFloatLit(v2, 0.0) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCMLE")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(1, size, 13, sa_n, sa_d))
    }
    // FCMLE  <Hd>, <Hn>, #0.0
    if isHr(v0) && isHr(v1) && isFloatLit(v2, 0.0) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(1, 1, 13, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCMLE")
}

// FCMLT instruction have 4 forms:
//
//   * FCMLT  <Vd>.<T>, <Vn>.<T>, #0.0
//   * FCMLT  <Vd>.<T>, <Vn>.<T>, #0.0
//   * FCMLT  <V><d>, <V><n>, #0.0
//   * FCMLT  <Hd>, <Hn>, #0.0
//
func (self *Program) FCMLT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FCMLT", 3, Operands { v0, v1, v2 })
    // FCMLT  <Vd>.<T>, <Vn>.<T>, #0.0
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isFloatLit(v2, 0.0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 14, sa_vn, sa_vd))
    }
    // FCMLT  <Vd>.<T>, <Vn>.<T>, #0.0
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isFloatLit(v2, 0.0) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 1, 14, sa_vn, sa_vd))
    }
    // FCMLT  <V><d>, <V><n>, #0.0
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFloatLit(v2, 0.0) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCMLT")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 14, sa_n, sa_d))
    }
    // FCMLT  <Hd>, <Hn>, #0.0
    if isHr(v0) && isHr(v1) && isFloatLit(v2, 0.0) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 1, 14, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCMLT")
}

// FCMP instruction have 6 forms:
//
//   * FCMP  <Dn>, #0.0
//   * FCMP  <Dn>, <Dm>
//   * FCMP  <Hn>, #0.0
//   * FCMP  <Hn>, <Hm>
//   * FCMP  <Sn>, #0.0
//   * FCMP  <Sn>, <Sm>
//
func (self *Program) FCMP(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCMP", 2, Operands { v0, v1 })
    // FCMP  <Dn>, #0.0
    if isDr(v0) && isFloatLit(v1, 0.0) {
        sa_dn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 1, 0, 0, sa_dn, 8))
    }
    // FCMP  <Dn>, <Dm>
    if isDr(v0) && isDr(v1) {
        sa_dn_1 := uint32(v0.(asm.Register).ID())
        sa_dm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 1, sa_dm, 0, sa_dn_1, 0))
    }
    // FCMP  <Hn>, #0.0
    if isHr(v0) && isFloatLit(v1, 0.0) {
        sa_hn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 3, 0, 0, sa_hn, 8))
    }
    // FCMP  <Hn>, <Hm>
    if isHr(v0) && isHr(v1) {
        sa_hn_1 := uint32(v0.(asm.Register).ID())
        sa_hm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 3, sa_hm, 0, sa_hn_1, 0))
    }
    // FCMP  <Sn>, #0.0
    if isSr(v0) && isFloatLit(v1, 0.0) {
        sa_sn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 0, 0, 0, sa_sn, 8))
    }
    // FCMP  <Sn>, <Sm>
    if isSr(v0) && isSr(v1) {
        sa_sn_1 := uint32(v0.(asm.Register).ID())
        sa_sm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 0, sa_sm, 0, sa_sn_1, 0))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCMP")
}

// FCMPE instruction have 6 forms:
//
//   * FCMPE  <Dn>, #0.0
//   * FCMPE  <Dn>, <Dm>
//   * FCMPE  <Hn>, #0.0
//   * FCMPE  <Hn>, <Hm>
//   * FCMPE  <Sn>, #0.0
//   * FCMPE  <Sn>, <Sm>
//
func (self *Program) FCMPE(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCMPE", 2, Operands { v0, v1 })
    // FCMPE  <Dn>, #0.0
    if isDr(v0) && isFloatLit(v1, 0.0) {
        sa_dn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 1, 0, 0, sa_dn, 24))
    }
    // FCMPE  <Dn>, <Dm>
    if isDr(v0) && isDr(v1) {
        sa_dn_1 := uint32(v0.(asm.Register).ID())
        sa_dm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 1, sa_dm, 0, sa_dn_1, 16))
    }
    // FCMPE  <Hn>, #0.0
    if isHr(v0) && isFloatLit(v1, 0.0) {
        sa_hn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 3, 0, 0, sa_hn, 24))
    }
    // FCMPE  <Hn>, <Hm>
    if isHr(v0) && isHr(v1) {
        sa_hn_1 := uint32(v0.(asm.Register).ID())
        sa_hm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 3, sa_hm, 0, sa_hn_1, 16))
    }
    // FCMPE  <Sn>, #0.0
    if isSr(v0) && isFloatLit(v1, 0.0) {
        sa_sn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 0, 0, 0, sa_sn, 24))
    }
    // FCMPE  <Sn>, <Sm>
    if isSr(v0) && isSr(v1) {
        sa_sn_1 := uint32(v0.(asm.Register).ID())
        sa_sm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 0, sa_sm, 0, sa_sn_1, 16))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCMPE")
}

// FCSEL instruction have 3 forms:
//
//   * FCSEL  <Dd>, <Dn>, <Dm>, <cond>
//   * FCSEL  <Hd>, <Hn>, <Hm>, <cond>
//   * FCSEL  <Sd>, <Sn>, <Sm>, <cond>
//
func (self *Program) FCSEL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FCSEL", 4, Operands { v0, v1, v2, v3 })
    // FCSEL  <Dd>, <Dn>, <Dm>, <cond>
    if isDr(v0) && isDr(v1) && isDr(v2) && isBrCond(v3) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatsel(0, 0, 1, sa_dm, sa_cond, sa_dn, sa_dd))
    }
    // FCSEL  <Hd>, <Hn>, <Hm>, <cond>
    if isHr(v0) && isHr(v1) && isHr(v2) && isBrCond(v3) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatsel(0, 0, 3, sa_hm, sa_cond, sa_hn, sa_hd))
    }
    // FCSEL  <Sd>, <Sn>, <Sm>, <cond>
    if isSr(v0) && isSr(v1) && isSr(v2) && isBrCond(v3) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatsel(0, 0, 0, sa_sm, sa_cond, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCSEL")
}

// FCVT instruction have 6 forms:
//
//   * FCVT  <Dd>, <Hn>
//   * FCVT  <Dd>, <Sn>
//   * FCVT  <Hd>, <Dn>
//   * FCVT  <Hd>, <Sn>
//   * FCVT  <Sd>, <Dn>
//   * FCVT  <Sd>, <Hn>
//
func (self *Program) FCVT(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVT", 2, Operands { v0, v1 })
    // FCVT  <Dd>, <Hn>
    if isDr(v0) && isHr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 5, sa_hn, sa_dd))
    }
    // FCVT  <Dd>, <Sn>
    if isDr(v0) && isSr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 5, sa_sn, sa_dd))
    }
    // FCVT  <Hd>, <Dn>
    if isHr(v0) && isDr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 7, sa_dn, sa_hd))
    }
    // FCVT  <Hd>, <Sn>
    if isHr(v0) && isSr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 7, sa_sn, sa_hd))
    }
    // FCVT  <Sd>, <Dn>
    if isSr(v0) && isDr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 4, sa_dn, sa_sd))
    }
    // FCVT  <Sd>, <Hn>
    if isSr(v0) && isHr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 4, sa_hn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVT")
}

// FCVTAS instruction have 10 forms:
//
//   * FCVTAS  <Wd>, <Dn>
//   * FCVTAS  <Wd>, <Hn>
//   * FCVTAS  <Wd>, <Sn>
//   * FCVTAS  <Xd>, <Dn>
//   * FCVTAS  <Xd>, <Hn>
//   * FCVTAS  <Xd>, <Sn>
//   * FCVTAS  <Vd>.<T>, <Vn>.<T>
//   * FCVTAS  <Vd>.<T>, <Vn>.<T>
//   * FCVTAS  <V><d>, <V><n>
//   * FCVTAS  <Hd>, <Hn>
//
func (self *Program) FCVTAS(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTAS", 2, Operands { v0, v1 })
    // FCVTAS  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 4, sa_dn, sa_wd))
    }
    // FCVTAS  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 4, sa_hn, sa_wd))
    }
    // FCVTAS  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 4, sa_sn, sa_wd))
    }
    // FCVTAS  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 4, sa_dn, sa_xd))
    }
    // FCVTAS  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 4, sa_hn, sa_xd))
    }
    // FCVTAS  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 4, sa_sn, sa_xd))
    }
    // FCVTAS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 28, sa_vn, sa_vd))
    }
    // FCVTAS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 0, 28, sa_vn, sa_vd))
    }
    // FCVTAS  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTAS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 28, sa_n, sa_d))
    }
    // FCVTAS  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 0, 28, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTAS")
}

// FCVTAU instruction have 10 forms:
//
//   * FCVTAU  <Wd>, <Dn>
//   * FCVTAU  <Wd>, <Hn>
//   * FCVTAU  <Wd>, <Sn>
//   * FCVTAU  <Xd>, <Dn>
//   * FCVTAU  <Xd>, <Hn>
//   * FCVTAU  <Xd>, <Sn>
//   * FCVTAU  <Vd>.<T>, <Vn>.<T>
//   * FCVTAU  <Vd>.<T>, <Vn>.<T>
//   * FCVTAU  <V><d>, <V><n>
//   * FCVTAU  <Hd>, <Hn>
//
func (self *Program) FCVTAU(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTAU", 2, Operands { v0, v1 })
    // FCVTAU  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 5, sa_dn, sa_wd))
    }
    // FCVTAU  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 5, sa_hn, sa_wd))
    }
    // FCVTAU  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 5, sa_sn, sa_wd))
    }
    // FCVTAU  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 5, sa_dn, sa_xd))
    }
    // FCVTAU  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 5, sa_hn, sa_xd))
    }
    // FCVTAU  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 5, sa_sn, sa_xd))
    }
    // FCVTAU  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 28, sa_vn, sa_vd))
    }
    // FCVTAU  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 0, 28, sa_vn, sa_vd))
    }
    // FCVTAU  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTAU")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdmisc(1, size, 28, sa_n, sa_d))
    }
    // FCVTAU  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(1, 0, 28, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTAU")
}

// FCVTL instruction have one single form:
//
//   * FCVTL  <Vd>.<Ta>, <Vn>.<Tb>
//
func (self *Program) FCVTL(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTL", 2, Operands { v0, v1 })
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b0
            case Vec2D: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b00
            case Vec8H: sa_tb = 0b01
            case Vec2S: sa_tb = 0b10
            case Vec4S: sa_tb = 0b11
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= sa_ta
        if sa_ta != maskp(sa_tb, 1, 1) {
            panic("aarch64: invalid combination of operands for FCVTL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for FCVTL")
        }
        return p.setins(asimdmisc(0, 0, size, 23, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTL")
}

// FCVTL2 instruction have one single form:
//
//   * FCVTL2  <Vd>.<Ta>, <Vn>.<Tb>
//
func (self *Program) FCVTL2(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTL2", 2, Operands { v0, v1 })
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b0
            case Vec2D: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b00
            case Vec8H: sa_tb = 0b01
            case Vec2S: sa_tb = 0b10
            case Vec4S: sa_tb = 0b11
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= sa_ta
        if sa_ta != maskp(sa_tb, 1, 1) {
            panic("aarch64: invalid combination of operands for FCVTL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for FCVTL2")
        }
        return p.setins(asimdmisc(1, 0, size, 23, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTL2")
}

// FCVTMS instruction have 10 forms:
//
//   * FCVTMS  <Wd>, <Dn>
//   * FCVTMS  <Wd>, <Hn>
//   * FCVTMS  <Wd>, <Sn>
//   * FCVTMS  <Xd>, <Dn>
//   * FCVTMS  <Xd>, <Hn>
//   * FCVTMS  <Xd>, <Sn>
//   * FCVTMS  <Vd>.<T>, <Vn>.<T>
//   * FCVTMS  <Vd>.<T>, <Vn>.<T>
//   * FCVTMS  <V><d>, <V><n>
//   * FCVTMS  <Hd>, <Hn>
//
func (self *Program) FCVTMS(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTMS", 2, Operands { v0, v1 })
    // FCVTMS  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 2, 0, sa_dn, sa_wd))
    }
    // FCVTMS  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 2, 0, sa_hn, sa_wd))
    }
    // FCVTMS  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 2, 0, sa_sn, sa_wd))
    }
    // FCVTMS  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 2, 0, sa_dn, sa_xd))
    }
    // FCVTMS  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 2, 0, sa_hn, sa_xd))
    }
    // FCVTMS  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 2, 0, sa_sn, sa_xd))
    }
    // FCVTMS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 27, sa_vn, sa_vd))
    }
    // FCVTMS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 0, 27, sa_vn, sa_vd))
    }
    // FCVTMS  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTMS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 27, sa_n, sa_d))
    }
    // FCVTMS  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 0, 27, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTMS")
}

// FCVTMU instruction have 10 forms:
//
//   * FCVTMU  <Wd>, <Dn>
//   * FCVTMU  <Wd>, <Hn>
//   * FCVTMU  <Wd>, <Sn>
//   * FCVTMU  <Xd>, <Dn>
//   * FCVTMU  <Xd>, <Hn>
//   * FCVTMU  <Xd>, <Sn>
//   * FCVTMU  <Vd>.<T>, <Vn>.<T>
//   * FCVTMU  <Vd>.<T>, <Vn>.<T>
//   * FCVTMU  <V><d>, <V><n>
//   * FCVTMU  <Hd>, <Hn>
//
func (self *Program) FCVTMU(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTMU", 2, Operands { v0, v1 })
    // FCVTMU  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 2, 1, sa_dn, sa_wd))
    }
    // FCVTMU  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 2, 1, sa_hn, sa_wd))
    }
    // FCVTMU  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 2, 1, sa_sn, sa_wd))
    }
    // FCVTMU  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 2, 1, sa_dn, sa_xd))
    }
    // FCVTMU  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 2, 1, sa_hn, sa_xd))
    }
    // FCVTMU  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 2, 1, sa_sn, sa_xd))
    }
    // FCVTMU  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 27, sa_vn, sa_vd))
    }
    // FCVTMU  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 0, 27, sa_vn, sa_vd))
    }
    // FCVTMU  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTMU")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdmisc(1, size, 27, sa_n, sa_d))
    }
    // FCVTMU  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(1, 0, 27, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTMU")
}

// FCVTN instruction have one single form:
//
//   * FCVTN  <Vd>.<Tb>, <Vn>.<Ta>
//
func (self *Program) FCVTN(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTN", 2, Operands { v0, v1 })
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) && isVr(v1) && isVfmt(v1, Vec4S, Vec2D) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_tb = 0b00
            case Vec8H: sa_tb = 0b01
            case Vec2S: sa_tb = 0b10
            case Vec4S: sa_tb = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4S: sa_ta = 0b0
            case Vec2D: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= maskp(sa_tb, 1, 1)
        if maskp(sa_tb, 1, 1) != sa_ta {
            panic("aarch64: invalid combination of operands for FCVTN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for FCVTN")
        }
        return p.setins(asimdmisc(0, 0, size, 22, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTN")
}

// FCVTN2 instruction have one single form:
//
//   * FCVTN2  <Vd>.<Tb>, <Vn>.<Ta>
//
func (self *Program) FCVTN2(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTN2", 2, Operands { v0, v1 })
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) && isVr(v1) && isVfmt(v1, Vec4S, Vec2D) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_tb = 0b00
            case Vec8H: sa_tb = 0b01
            case Vec2S: sa_tb = 0b10
            case Vec4S: sa_tb = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4S: sa_ta = 0b0
            case Vec2D: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= maskp(sa_tb, 1, 1)
        if maskp(sa_tb, 1, 1) != sa_ta {
            panic("aarch64: invalid combination of operands for FCVTN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for FCVTN2")
        }
        return p.setins(asimdmisc(1, 0, size, 22, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTN2")
}

// FCVTNS instruction have 10 forms:
//
//   * FCVTNS  <Wd>, <Dn>
//   * FCVTNS  <Wd>, <Hn>
//   * FCVTNS  <Wd>, <Sn>
//   * FCVTNS  <Xd>, <Dn>
//   * FCVTNS  <Xd>, <Hn>
//   * FCVTNS  <Xd>, <Sn>
//   * FCVTNS  <Vd>.<T>, <Vn>.<T>
//   * FCVTNS  <Vd>.<T>, <Vn>.<T>
//   * FCVTNS  <V><d>, <V><n>
//   * FCVTNS  <Hd>, <Hn>
//
func (self *Program) FCVTNS(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTNS", 2, Operands { v0, v1 })
    // FCVTNS  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 0, sa_dn, sa_wd))
    }
    // FCVTNS  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 0, sa_hn, sa_wd))
    }
    // FCVTNS  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 0, sa_sn, sa_wd))
    }
    // FCVTNS  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 0, sa_dn, sa_xd))
    }
    // FCVTNS  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 0, sa_hn, sa_xd))
    }
    // FCVTNS  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 0, sa_sn, sa_xd))
    }
    // FCVTNS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 26, sa_vn, sa_vd))
    }
    // FCVTNS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 0, 26, sa_vn, sa_vd))
    }
    // FCVTNS  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTNS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 26, sa_n, sa_d))
    }
    // FCVTNS  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 0, 26, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTNS")
}

// FCVTNU instruction have 10 forms:
//
//   * FCVTNU  <Wd>, <Dn>
//   * FCVTNU  <Wd>, <Hn>
//   * FCVTNU  <Wd>, <Sn>
//   * FCVTNU  <Xd>, <Dn>
//   * FCVTNU  <Xd>, <Hn>
//   * FCVTNU  <Xd>, <Sn>
//   * FCVTNU  <Vd>.<T>, <Vn>.<T>
//   * FCVTNU  <Vd>.<T>, <Vn>.<T>
//   * FCVTNU  <V><d>, <V><n>
//   * FCVTNU  <Hd>, <Hn>
//
func (self *Program) FCVTNU(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTNU", 2, Operands { v0, v1 })
    // FCVTNU  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 1, sa_dn, sa_wd))
    }
    // FCVTNU  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 1, sa_hn, sa_wd))
    }
    // FCVTNU  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 1, sa_sn, sa_wd))
    }
    // FCVTNU  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 1, sa_dn, sa_xd))
    }
    // FCVTNU  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 1, sa_hn, sa_xd))
    }
    // FCVTNU  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 1, sa_sn, sa_xd))
    }
    // FCVTNU  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 26, sa_vn, sa_vd))
    }
    // FCVTNU  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 0, 26, sa_vn, sa_vd))
    }
    // FCVTNU  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTNU")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdmisc(1, size, 26, sa_n, sa_d))
    }
    // FCVTNU  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(1, 0, 26, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTNU")
}

// FCVTPS instruction have 10 forms:
//
//   * FCVTPS  <Wd>, <Dn>
//   * FCVTPS  <Wd>, <Hn>
//   * FCVTPS  <Wd>, <Sn>
//   * FCVTPS  <Xd>, <Dn>
//   * FCVTPS  <Xd>, <Hn>
//   * FCVTPS  <Xd>, <Sn>
//   * FCVTPS  <Vd>.<T>, <Vn>.<T>
//   * FCVTPS  <Vd>.<T>, <Vn>.<T>
//   * FCVTPS  <V><d>, <V><n>
//   * FCVTPS  <Hd>, <Hn>
//
func (self *Program) FCVTPS(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTPS", 2, Operands { v0, v1 })
    // FCVTPS  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 1, 0, sa_dn, sa_wd))
    }
    // FCVTPS  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 1, 0, sa_hn, sa_wd))
    }
    // FCVTPS  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 1, 0, sa_sn, sa_wd))
    }
    // FCVTPS  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 1, 0, sa_dn, sa_xd))
    }
    // FCVTPS  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 1, 0, sa_hn, sa_xd))
    }
    // FCVTPS  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 1, 0, sa_sn, sa_xd))
    }
    // FCVTPS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 26, sa_vn, sa_vd))
    }
    // FCVTPS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 1, 26, sa_vn, sa_vd))
    }
    // FCVTPS  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTPS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 26, sa_n, sa_d))
    }
    // FCVTPS  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 1, 26, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTPS")
}

// FCVTPU instruction have 10 forms:
//
//   * FCVTPU  <Wd>, <Dn>
//   * FCVTPU  <Wd>, <Hn>
//   * FCVTPU  <Wd>, <Sn>
//   * FCVTPU  <Xd>, <Dn>
//   * FCVTPU  <Xd>, <Hn>
//   * FCVTPU  <Xd>, <Sn>
//   * FCVTPU  <Vd>.<T>, <Vn>.<T>
//   * FCVTPU  <Vd>.<T>, <Vn>.<T>
//   * FCVTPU  <V><d>, <V><n>
//   * FCVTPU  <Hd>, <Hn>
//
func (self *Program) FCVTPU(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTPU", 2, Operands { v0, v1 })
    // FCVTPU  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 1, 1, sa_dn, sa_wd))
    }
    // FCVTPU  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 1, 1, sa_hn, sa_wd))
    }
    // FCVTPU  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 1, 1, sa_sn, sa_wd))
    }
    // FCVTPU  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 1, 1, sa_dn, sa_xd))
    }
    // FCVTPU  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 1, 1, sa_hn, sa_xd))
    }
    // FCVTPU  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 1, 1, sa_sn, sa_xd))
    }
    // FCVTPU  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 26, sa_vn, sa_vd))
    }
    // FCVTPU  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 1, 26, sa_vn, sa_vd))
    }
    // FCVTPU  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTPU")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(1, size, 26, sa_n, sa_d))
    }
    // FCVTPU  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(1, 1, 26, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTPU")
}

// FCVTXN instruction have 2 forms:
//
//   * FCVTXN  <Vb><d>, <Va><n>
//   * FCVTXN  <Vd>.<Tb>, <Vn>.<Ta>
//
func (self *Program) FCVTXN(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTXN", 2, Operands { v0, v1 })
    // FCVTXN  <Vb><d>, <Va><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) {
        var sa_va uint32
        var sa_vb uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_vb = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTXN")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case DRegister: sa_va = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTXN")
        }
        size := uint32(0b00)
        size |= sa_vb
        if sa_vb != sa_va {
            panic("aarch64: invalid combination of operands for FCVTXN")
        }
        return p.setins(asisdmisc(1, size, 22, sa_n, sa_d))
    }
    // FCVTXN  <Vd>.<Tb>, <Vn>.<Ta>
    if isVr(v0) && isVfmt(v0, Vec2S, Vec4S) && isVr(v1) && vfmt(v1) == Vec2D {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_tb = 0b10
            case Vec4S: sa_tb = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2D: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= maskp(sa_tb, 1, 1)
        if maskp(sa_tb, 1, 1) != sa_ta {
            panic("aarch64: invalid combination of operands for FCVTXN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for FCVTXN")
        }
        return p.setins(asimdmisc(0, 1, size, 22, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTXN")
}

// FCVTXN2 instruction have one single form:
//
//   * FCVTXN2  <Vd>.<Tb>, <Vn>.<Ta>
//
func (self *Program) FCVTXN2(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTXN2", 2, Operands { v0, v1 })
    if isVr(v0) && isVfmt(v0, Vec2S, Vec4S) && isVr(v1) && vfmt(v1) == Vec2D {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_tb = 0b10
            case Vec4S: sa_tb = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2D: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= maskp(sa_tb, 1, 1)
        if maskp(sa_tb, 1, 1) != sa_ta {
            panic("aarch64: invalid combination of operands for FCVTXN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for FCVTXN2")
        }
        return p.setins(asimdmisc(1, 1, size, 22, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTXN2")
}

// FCVTZS instruction have 18 forms:
//
//   * FCVTZS  <Wd>, <Dn>, #<fbits>
//   * FCVTZS  <Wd>, <Dn>
//   * FCVTZS  <Wd>, <Hn>, #<fbits>
//   * FCVTZS  <Wd>, <Hn>
//   * FCVTZS  <Wd>, <Sn>, #<fbits>
//   * FCVTZS  <Wd>, <Sn>
//   * FCVTZS  <Xd>, <Dn>, #<fbits>
//   * FCVTZS  <Xd>, <Dn>
//   * FCVTZS  <Xd>, <Hn>, #<fbits>
//   * FCVTZS  <Xd>, <Hn>
//   * FCVTZS  <Xd>, <Sn>, #<fbits>
//   * FCVTZS  <Xd>, <Sn>
//   * FCVTZS  <Vd>.<T>, <Vn>.<T>
//   * FCVTZS  <Vd>.<T>, <Vn>.<T>
//   * FCVTZS  <Vd>.<T>, <Vn>.<T>, #<fbits>
//   * FCVTZS  <V><d>, <V><n>
//   * FCVTZS  <Hd>, <Hn>
//   * FCVTZS  <V><d>, <V><n>, #<fbits>
//
func (self *Program) FCVTZS(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("FCVTZS", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("FCVTZS", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction FCVTZS takes 2 or 3 operands")
    }
    // FCVTZS  <Wd>, <Dn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isDr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 1, 3, 0, sa_fbits, sa_dn, sa_wd))
    }
    // FCVTZS  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 3, 0, sa_dn, sa_wd))
    }
    // FCVTZS  <Wd>, <Hn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isHr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 3, 3, 0, sa_fbits, sa_hn, sa_wd))
    }
    // FCVTZS  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 3, 0, sa_hn, sa_wd))
    }
    // FCVTZS  <Wd>, <Sn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isSr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 0, 3, 0, sa_fbits, sa_sn, sa_wd))
    }
    // FCVTZS  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 3, 0, sa_sn, sa_wd))
    }
    // FCVTZS  <Xd>, <Dn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isDr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 1, 3, 0, sa_fbits_1, sa_dn, sa_xd))
    }
    // FCVTZS  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 3, 0, sa_dn, sa_xd))
    }
    // FCVTZS  <Xd>, <Hn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isHr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 3, 3, 0, sa_fbits_1, sa_hn, sa_xd))
    }
    // FCVTZS  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 3, 0, sa_hn, sa_xd))
    }
    // FCVTZS  <Xd>, <Sn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isSr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 0, 3, 0, sa_fbits_1, sa_sn, sa_xd))
    }
    // FCVTZS  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 3, 0, sa_sn, sa_xd))
    }
    // FCVTZS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 27, sa_vn, sa_vd))
    }
    // FCVTZS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 1, 27, sa_vn, sa_vd))
    }
    // FCVTZS  <Vd>.<T>, <Vn>.<T>, #<fbits>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(vv[0]) &&
       vfmt(v0) == vfmt(v1) {
        var sa_fbits uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0010: sa_fbits = 32 - uint32(asLit(vv[0]))
            case 0b0100: sa_fbits = 64 - uint32(asLit(vv[0]))
            case 0b1000: sa_fbits = 128 - uint32(asLit(vv[0]))
            default: panic("aarch64: invalid operand 'sa_fbits' for FCVTZS")
        }
        if maskp(sa_fbits, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for FCVTZS")
        }
        return p.setins(asimdshf(sa_t & 1, 0, maskp(sa_fbits, 3, 4), mask(sa_fbits, 3), 31, sa_vn, sa_vd))
    }
    // FCVTZS  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTZS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 27, sa_n, sa_d))
    }
    // FCVTZS  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 1, 27, sa_hn, sa_hd))
    }
    // FCVTZS  <V><d>, <V><n>, #<fbits>
    if len(vv) == 1 && isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(vv[0]) && isSameType(v0, v1) {
        var sa_fbits_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b0010
            case SRegister: sa_v = 0b0100
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for FCVTZS")
        }
        switch v0.(type) {
            case HRegister: sa_v__bit_mask = 0b1110
            case SRegister: sa_v__bit_mask = 0b1100
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for FCVTZS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b0010: sa_fbits_1 = 32 - uint32(asLit(vv[0]))
            case 0b0100: sa_fbits_1 = 64 - uint32(asLit(vv[0]))
            case 0b1000: sa_fbits_1 = 128 - uint32(asLit(vv[0]))
            default: panic("aarch64: invalid operand 'sa_fbits_1' for FCVTZS")
        }
        if maskp(sa_fbits_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for FCVTZS")
        }
        return p.setins(asisdshf(0, maskp(sa_fbits_1, 3, 4), mask(sa_fbits_1, 3), 31, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTZS")
}

// FCVTZU instruction have 18 forms:
//
//   * FCVTZU  <Wd>, <Dn>, #<fbits>
//   * FCVTZU  <Wd>, <Dn>
//   * FCVTZU  <Wd>, <Hn>, #<fbits>
//   * FCVTZU  <Wd>, <Hn>
//   * FCVTZU  <Wd>, <Sn>, #<fbits>
//   * FCVTZU  <Wd>, <Sn>
//   * FCVTZU  <Xd>, <Dn>, #<fbits>
//   * FCVTZU  <Xd>, <Dn>
//   * FCVTZU  <Xd>, <Hn>, #<fbits>
//   * FCVTZU  <Xd>, <Hn>
//   * FCVTZU  <Xd>, <Sn>, #<fbits>
//   * FCVTZU  <Xd>, <Sn>
//   * FCVTZU  <Vd>.<T>, <Vn>.<T>
//   * FCVTZU  <Vd>.<T>, <Vn>.<T>
//   * FCVTZU  <Vd>.<T>, <Vn>.<T>, #<fbits>
//   * FCVTZU  <V><d>, <V><n>
//   * FCVTZU  <Hd>, <Hn>
//   * FCVTZU  <V><d>, <V><n>, #<fbits>
//
func (self *Program) FCVTZU(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("FCVTZU", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("FCVTZU", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction FCVTZU takes 2 or 3 operands")
    }
    // FCVTZU  <Wd>, <Dn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isDr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 1, 3, 1, sa_fbits, sa_dn, sa_wd))
    }
    // FCVTZU  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 3, 1, sa_dn, sa_wd))
    }
    // FCVTZU  <Wd>, <Hn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isHr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 3, 3, 1, sa_fbits, sa_hn, sa_wd))
    }
    // FCVTZU  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 3, 1, sa_hn, sa_wd))
    }
    // FCVTZU  <Wd>, <Sn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isSr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 0, 3, 1, sa_fbits, sa_sn, sa_wd))
    }
    // FCVTZU  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 3, 1, sa_sn, sa_wd))
    }
    // FCVTZU  <Xd>, <Dn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isDr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 1, 3, 1, sa_fbits_1, sa_dn, sa_xd))
    }
    // FCVTZU  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 3, 1, sa_dn, sa_xd))
    }
    // FCVTZU  <Xd>, <Hn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isHr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 3, 3, 1, sa_fbits_1, sa_hn, sa_xd))
    }
    // FCVTZU  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 3, 1, sa_hn, sa_xd))
    }
    // FCVTZU  <Xd>, <Sn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isSr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 0, 3, 1, sa_fbits_1, sa_sn, sa_xd))
    }
    // FCVTZU  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 3, 1, sa_sn, sa_xd))
    }
    // FCVTZU  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 27, sa_vn, sa_vd))
    }
    // FCVTZU  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 1, 27, sa_vn, sa_vd))
    }
    // FCVTZU  <Vd>.<T>, <Vn>.<T>, #<fbits>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(vv[0]) &&
       vfmt(v0) == vfmt(v1) {
        var sa_fbits uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0010: sa_fbits = 32 - uint32(asLit(vv[0]))
            case 0b0100: sa_fbits = 64 - uint32(asLit(vv[0]))
            case 0b1000: sa_fbits = 128 - uint32(asLit(vv[0]))
            default: panic("aarch64: invalid operand 'sa_fbits' for FCVTZU")
        }
        if maskp(sa_fbits, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for FCVTZU")
        }
        return p.setins(asimdshf(sa_t & 1, 1, maskp(sa_fbits, 3, 4), mask(sa_fbits, 3), 31, sa_vn, sa_vd))
    }
    // FCVTZU  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FCVTZU")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(1, size, 27, sa_n, sa_d))
    }
    // FCVTZU  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(1, 1, 27, sa_hn, sa_hd))
    }
    // FCVTZU  <V><d>, <V><n>, #<fbits>
    if len(vv) == 1 && isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(vv[0]) && isSameType(v0, v1) {
        var sa_fbits_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b0010
            case SRegister: sa_v = 0b0100
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for FCVTZU")
        }
        switch v0.(type) {
            case HRegister: sa_v__bit_mask = 0b1110
            case SRegister: sa_v__bit_mask = 0b1100
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for FCVTZU")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b0010: sa_fbits_1 = 32 - uint32(asLit(vv[0]))
            case 0b0100: sa_fbits_1 = 64 - uint32(asLit(vv[0]))
            case 0b1000: sa_fbits_1 = 128 - uint32(asLit(vv[0]))
            default: panic("aarch64: invalid operand 'sa_fbits_1' for FCVTZU")
        }
        if maskp(sa_fbits_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for FCVTZU")
        }
        return p.setins(asisdshf(1, maskp(sa_fbits_1, 3, 4), mask(sa_fbits_1, 3), 31, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTZU")
}

// FDIV instruction have 5 forms:
//
//   * FDIV  <Dd>, <Dn>, <Dm>
//   * FDIV  <Hd>, <Hn>, <Hm>
//   * FDIV  <Sd>, <Sn>, <Sm>
//   * FDIV  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FDIV  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) FDIV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FDIV", 3, Operands { v0, v1, v2 })
    // FDIV  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 1, sa_dn, sa_dd))
    }
    // FDIV  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 1, sa_hn, sa_hd))
    }
    // FDIV  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 1, sa_sn, sa_sd))
    }
    // FDIV  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 1, size, sa_vm, 31, sa_vn, sa_vd))
    }
    // FDIV  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 1, 0, sa_vm, 7, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FDIV")
}

// FJCVTZS instruction have one single form:
//
//   * FJCVTZS  <Wd>, <Dn>
//
func (self *Program) FJCVTZS(v0, v1 interface{}) *Instruction {
    p := self.alloc("FJCVTZS", 2, Operands { v0, v1 })
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 3, 6, sa_dn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for FJCVTZS")
}

// FMADD instruction have 3 forms:
//
//   * FMADD  <Dd>, <Dn>, <Dm>, <Da>
//   * FMADD  <Hd>, <Hn>, <Hm>, <Ha>
//   * FMADD  <Sd>, <Sn>, <Sm>, <Sa>
//
func (self *Program) FMADD(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FMADD", 4, Operands { v0, v1, v2, v3 })
    // FMADD  <Dd>, <Dn>, <Dm>, <Da>
    if isDr(v0) && isDr(v1) && isDr(v2) && isDr(v3) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        sa_da := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 1, 0, sa_dm, 0, sa_da, sa_dn, sa_dd))
    }
    // FMADD  <Hd>, <Hn>, <Hm>, <Ha>
    if isHr(v0) && isHr(v1) && isHr(v2) && isHr(v3) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        sa_ha := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 3, 0, sa_hm, 0, sa_ha, sa_hn, sa_hd))
    }
    // FMADD  <Sd>, <Sn>, <Sm>, <Sa>
    if isSr(v0) && isSr(v1) && isSr(v2) && isSr(v3) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        sa_sa := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 0, 0, sa_sm, 0, sa_sa, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMADD")
}

// FMAX instruction have 5 forms:
//
//   * FMAX  <Dd>, <Dn>, <Dm>
//   * FMAX  <Hd>, <Hn>, <Hm>
//   * FMAX  <Sd>, <Sn>, <Sm>
//   * FMAX  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMAX  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) FMAX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMAX", 3, Operands { v0, v1, v2 })
    // FMAX  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 4, sa_dn, sa_dd))
    }
    // FMAX  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 4, sa_hn, sa_hd))
    }
    // FMAX  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 4, sa_sn, sa_sd))
    }
    // FMAX  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 0, size, sa_vm, 30, sa_vn, sa_vd))
    }
    // FMAX  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 0, 0, sa_vm, 6, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMAX")
}

// FMAXNM instruction have 5 forms:
//
//   * FMAXNM  <Dd>, <Dn>, <Dm>
//   * FMAXNM  <Hd>, <Hn>, <Hm>
//   * FMAXNM  <Sd>, <Sn>, <Sm>
//   * FMAXNM  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMAXNM  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) FMAXNM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMAXNM", 3, Operands { v0, v1, v2 })
    // FMAXNM  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 6, sa_dn, sa_dd))
    }
    // FMAXNM  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 6, sa_hn, sa_hd))
    }
    // FMAXNM  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 6, sa_sn, sa_sd))
    }
    // FMAXNM  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 0, size, sa_vm, 24, sa_vn, sa_vd))
    }
    // FMAXNM  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 0, 0, sa_vm, 0, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMAXNM")
}

// FMAXNMP instruction have 4 forms:
//
//   * FMAXNMP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMAXNMP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMAXNMP  <V><d>, <Vn>.<T>
//   * FMAXNMP  <V><d>, <Vn>.<T>
//
func (self *Program) FMAXNMP(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("FMAXNMP", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("FMAXNMP", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction FMAXNMP takes 2 or 3 operands")
    }
    // FMAXNMP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 1, size, sa_vm, 24, sa_vn, sa_vd))
    }
    // FMAXNMP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 1, 0, sa_vm, 0, sa_vn, sa_vd))
    }
    // FMAXNMP  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && vfmt(v1) == Vec2H {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b0
            default: panic("aarch64: invalid scalar operand size for FMAXNMP")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_t = 0b0
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= sa_v
        if sa_v != sa_t {
            panic("aarch64: invalid combination of operands for FMAXNMP")
        }
        return p.setins(asisdpair(0, size, 12, sa_vn, sa_d))
    }
    // FMAXNMP  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && isVfmt(v1, Vec2S, Vec2D) {
        var sa_t_1 uint32
        var sa_v_1 uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v_1 = 0b0
            case DRegister: sa_v_1 = 0b1
            default: panic("aarch64: invalid scalar operand size for FMAXNMP")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2S: sa_t_1 = 0b0
            case Vec2D: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= sa_v_1
        if sa_v_1 != sa_t_1 {
            panic("aarch64: invalid combination of operands for FMAXNMP")
        }
        return p.setins(asisdpair(1, size, 12, sa_vn, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMAXNMP")
}

// FMAXNMV instruction have 2 forms:
//
//   * FMAXNMV  <V><d>, <Vn>.<T>
//   * FMAXNMV  <V><d>, <Vn>.<T>
//
func (self *Program) FMAXNMV(v0, v1 interface{}) *Instruction {
    p := self.alloc("FMAXNMV", 2, Operands { v0, v1 })
    // FMAXNMV  <V><d>, <Vn>.<T>
    if isHr(v0) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) {
        var sa_t uint32
        sa_d := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        return p.setins(asimdall(sa_t, 0, 0, 12, sa_vn, sa_d))
    }
    // FMAXNMV  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && vfmt(v1) == Vec4S {
        var sa_t_1 uint32
        var sa_v_1 uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v_1 = 0b0
            default: panic("aarch64: invalid scalar operand size for FMAXNMV")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4S: sa_t_1 = 0b10
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= sa_v_1
        if sa_v_1 != sa_t_1 & 1 {
            panic("aarch64: invalid combination of operands for FMAXNMV")
        }
        return p.setins(asimdall(maskp(sa_t_1, 1, 1), 1, size, 12, sa_vn, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMAXNMV")
}

// FMAXP instruction have 4 forms:
//
//   * FMAXP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMAXP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMAXP  <V><d>, <Vn>.<T>
//   * FMAXP  <V><d>, <Vn>.<T>
//
func (self *Program) FMAXP(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("FMAXP", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("FMAXP", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction FMAXP takes 2 or 3 operands")
    }
    // FMAXP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 1, size, sa_vm, 30, sa_vn, sa_vd))
    }
    // FMAXP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 1, 0, sa_vm, 6, sa_vn, sa_vd))
    }
    // FMAXP  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && vfmt(v1) == Vec2H {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b0
            default: panic("aarch64: invalid scalar operand size for FMAXP")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_t = 0b0
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= sa_v
        if sa_v != sa_t {
            panic("aarch64: invalid combination of operands for FMAXP")
        }
        return p.setins(asisdpair(0, size, 15, sa_vn, sa_d))
    }
    // FMAXP  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && isVfmt(v1, Vec2S, Vec2D) {
        var sa_t_1 uint32
        var sa_v_1 uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v_1 = 0b0
            case DRegister: sa_v_1 = 0b1
            default: panic("aarch64: invalid scalar operand size for FMAXP")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2S: sa_t_1 = 0b0
            case Vec2D: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= sa_v_1
        if sa_v_1 != sa_t_1 {
            panic("aarch64: invalid combination of operands for FMAXP")
        }
        return p.setins(asisdpair(1, size, 15, sa_vn, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMAXP")
}

// FMAXV instruction have 2 forms:
//
//   * FMAXV  <V><d>, <Vn>.<T>
//   * FMAXV  <V><d>, <Vn>.<T>
//
func (self *Program) FMAXV(v0, v1 interface{}) *Instruction {
    p := self.alloc("FMAXV", 2, Operands { v0, v1 })
    // FMAXV  <V><d>, <Vn>.<T>
    if isHr(v0) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) {
        var sa_t uint32
        sa_d := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        return p.setins(asimdall(sa_t, 0, 0, 15, sa_vn, sa_d))
    }
    // FMAXV  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && vfmt(v1) == Vec4S {
        var sa_t_1 uint32
        var sa_v_1 uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v_1 = 0b0
            default: panic("aarch64: invalid scalar operand size for FMAXV")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4S: sa_t_1 = 0b10
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b00)
        size |= sa_v_1
        if sa_v_1 != sa_t_1 & 1 {
            panic("aarch64: invalid combination of operands for FMAXV")
        }
        return p.setins(asimdall(maskp(sa_t_1, 1, 1), 1, size, 15, sa_vn, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMAXV")
}

// FMIN instruction have 5 forms:
//
//   * FMIN  <Dd>, <Dn>, <Dm>
//   * FMIN  <Hd>, <Hn>, <Hm>
//   * FMIN  <Sd>, <Sn>, <Sm>
//   * FMIN  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMIN  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) FMIN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMIN", 3, Operands { v0, v1, v2 })
    // FMIN  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 5, sa_dn, sa_dd))
    }
    // FMIN  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 5, sa_hn, sa_hd))
    }
    // FMIN  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 5, sa_sn, sa_sd))
    }
    // FMIN  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 0, size, sa_vm, 30, sa_vn, sa_vd))
    }
    // FMIN  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 0, 1, sa_vm, 6, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMIN")
}

// FMINNM instruction have 5 forms:
//
//   * FMINNM  <Dd>, <Dn>, <Dm>
//   * FMINNM  <Hd>, <Hn>, <Hm>
//   * FMINNM  <Sd>, <Sn>, <Sm>
//   * FMINNM  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMINNM  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) FMINNM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMINNM", 3, Operands { v0, v1, v2 })
    // FMINNM  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 7, sa_dn, sa_dd))
    }
    // FMINNM  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 7, sa_hn, sa_hd))
    }
    // FMINNM  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 7, sa_sn, sa_sd))
    }
    // FMINNM  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 0, size, sa_vm, 24, sa_vn, sa_vd))
    }
    // FMINNM  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 0, 1, sa_vm, 0, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMINNM")
}

// FMINNMP instruction have 4 forms:
//
//   * FMINNMP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMINNMP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMINNMP  <V><d>, <Vn>.<T>
//   * FMINNMP  <V><d>, <Vn>.<T>
//
func (self *Program) FMINNMP(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("FMINNMP", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("FMINNMP", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction FMINNMP takes 2 or 3 operands")
    }
    // FMINNMP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 1, size, sa_vm, 24, sa_vn, sa_vd))
    }
    // FMINNMP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 1, 1, sa_vm, 0, sa_vn, sa_vd))
    }
    // FMINNMP  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && vfmt(v1) == Vec2H {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b0
            default: panic("aarch64: invalid scalar operand size for FMINNMP")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_t = 0b0
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b10)
        size |= sa_v
        if sa_v != sa_t {
            panic("aarch64: invalid combination of operands for FMINNMP")
        }
        return p.setins(asisdpair(0, size, 12, sa_vn, sa_d))
    }
    // FMINNMP  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && isVfmt(v1, Vec2S, Vec2D) {
        var sa_t_1 uint32
        var sa_v_1 uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v_1 = 0b0
            case DRegister: sa_v_1 = 0b1
            default: panic("aarch64: invalid scalar operand size for FMINNMP")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2S: sa_t_1 = 0b0
            case Vec2D: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b10)
        size |= sa_v_1
        if sa_v_1 != sa_t_1 {
            panic("aarch64: invalid combination of operands for FMINNMP")
        }
        return p.setins(asisdpair(1, size, 12, sa_vn, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMINNMP")
}

// FMINNMV instruction have 2 forms:
//
//   * FMINNMV  <V><d>, <Vn>.<T>
//   * FMINNMV  <V><d>, <Vn>.<T>
//
func (self *Program) FMINNMV(v0, v1 interface{}) *Instruction {
    p := self.alloc("FMINNMV", 2, Operands { v0, v1 })
    // FMINNMV  <V><d>, <Vn>.<T>
    if isHr(v0) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) {
        var sa_t uint32
        sa_d := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        return p.setins(asimdall(sa_t, 0, 2, 12, sa_vn, sa_d))
    }
    // FMINNMV  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && vfmt(v1) == Vec4S {
        var sa_t_1 uint32
        var sa_v_1 uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v_1 = 0b0
            default: panic("aarch64: invalid scalar operand size for FMINNMV")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4S: sa_t_1 = 0b10
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b10)
        size |= sa_v_1
        if sa_v_1 != sa_t_1 & 1 {
            panic("aarch64: invalid combination of operands for FMINNMV")
        }
        return p.setins(asimdall(maskp(sa_t_1, 1, 1), 1, size, 12, sa_vn, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMINNMV")
}

// FMINP instruction have 4 forms:
//
//   * FMINP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMINP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMINP  <V><d>, <Vn>.<T>
//   * FMINP  <V><d>, <Vn>.<T>
//
func (self *Program) FMINP(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("FMINP", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("FMINP", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction FMINP takes 2 or 3 operands")
    }
    // FMINP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 1, size, sa_vm, 30, sa_vn, sa_vd))
    }
    // FMINP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 1, 1, sa_vm, 6, sa_vn, sa_vd))
    }
    // FMINP  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && vfmt(v1) == Vec2H {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b0
            default: panic("aarch64: invalid scalar operand size for FMINP")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_t = 0b0
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b10)
        size |= sa_v
        if sa_v != sa_t {
            panic("aarch64: invalid combination of operands for FMINP")
        }
        return p.setins(asisdpair(0, size, 15, sa_vn, sa_d))
    }
    // FMINP  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && isVfmt(v1, Vec2S, Vec2D) {
        var sa_t_1 uint32
        var sa_v_1 uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v_1 = 0b0
            case DRegister: sa_v_1 = 0b1
            default: panic("aarch64: invalid scalar operand size for FMINP")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2S: sa_t_1 = 0b0
            case Vec2D: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b10)
        size |= sa_v_1
        if sa_v_1 != sa_t_1 {
            panic("aarch64: invalid combination of operands for FMINP")
        }
        return p.setins(asisdpair(1, size, 15, sa_vn, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMINP")
}

// FMINV instruction have 2 forms:
//
//   * FMINV  <V><d>, <Vn>.<T>
//   * FMINV  <V><d>, <Vn>.<T>
//
func (self *Program) FMINV(v0, v1 interface{}) *Instruction {
    p := self.alloc("FMINV", 2, Operands { v0, v1 })
    // FMINV  <V><d>, <Vn>.<T>
    if isHr(v0) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) {
        var sa_t uint32
        sa_d := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        return p.setins(asimdall(sa_t, 0, 2, 15, sa_vn, sa_d))
    }
    // FMINV  <V><d>, <Vn>.<T>
    if isAdvSIMD(v0) && isVr(v1) && vfmt(v1) == Vec4S {
        var sa_t_1 uint32
        var sa_v_1 uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v_1 = 0b0
            default: panic("aarch64: invalid scalar operand size for FMINV")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4S: sa_t_1 = 0b10
            default: panic("aarch64: unreachable")
        }
        size := uint32(0b10)
        size |= sa_v_1
        if sa_v_1 != sa_t_1 & 1 {
            panic("aarch64: invalid combination of operands for FMINV")
        }
        return p.setins(asimdall(maskp(sa_t_1, 1, 1), 1, size, 15, sa_vn, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMINV")
}

// FMLA instruction have 6 forms:
//
//   * FMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.H[<index>]
//   * FMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
//   * FMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMLA  <Hd>, <Hn>, <Vm>.H[<index>]
//   * FMLA  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
//
func (self *Program) FMLA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMLA", 3, Operands { v0, v1, v2 })
    // FMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.H[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVri(v2) &&
       vmoder(v2) == ModeH &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        return p.setins(asimdelem(
            sa_t,
            0,
            0,
            maskp(sa_index, 1, 1),
            sa_index & 1,
            sa_vm,
            1,
            maskp(sa_index, 2, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // FMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVri(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t_1 = 0b00
            case Vec4S: sa_t_1 = 0b10
            case Vec2D: sa_t_1 = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm_1 := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeS: sa_ts = 0b0
            case ModeD: sa_ts = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_index_1 := uint32(vidxr(v2))
        size := uint32(0b10)
        size |= sa_t_1 & 1
        if sa_t_1 & 1 != sa_ts || sa_ts != maskp(sa_index_1, 2, 1) {
            panic("aarch64: invalid combination of operands for FMLA")
        }
        return p.setins(asimdelem(
            maskp(sa_t_1, 1, 1),
            0,
            size,
            maskp(sa_index_1, 1, 1),
            maskp(sa_vm_1, 4, 1),
            mask(sa_vm_1, 4),
            1,
            sa_index_1 & 1,
            sa_vn,
            sa_vd,
        ))
    }
    // FMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 0, size, sa_vm, 25, sa_vn, sa_vd))
    }
    // FMLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 0, 0, sa_vm, 1, sa_vn, sa_vd))
    }
    // FMLA  <Hd>, <Hn>, <Vm>.H[<index>]
    if isHr(v0) && isHr(v1) && isVri(v2) && vmoder(v2) == ModeH {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        return p.setins(asisdelem(0, 0, maskp(sa_index, 1, 1), sa_index & 1, sa_vm, 1, maskp(sa_index, 2, 1), sa_hn, sa_hd))
    }
    // FMLA  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isVri(v2) && isSameType(v0, v1) {
        var sa_ts uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FMLA")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_vm_1 := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeS: sa_ts = 0b0
            case ModeD: sa_ts = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_index_1 := uint32(vidxr(v2))
        size := uint32(0b10)
        size |= sa_v
        if sa_v != sa_ts || sa_ts != maskp(sa_index_1, 2, 1) {
            panic("aarch64: invalid combination of operands for FMLA")
        }
        return p.setins(asisdelem(
            0,
            size,
            maskp(sa_index_1, 1, 1),
            maskp(sa_vm_1, 4, 1),
            mask(sa_vm_1, 4),
            1,
            sa_index_1 & 1,
            sa_n,
            sa_d,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMLA")
}

// FMLAL instruction have 2 forms:
//
//   * FMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.H[<index>]
//   * FMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) FMLAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMLAL", 3, Operands { v0, v1, v2 })
    // FMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.H[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec2H, Vec4H) &&
       isVri(v2) &&
       vmoder(v2) == ModeH {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_tb = 0b0
            case Vec4H: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for FMLAL")
        }
        return p.setins(asimdelem(
            sa_ta,
            0,
            2,
            maskp(sa_index, 1, 1),
            sa_index & 1,
            sa_vm,
            0,
            maskp(sa_index, 2, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // FMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec2H, Vec4H) &&
       isVr(v2) &&
       isVfmt(v2, Vec2H, Vec4H) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_tb = 0b0
            case Vec4H: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for FMLAL")
        }
        return p.setins(asimdsame(sa_ta, 0, 0, sa_vm, 29, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMLAL")
}

// FMLAL2 instruction have 2 forms:
//
//   * FMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.H[<index>]
//   * FMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) FMLAL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMLAL2", 3, Operands { v0, v1, v2 })
    // FMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.H[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec2H, Vec4H) &&
       isVri(v2) &&
       vmoder(v2) == ModeH {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_tb = 0b0
            case Vec4H: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for FMLAL2")
        }
        return p.setins(asimdelem(
            sa_ta,
            1,
            2,
            maskp(sa_index, 1, 1),
            sa_index & 1,
            sa_vm,
            8,
            maskp(sa_index, 2, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // FMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec2H, Vec4H) &&
       isVr(v2) &&
       isVfmt(v2, Vec2H, Vec4H) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_tb = 0b0
            case Vec4H: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for FMLAL2")
        }
        return p.setins(asimdsame(sa_ta, 1, 0, sa_vm, 25, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMLAL2")
}

// FMLS instruction have 6 forms:
//
//   * FMLS  <Vd>.<T>, <Vn>.<T>, <Vm>.H[<index>]
//   * FMLS  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
//   * FMLS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMLS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMLS  <Hd>, <Hn>, <Vm>.H[<index>]
//   * FMLS  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
//
func (self *Program) FMLS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMLS", 3, Operands { v0, v1, v2 })
    // FMLS  <Vd>.<T>, <Vn>.<T>, <Vm>.H[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVri(v2) &&
       vmoder(v2) == ModeH &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        return p.setins(asimdelem(
            sa_t,
            0,
            0,
            maskp(sa_index, 1, 1),
            sa_index & 1,
            sa_vm,
            5,
            maskp(sa_index, 2, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // FMLS  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVri(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t_1 = 0b00
            case Vec4S: sa_t_1 = 0b10
            case Vec2D: sa_t_1 = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm_1 := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeS: sa_ts = 0b0
            case ModeD: sa_ts = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_index_1 := uint32(vidxr(v2))
        size := uint32(0b10)
        size |= sa_t_1 & 1
        if sa_t_1 & 1 != sa_ts || sa_ts != maskp(sa_index_1, 2, 1) {
            panic("aarch64: invalid combination of operands for FMLS")
        }
        return p.setins(asimdelem(
            maskp(sa_t_1, 1, 1),
            0,
            size,
            maskp(sa_index_1, 1, 1),
            maskp(sa_vm_1, 4, 1),
            mask(sa_vm_1, 4),
            5,
            sa_index_1 & 1,
            sa_vn,
            sa_vd,
        ))
    }
    // FMLS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 0, size, sa_vm, 25, sa_vn, sa_vd))
    }
    // FMLS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 0, 1, sa_vm, 1, sa_vn, sa_vd))
    }
    // FMLS  <Hd>, <Hn>, <Vm>.H[<index>]
    if isHr(v0) && isHr(v1) && isVri(v2) && vmoder(v2) == ModeH {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        return p.setins(asisdelem(0, 0, maskp(sa_index, 1, 1), sa_index & 1, sa_vm, 5, maskp(sa_index, 2, 1), sa_hn, sa_hd))
    }
    // FMLS  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isVri(v2) && isSameType(v0, v1) {
        var sa_ts uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FMLS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_vm_1 := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeS: sa_ts = 0b0
            case ModeD: sa_ts = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_index_1 := uint32(vidxr(v2))
        size := uint32(0b10)
        size |= sa_v
        if sa_v != sa_ts || sa_ts != maskp(sa_index_1, 2, 1) {
            panic("aarch64: invalid combination of operands for FMLS")
        }
        return p.setins(asisdelem(
            0,
            size,
            maskp(sa_index_1, 1, 1),
            maskp(sa_vm_1, 4, 1),
            mask(sa_vm_1, 4),
            5,
            sa_index_1 & 1,
            sa_n,
            sa_d,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMLS")
}

// FMLSL instruction have 2 forms:
//
//   * FMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.H[<index>]
//   * FMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) FMLSL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMLSL", 3, Operands { v0, v1, v2 })
    // FMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.H[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec2H, Vec4H) &&
       isVri(v2) &&
       vmoder(v2) == ModeH {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_tb = 0b0
            case Vec4H: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for FMLSL")
        }
        return p.setins(asimdelem(
            sa_ta,
            0,
            2,
            maskp(sa_index, 1, 1),
            sa_index & 1,
            sa_vm,
            4,
            maskp(sa_index, 2, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // FMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec2H, Vec4H) &&
       isVr(v2) &&
       isVfmt(v2, Vec2H, Vec4H) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_tb = 0b0
            case Vec4H: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for FMLSL")
        }
        return p.setins(asimdsame(sa_ta, 0, 2, sa_vm, 29, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMLSL")
}

// FMLSL2 instruction have 2 forms:
//
//   * FMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.H[<index>]
//   * FMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) FMLSL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMLSL2", 3, Operands { v0, v1, v2 })
    // FMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.H[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec2H, Vec4H) &&
       isVri(v2) &&
       vmoder(v2) == ModeH {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_tb = 0b0
            case Vec4H: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for FMLSL2")
        }
        return p.setins(asimdelem(
            sa_ta,
            1,
            2,
            maskp(sa_index, 1, 1),
            sa_index & 1,
            sa_vm,
            12,
            maskp(sa_index, 2, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // FMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec2H, Vec4H) &&
       isVr(v2) &&
       isVfmt(v2, Vec2H, Vec4H) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec2H: sa_tb = 0b0
            case Vec4H: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for FMLSL2")
        }
        return p.setins(asimdsame(sa_ta, 1, 2, sa_vm, 25, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMLSL2")
}

// FMOV instruction have 19 forms:
//
//   * FMOV  <Wd>, <Hn>
//   * FMOV  <Wd>, <Sn>
//   * FMOV  <Xd>, <Dn>
//   * FMOV  <Xd>, <Hn>
//   * FMOV  <Xd>, <Vn>.D[1]
//   * FMOV  <Dd>, <Xn>
//   * FMOV  <Dd>, <Dn>
//   * FMOV  <Dd>, #<imm>
//   * FMOV  <Hd>, <Wn>
//   * FMOV  <Hd>, <Xn>
//   * FMOV  <Hd>, <Hn>
//   * FMOV  <Hd>, #<imm>
//   * FMOV  <Sd>, <Wn>
//   * FMOV  <Sd>, <Sn>
//   * FMOV  <Sd>, #<imm>
//   * FMOV  <Vd>.D[1], <Xn>
//   * FMOV  <Vd>.2D, #<imm>
//   * FMOV  <Vd>.<T>, #<imm>
//   * FMOV  <Vd>.<T>, #<imm>
//
func (self *Program) FMOV(v0, v1 interface{}) *Instruction {
    p := self.alloc("FMOV", 2, Operands { v0, v1 })
    // FMOV  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 6, sa_hn, sa_wd))
    }
    // FMOV  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 6, sa_sn, sa_wd))
    }
    // FMOV  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 6, sa_dn, sa_xd))
    }
    // FMOV  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 6, sa_hn, sa_xd))
    }
    // FMOV  <Xd>, <Vn>.D[1]
    if isXr(v0) && isVri(v1) && vmoder(v1) == ModeD && vidxr(v1) == 1 {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(VidxRegister).ID())
        return p.setins(float2int(1, 0, 2, 1, 6, sa_vn, sa_xd))
    }
    // FMOV  <Dd>, <Xn>
    if isDr(v0) && isXr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 7, sa_xn, sa_dd))
    }
    // FMOV  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 0, sa_dn, sa_dd))
    }
    // FMOV  <Dd>, #<imm>
    if isDr(v0) && isFpImm8(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_imm := asFpImm8(v1)
        return p.setins(floatimm(0, 0, 1, sa_imm, 0, sa_dd))
    }
    // FMOV  <Hd>, <Wn>
    if isHr(v0) && isWr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 7, sa_wn, sa_hd))
    }
    // FMOV  <Hd>, <Xn>
    if isHr(v0) && isXr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 7, sa_xn, sa_hd))
    }
    // FMOV  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 0, sa_hn, sa_hd))
    }
    // FMOV  <Hd>, #<imm>
    if isHr(v0) && isFpImm8(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_imm := asFpImm8(v1)
        return p.setins(floatimm(0, 0, 3, sa_imm, 0, sa_hd))
    }
    // FMOV  <Sd>, <Wn>
    if isSr(v0) && isWr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 7, sa_wn, sa_sd))
    }
    // FMOV  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 0, sa_sn, sa_sd))
    }
    // FMOV  <Sd>, #<imm>
    if isSr(v0) && isFpImm8(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_imm := asFpImm8(v1)
        return p.setins(floatimm(0, 0, 0, sa_imm, 0, sa_sd))
    }
    // FMOV  <Vd>.D[1], <Xn>
    if isVri(v0) && vmoder(v0) == ModeD && vidxr(v0) == 1 && isXr(v1) {
        sa_vd := uint32(v0.(VidxRegister).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 2, 1, 7, sa_xn, sa_vd))
    }
    // FMOV  <Vd>.2D, #<imm>
    if isVr(v0) && vfmt(v0) == Vec2D && isUimm8(v1) {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm8(v1)
        return p.setins(asimdimm(
            1,
            1,
            maskp(sa_imm, 7, 1),
            maskp(sa_imm, 6, 1),
            maskp(sa_imm, 5, 1),
            15,
            0,
            maskp(sa_imm, 4, 1),
            maskp(sa_imm, 3, 1),
            maskp(sa_imm, 2, 1),
            maskp(sa_imm, 1, 1),
            sa_imm & 1,
            sa_vd,
        ))
    }
    // FMOV  <Vd>.<T>, #<imm>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isUimm8(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm := asUimm8(v1)
        return p.setins(asimdimm(
            sa_t_1,
            0,
            maskp(sa_imm, 7, 1),
            maskp(sa_imm, 6, 1),
            maskp(sa_imm, 5, 1),
            15,
            1,
            maskp(sa_imm, 4, 1),
            maskp(sa_imm, 3, 1),
            maskp(sa_imm, 2, 1),
            maskp(sa_imm, 1, 1),
            sa_imm & 1,
            sa_vd,
        ))
    }
    // FMOV  <Vd>.<T>, #<imm>
    if isVr(v0) && isVfmt(v0, Vec2S, Vec4S) && isUimm8(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b0
            case Vec4S: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm := asUimm8(v1)
        return p.setins(asimdimm(
            sa_t,
            0,
            maskp(sa_imm, 7, 1),
            maskp(sa_imm, 6, 1),
            maskp(sa_imm, 5, 1),
            15,
            0,
            maskp(sa_imm, 4, 1),
            maskp(sa_imm, 3, 1),
            maskp(sa_imm, 2, 1),
            maskp(sa_imm, 1, 1),
            sa_imm & 1,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMOV")
}

// FMSUB instruction have 3 forms:
//
//   * FMSUB  <Dd>, <Dn>, <Dm>, <Da>
//   * FMSUB  <Hd>, <Hn>, <Hm>, <Ha>
//   * FMSUB  <Sd>, <Sn>, <Sm>, <Sa>
//
func (self *Program) FMSUB(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FMSUB", 4, Operands { v0, v1, v2, v3 })
    // FMSUB  <Dd>, <Dn>, <Dm>, <Da>
    if isDr(v0) && isDr(v1) && isDr(v2) && isDr(v3) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        sa_da := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 1, 0, sa_dm, 1, sa_da, sa_dn, sa_dd))
    }
    // FMSUB  <Hd>, <Hn>, <Hm>, <Ha>
    if isHr(v0) && isHr(v1) && isHr(v2) && isHr(v3) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        sa_ha := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 3, 0, sa_hm, 1, sa_ha, sa_hn, sa_hd))
    }
    // FMSUB  <Sd>, <Sn>, <Sm>, <Sa>
    if isSr(v0) && isSr(v1) && isSr(v2) && isSr(v3) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        sa_sa := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 0, 0, sa_sm, 1, sa_sa, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMSUB")
}

// FMUL instruction have 9 forms:
//
//   * FMUL  <Dd>, <Dn>, <Dm>
//   * FMUL  <Hd>, <Hn>, <Hm>
//   * FMUL  <Sd>, <Sn>, <Sm>
//   * FMUL  <Vd>.<T>, <Vn>.<T>, <Vm>.H[<index>]
//   * FMUL  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
//   * FMUL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMUL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMUL  <Hd>, <Hn>, <Vm>.H[<index>]
//   * FMUL  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
//
func (self *Program) FMUL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMUL", 3, Operands { v0, v1, v2 })
    // FMUL  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 0, sa_dn, sa_dd))
    }
    // FMUL  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 0, sa_hn, sa_hd))
    }
    // FMUL  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 0, sa_sn, sa_sd))
    }
    // FMUL  <Vd>.<T>, <Vn>.<T>, <Vm>.H[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVri(v2) &&
       vmoder(v2) == ModeH &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        return p.setins(asimdelem(
            sa_t,
            0,
            0,
            maskp(sa_index, 1, 1),
            sa_index & 1,
            sa_vm,
            9,
            maskp(sa_index, 2, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // FMUL  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVri(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t_1 = 0b00
            case Vec4S: sa_t_1 = 0b10
            case Vec2D: sa_t_1 = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm_1 := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeS: sa_ts = 0b0
            case ModeD: sa_ts = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_index_1 := uint32(vidxr(v2))
        size := uint32(0b10)
        size |= sa_t_1 & 1
        if sa_t_1 & 1 != sa_ts || sa_ts != maskp(sa_index_1, 2, 1) {
            panic("aarch64: invalid combination of operands for FMUL")
        }
        return p.setins(asimdelem(
            maskp(sa_t_1, 1, 1),
            0,
            size,
            maskp(sa_index_1, 1, 1),
            maskp(sa_vm_1, 4, 1),
            mask(sa_vm_1, 4),
            9,
            sa_index_1 & 1,
            sa_vn,
            sa_vd,
        ))
    }
    // FMUL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 1, size, sa_vm, 27, sa_vn, sa_vd))
    }
    // FMUL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 1, 0, sa_vm, 3, sa_vn, sa_vd))
    }
    // FMUL  <Hd>, <Hn>, <Vm>.H[<index>]
    if isHr(v0) && isHr(v1) && isVri(v2) && vmoder(v2) == ModeH {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        return p.setins(asisdelem(0, 0, maskp(sa_index, 1, 1), sa_index & 1, sa_vm, 9, maskp(sa_index, 2, 1), sa_hn, sa_hd))
    }
    // FMUL  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isVri(v2) && isSameType(v0, v1) {
        var sa_ts uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FMUL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_vm_1 := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeS: sa_ts = 0b0
            case ModeD: sa_ts = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_index_1 := uint32(vidxr(v2))
        size := uint32(0b10)
        size |= sa_v
        if sa_v != sa_ts || sa_ts != maskp(sa_index_1, 2, 1) {
            panic("aarch64: invalid combination of operands for FMUL")
        }
        return p.setins(asisdelem(
            0,
            size,
            maskp(sa_index_1, 1, 1),
            maskp(sa_vm_1, 4, 1),
            mask(sa_vm_1, 4),
            9,
            sa_index_1 & 1,
            sa_n,
            sa_d,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMUL")
}

// FMULX instruction have 8 forms:
//
//   * FMULX  <Vd>.<T>, <Vn>.<T>, <Vm>.H[<index>]
//   * FMULX  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
//   * FMULX  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMULX  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FMULX  <Hd>, <Hn>, <Vm>.H[<index>]
//   * FMULX  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
//   * FMULX  <V><d>, <V><n>, <V><m>
//   * FMULX  <Hd>, <Hn>, <Hm>
//
func (self *Program) FMULX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMULX", 3, Operands { v0, v1, v2 })
    // FMULX  <Vd>.<T>, <Vn>.<T>, <Vm>.H[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVri(v2) &&
       vmoder(v2) == ModeH &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        return p.setins(asimdelem(
            sa_t,
            1,
            0,
            maskp(sa_index, 1, 1),
            sa_index & 1,
            sa_vm,
            9,
            maskp(sa_index, 2, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // FMULX  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVri(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t_1 = 0b00
            case Vec4S: sa_t_1 = 0b10
            case Vec2D: sa_t_1 = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm_1 := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeS: sa_ts = 0b0
            case ModeD: sa_ts = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_index_1 := uint32(vidxr(v2))
        size := uint32(0b10)
        size |= sa_t_1 & 1
        if sa_t_1 & 1 != sa_ts || sa_ts != maskp(sa_index_1, 2, 1) {
            panic("aarch64: invalid combination of operands for FMULX")
        }
        return p.setins(asimdelem(
            maskp(sa_t_1, 1, 1),
            1,
            size,
            maskp(sa_index_1, 1, 1),
            maskp(sa_vm_1, 4, 1),
            mask(sa_vm_1, 4),
            9,
            sa_index_1 & 1,
            sa_vn,
            sa_vd,
        ))
    }
    // FMULX  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 0, size, sa_vm, 27, sa_vn, sa_vd))
    }
    // FMULX  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 0, 0, sa_vm, 3, sa_vn, sa_vd))
    }
    // FMULX  <Hd>, <Hn>, <Vm>.H[<index>]
    if isHr(v0) && isHr(v1) && isVri(v2) && vmoder(v2) == ModeH {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        return p.setins(asisdelem(1, 0, maskp(sa_index, 1, 1), sa_index & 1, sa_vm, 9, maskp(sa_index, 2, 1), sa_hn, sa_hd))
    }
    // FMULX  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isVri(v2) && isSameType(v0, v1) {
        var sa_ts uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FMULX")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_vm_1 := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeS: sa_ts = 0b0
            case ModeD: sa_ts = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_index_1 := uint32(vidxr(v2))
        size := uint32(0b10)
        size |= sa_v
        if sa_v != sa_ts || sa_ts != maskp(sa_index_1, 2, 1) {
            panic("aarch64: invalid combination of operands for FMULX")
        }
        return p.setins(asisdelem(
            1,
            size,
            maskp(sa_index_1, 1, 1),
            maskp(sa_vm_1, 4, 1),
            mask(sa_vm_1, 4),
            9,
            sa_index_1 & 1,
            sa_n,
            sa_d,
        ))
    }
    // FMULX  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FMULX")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdsame(0, size, sa_m, 27, sa_n, sa_d))
    }
    // FMULX  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsamefp16(0, 0, sa_hm, 3, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMULX")
}

// FNEG instruction have 5 forms:
//
//   * FNEG  <Dd>, <Dn>
//   * FNEG  <Hd>, <Hn>
//   * FNEG  <Sd>, <Sn>
//   * FNEG  <Vd>.<T>, <Vn>.<T>
//   * FNEG  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FNEG(v0, v1 interface{}) *Instruction {
    p := self.alloc("FNEG", 2, Operands { v0, v1 })
    // FNEG  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 2, sa_dn, sa_dd))
    }
    // FNEG  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 2, sa_hn, sa_hd))
    }
    // FNEG  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 2, sa_sn, sa_sd))
    }
    // FNEG  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 15, sa_vn, sa_vd))
    }
    // FNEG  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 1, 15, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FNEG")
}

// FNMADD instruction have 3 forms:
//
//   * FNMADD  <Dd>, <Dn>, <Dm>, <Da>
//   * FNMADD  <Hd>, <Hn>, <Hm>, <Ha>
//   * FNMADD  <Sd>, <Sn>, <Sm>, <Sa>
//
func (self *Program) FNMADD(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FNMADD", 4, Operands { v0, v1, v2, v3 })
    // FNMADD  <Dd>, <Dn>, <Dm>, <Da>
    if isDr(v0) && isDr(v1) && isDr(v2) && isDr(v3) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        sa_da := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 1, 1, sa_dm, 0, sa_da, sa_dn, sa_dd))
    }
    // FNMADD  <Hd>, <Hn>, <Hm>, <Ha>
    if isHr(v0) && isHr(v1) && isHr(v2) && isHr(v3) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        sa_ha := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 3, 1, sa_hm, 0, sa_ha, sa_hn, sa_hd))
    }
    // FNMADD  <Sd>, <Sn>, <Sm>, <Sa>
    if isSr(v0) && isSr(v1) && isSr(v2) && isSr(v3) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        sa_sa := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 0, 1, sa_sm, 0, sa_sa, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FNMADD")
}

// FNMSUB instruction have 3 forms:
//
//   * FNMSUB  <Dd>, <Dn>, <Dm>, <Da>
//   * FNMSUB  <Hd>, <Hn>, <Hm>, <Ha>
//   * FNMSUB  <Sd>, <Sn>, <Sm>, <Sa>
//
func (self *Program) FNMSUB(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FNMSUB", 4, Operands { v0, v1, v2, v3 })
    // FNMSUB  <Dd>, <Dn>, <Dm>, <Da>
    if isDr(v0) && isDr(v1) && isDr(v2) && isDr(v3) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        sa_da := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 1, 1, sa_dm, 1, sa_da, sa_dn, sa_dd))
    }
    // FNMSUB  <Hd>, <Hn>, <Hm>, <Ha>
    if isHr(v0) && isHr(v1) && isHr(v2) && isHr(v3) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        sa_ha := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 3, 1, sa_hm, 1, sa_ha, sa_hn, sa_hd))
    }
    // FNMSUB  <Sd>, <Sn>, <Sm>, <Sa>
    if isSr(v0) && isSr(v1) && isSr(v2) && isSr(v3) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        sa_sa := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 0, 1, sa_sm, 1, sa_sa, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FNMSUB")
}

// FNMUL instruction have 3 forms:
//
//   * FNMUL  <Dd>, <Dn>, <Dm>
//   * FNMUL  <Hd>, <Hn>, <Hm>
//   * FNMUL  <Sd>, <Sn>, <Sm>
//
func (self *Program) FNMUL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FNMUL", 3, Operands { v0, v1, v2 })
    // FNMUL  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 8, sa_dn, sa_dd))
    }
    // FNMUL  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 8, sa_hn, sa_hd))
    }
    // FNMUL  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 8, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FNMUL")
}

// FRECPE instruction have 4 forms:
//
//   * FRECPE  <Vd>.<T>, <Vn>.<T>
//   * FRECPE  <Vd>.<T>, <Vn>.<T>
//   * FRECPE  <V><d>, <V><n>
//   * FRECPE  <Hd>, <Hn>
//
func (self *Program) FRECPE(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRECPE", 2, Operands { v0, v1 })
    // FRECPE  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 29, sa_vn, sa_vd))
    }
    // FRECPE  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 1, 29, sa_vn, sa_vd))
    }
    // FRECPE  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FRECPE")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 29, sa_n, sa_d))
    }
    // FRECPE  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 1, 29, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRECPE")
}

// FRECPS instruction have 4 forms:
//
//   * FRECPS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FRECPS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FRECPS  <V><d>, <V><n>, <V><m>
//   * FRECPS  <Hd>, <Hn>, <Hm>
//
func (self *Program) FRECPS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FRECPS", 3, Operands { v0, v1, v2 })
    // FRECPS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 0, size, sa_vm, 31, sa_vn, sa_vd))
    }
    // FRECPS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 0, 0, sa_vm, 7, sa_vn, sa_vd))
    }
    // FRECPS  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FRECPS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdsame(0, size, sa_m, 31, sa_n, sa_d))
    }
    // FRECPS  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsamefp16(0, 0, sa_hm, 7, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRECPS")
}

// FRECPX instruction have 2 forms:
//
//   * FRECPX  <V><d>, <V><n>
//   * FRECPX  <Hd>, <Hn>
//
func (self *Program) FRECPX(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRECPX", 2, Operands { v0, v1 })
    // FRECPX  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FRECPX")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 31, sa_n, sa_d))
    }
    // FRECPX  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 1, 31, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRECPX")
}

// FRINT32X instruction have 3 forms:
//
//   * FRINT32X  <Dd>, <Dn>
//   * FRINT32X  <Sd>, <Sn>
//   * FRINT32X  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FRINT32X(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINT32X", 2, Operands { v0, v1 })
    // FRINT32X  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 17, sa_dn, sa_dd))
    }
    // FRINT32X  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 17, sa_sn, sa_sd))
    }
    // FRINT32X  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 30, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINT32X")
}

// FRINT32Z instruction have 3 forms:
//
//   * FRINT32Z  <Dd>, <Dn>
//   * FRINT32Z  <Sd>, <Sn>
//   * FRINT32Z  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FRINT32Z(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINT32Z", 2, Operands { v0, v1 })
    // FRINT32Z  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 16, sa_dn, sa_dd))
    }
    // FRINT32Z  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 16, sa_sn, sa_sd))
    }
    // FRINT32Z  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 30, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINT32Z")
}

// FRINT64X instruction have 3 forms:
//
//   * FRINT64X  <Dd>, <Dn>
//   * FRINT64X  <Sd>, <Sn>
//   * FRINT64X  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FRINT64X(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINT64X", 2, Operands { v0, v1 })
    // FRINT64X  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 19, sa_dn, sa_dd))
    }
    // FRINT64X  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 19, sa_sn, sa_sd))
    }
    // FRINT64X  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 31, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINT64X")
}

// FRINT64Z instruction have 3 forms:
//
//   * FRINT64Z  <Dd>, <Dn>
//   * FRINT64Z  <Sd>, <Sn>
//   * FRINT64Z  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FRINT64Z(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINT64Z", 2, Operands { v0, v1 })
    // FRINT64Z  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 18, sa_dn, sa_dd))
    }
    // FRINT64Z  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 18, sa_sn, sa_sd))
    }
    // FRINT64Z  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 31, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINT64Z")
}

// FRINTA instruction have 5 forms:
//
//   * FRINTA  <Dd>, <Dn>
//   * FRINTA  <Hd>, <Hn>
//   * FRINTA  <Sd>, <Sn>
//   * FRINTA  <Vd>.<T>, <Vn>.<T>
//   * FRINTA  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FRINTA(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTA", 2, Operands { v0, v1 })
    // FRINTA  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 12, sa_dn, sa_dd))
    }
    // FRINTA  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 12, sa_hn, sa_hd))
    }
    // FRINTA  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 12, sa_sn, sa_sd))
    }
    // FRINTA  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 24, sa_vn, sa_vd))
    }
    // FRINTA  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 0, 24, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTA")
}

// FRINTI instruction have 5 forms:
//
//   * FRINTI  <Dd>, <Dn>
//   * FRINTI  <Hd>, <Hn>
//   * FRINTI  <Sd>, <Sn>
//   * FRINTI  <Vd>.<T>, <Vn>.<T>
//   * FRINTI  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FRINTI(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTI", 2, Operands { v0, v1 })
    // FRINTI  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 15, sa_dn, sa_dd))
    }
    // FRINTI  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 15, sa_hn, sa_hd))
    }
    // FRINTI  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 15, sa_sn, sa_sd))
    }
    // FRINTI  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 25, sa_vn, sa_vd))
    }
    // FRINTI  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 1, 25, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTI")
}

// FRINTM instruction have 5 forms:
//
//   * FRINTM  <Dd>, <Dn>
//   * FRINTM  <Hd>, <Hn>
//   * FRINTM  <Sd>, <Sn>
//   * FRINTM  <Vd>.<T>, <Vn>.<T>
//   * FRINTM  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FRINTM(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTM", 2, Operands { v0, v1 })
    // FRINTM  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 10, sa_dn, sa_dd))
    }
    // FRINTM  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 10, sa_hn, sa_hd))
    }
    // FRINTM  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 10, sa_sn, sa_sd))
    }
    // FRINTM  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 25, sa_vn, sa_vd))
    }
    // FRINTM  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 0, 25, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTM")
}

// FRINTN instruction have 5 forms:
//
//   * FRINTN  <Dd>, <Dn>
//   * FRINTN  <Hd>, <Hn>
//   * FRINTN  <Sd>, <Sn>
//   * FRINTN  <Vd>.<T>, <Vn>.<T>
//   * FRINTN  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FRINTN(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTN", 2, Operands { v0, v1 })
    // FRINTN  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 8, sa_dn, sa_dd))
    }
    // FRINTN  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 8, sa_hn, sa_hd))
    }
    // FRINTN  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 8, sa_sn, sa_sd))
    }
    // FRINTN  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 24, sa_vn, sa_vd))
    }
    // FRINTN  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 0, 24, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTN")
}

// FRINTP instruction have 5 forms:
//
//   * FRINTP  <Dd>, <Dn>
//   * FRINTP  <Hd>, <Hn>
//   * FRINTP  <Sd>, <Sn>
//   * FRINTP  <Vd>.<T>, <Vn>.<T>
//   * FRINTP  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FRINTP(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTP", 2, Operands { v0, v1 })
    // FRINTP  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 9, sa_dn, sa_dd))
    }
    // FRINTP  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 9, sa_hn, sa_hd))
    }
    // FRINTP  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 9, sa_sn, sa_sd))
    }
    // FRINTP  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 24, sa_vn, sa_vd))
    }
    // FRINTP  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 1, 24, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTP")
}

// FRINTX instruction have 5 forms:
//
//   * FRINTX  <Dd>, <Dn>
//   * FRINTX  <Hd>, <Hn>
//   * FRINTX  <Sd>, <Sn>
//   * FRINTX  <Vd>.<T>, <Vn>.<T>
//   * FRINTX  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FRINTX(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTX", 2, Operands { v0, v1 })
    // FRINTX  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 14, sa_dn, sa_dd))
    }
    // FRINTX  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 14, sa_hn, sa_hd))
    }
    // FRINTX  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 14, sa_sn, sa_sd))
    }
    // FRINTX  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 25, sa_vn, sa_vd))
    }
    // FRINTX  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 0, 25, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTX")
}

// FRINTZ instruction have 5 forms:
//
//   * FRINTZ  <Dd>, <Dn>
//   * FRINTZ  <Hd>, <Hn>
//   * FRINTZ  <Sd>, <Sn>
//   * FRINTZ  <Vd>.<T>, <Vn>.<T>
//   * FRINTZ  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FRINTZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTZ", 2, Operands { v0, v1 })
    // FRINTZ  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 11, sa_dn, sa_dd))
    }
    // FRINTZ  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 11, sa_hn, sa_hd))
    }
    // FRINTZ  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 11, sa_sn, sa_sd))
    }
    // FRINTZ  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 25, sa_vn, sa_vd))
    }
    // FRINTZ  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 1, 25, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTZ")
}

// FRSQRTE instruction have 4 forms:
//
//   * FRSQRTE  <Vd>.<T>, <Vn>.<T>
//   * FRSQRTE  <Vd>.<T>, <Vn>.<T>
//   * FRSQRTE  <V><d>, <V><n>
//   * FRSQRTE  <Hd>, <Hn>
//
func (self *Program) FRSQRTE(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRSQRTE", 2, Operands { v0, v1 })
    // FRSQRTE  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 29, sa_vn, sa_vd))
    }
    // FRSQRTE  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 1, 29, sa_vn, sa_vd))
    }
    // FRSQRTE  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FRSQRTE")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdmisc(1, size, 29, sa_n, sa_d))
    }
    // FRSQRTE  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(1, 1, 29, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRSQRTE")
}

// FRSQRTS instruction have 4 forms:
//
//   * FRSQRTS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FRSQRTS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FRSQRTS  <V><d>, <V><n>, <V><m>
//   * FRSQRTS  <Hd>, <Hn>, <Hm>
//
func (self *Program) FRSQRTS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FRSQRTS", 3, Operands { v0, v1, v2 })
    // FRSQRTS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 0, size, sa_vm, 31, sa_vn, sa_vd))
    }
    // FRSQRTS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 0, 1, sa_vm, 7, sa_vn, sa_vd))
    }
    // FRSQRTS  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for FRSQRTS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        size := uint32(0b10)
        size |= sa_v
        return p.setins(asisdsame(0, size, sa_m, 31, sa_n, sa_d))
    }
    // FRSQRTS  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsamefp16(0, 1, sa_hm, 7, sa_hn, sa_hd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRSQRTS")
}

// FSQRT instruction have 5 forms:
//
//   * FSQRT  <Dd>, <Dn>
//   * FSQRT  <Hd>, <Hn>
//   * FSQRT  <Sd>, <Sn>
//   * FSQRT  <Vd>.<T>, <Vn>.<T>
//   * FSQRT  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) FSQRT(v0, v1 interface{}) *Instruction {
    p := self.alloc("FSQRT", 2, Operands { v0, v1 })
    // FSQRT  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 3, sa_dn, sa_dd))
    }
    // FSQRT  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 3, sa_hn, sa_hd))
    }
    // FSQRT  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 3, sa_sn, sa_sd))
    }
    // FSQRT  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 31, sa_vn, sa_vd))
    }
    // FSQRT  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 1, 31, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FSQRT")
}

// FSUB instruction have 5 forms:
//
//   * FSUB  <Dd>, <Dn>, <Dm>
//   * FSUB  <Hd>, <Hn>, <Hm>
//   * FSUB  <Sd>, <Sn>, <Sm>
//   * FSUB  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * FSUB  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) FSUB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FSUB", 3, Operands { v0, v1, v2 })
    // FSUB  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 3, sa_dn, sa_dd))
    }
    // FSUB  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 3, sa_hn, sa_hd))
    }
    // FSUB  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 3, sa_sn, sa_sd))
    }
    // FSUB  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdsame(sa_t & 1, 0, size, sa_vm, 26, sa_vn, sa_vd))
    }
    // FSUB  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsamefp16(sa_t_1, 0, 1, sa_vm, 2, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FSUB")
}

// GCSB instruction have one single form:
//
//   * GCSB DSYNC
//
func (self *Program) GCSB(v0 interface{}) *Instruction {
    p := self.alloc("GCSB", 1, Operands { v0 })
    if v0 == DSYNC {
        return p.setins(hints(2, 3))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for GCSB")
}

// GCSSTR instruction have one single form:
//
//   * GCSSTR  <Xt>, [<Xn|SP>]
//
func (self *Program) GCSSTR(v0, v1 interface{}) *Instruction {
    p := self.alloc("GCSSTR", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldst_gcs(0, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for GCSSTR")
}

// GCSSTTR instruction have one single form:
//
//   * GCSSTTR  <Xt>, [<Xn|SP>]
//
func (self *Program) GCSSTTR(v0, v1 interface{}) *Instruction {
    p := self.alloc("GCSSTTR", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldst_gcs(1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for GCSSTTR")
}

// GMI instruction have one single form:
//
//   * GMI  <Xd>, <Xn|SP>, <Xm>
//
func (self *Program) GMI(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("GMI", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXrOrSP(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        Rm := uint32(0b00000)
        Rm |= sa_xm
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd
        return p.setins(dp_2src(1, 0, Rm, 5, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for GMI")
}

// HINT instruction have one single form:
//
//   * HINT  #<imm>
//
func (self *Program) HINT(v0 interface{}) *Instruction {
    p := self.alloc("HINT", 1, Operands { v0 })
    if isUimm7(v0) {
        sa_imm := asUimm7(v0)
        return p.setins(hints(maskp(sa_imm, 3, 4), mask(sa_imm, 3)))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for HINT")
}

// HLT instruction have one single form:
//
//   * HLT  #<imm>
//
func (self *Program) HLT(v0 interface{}) *Instruction {
    p := self.alloc("HLT", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(2, sa_imm, 0, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for HLT")
}

// HVC instruction have one single form:
//
//   * HVC  #<imm>
//
func (self *Program) HVC(v0 interface{}) *Instruction {
    p := self.alloc("HVC", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(0, sa_imm, 0, 2))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for HVC")
}

// INS instruction have 2 forms:
//
//   * INS  <Vd>.<Ts>[<index>], <R><n>
//   * INS  <Vd>.<Ts>[<index1>], <Vn>.<Ts>[<index2>]
//
func (self *Program) INS(v0, v1 interface{}) *Instruction {
    p := self.alloc("INS", 2, Operands { v0, v1 })
    // INS  <Vd>.<Ts>[<index>], <R><n>
    if isVri(v0) && isWrOrXr(v1) {
        var sa_r [3]uint32
        var sa_r__bit_mask [3]uint32
        var sa_ts uint32
        var sa_ts__bit_mask uint32
        sa_vd := uint32(v0.(VidxRegister).ID())
        switch vmoder(v0) {
            case ModeB: sa_ts = 0b00001
            case ModeH: sa_ts = 0b00010
            case ModeS: sa_ts = 0b00100
            case ModeD: sa_ts = 0b01000
            default: panic("aarch64: unreachable")
        }
        switch vmoder(v0) {
            case ModeB: sa_ts__bit_mask = 0b00001
            case ModeH: sa_ts__bit_mask = 0b00011
            case ModeS: sa_ts__bit_mask = 0b00111
            case ModeD: sa_ts__bit_mask = 0b01111
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v0))
        sa_n := uint32(v1.(asm.Register).ID())
        switch true {
            case isWr(v1): sa_r = [3]uint32{0b00100, 0b00010, 0b00001}
            case isXr(v1): sa_r = [3]uint32{0b01000}
            default: panic("aarch64: unreachable")
        }
        switch true {
            case isWr(v1): sa_r__bit_mask = [3]uint32{0b00111, 0b00011, 0b00001}
            case isXr(v1): sa_r__bit_mask = [3]uint32{0b01111}
            default: panic("aarch64: unreachable")
        }
        if sa_index != sa_ts & sa_ts__bit_mask || !matchany(sa_ts & sa_ts__bit_mask, &sa_r[0], &sa_r__bit_mask[0], 3) {
            panic("aarch64: invalid combination of operands for INS")
        }
        return p.setins(asimdins(1, 0, sa_index, 3, sa_n, sa_vd))
    }
    // INS  <Vd>.<Ts>[<index1>], <Vn>.<Ts>[<index2>]
    if isVri(v0) && isVri(v1) {
        var sa_ts uint32
        var sa_ts__bit_mask uint32
        sa_vd := uint32(v0.(VidxRegister).ID())
        switch vmoder(v0) {
            case ModeB: sa_ts = 0b00001
            case ModeH: sa_ts = 0b00010
            case ModeS: sa_ts = 0b00100
            case ModeD: sa_ts = 0b01000
            default: panic("aarch64: unreachable")
        }
        switch vmoder(v0) {
            case ModeB: sa_ts__bit_mask = 0b00001
            case ModeH: sa_ts__bit_mask = 0b00011
            case ModeS: sa_ts__bit_mask = 0b00111
            case ModeD: sa_ts__bit_mask = 0b01111
            default: panic("aarch64: unreachable")
        }
        sa_index1 := uint32(vidxr(v0))
        sa_vn := uint32(v1.(VidxRegister).ID())
        sa_index2 := uint32(vidxr(v1))
        if sa_index1 != maskp(sa_index2, 4, 5) || maskp(sa_index2, 4, 5) != sa_ts & sa_ts__bit_mask {
            panic("aarch64: invalid combination of operands for INS")
        }
        return p.setins(asimdins(1, 1, sa_index1, mask(sa_index2, 4), sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for INS")
}

// IRG instruction have one single form:
//
//   * IRG  <Xd|SP>, <Xn|SP>{, <Xm>}
//
func (self *Program) IRG(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("IRG", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("IRG", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction IRG takes 2 or 3 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && isXrOrSP(v0) && isXrOrSP(v1) && (len(vv) == 0 || isXr(vv[0])) {
        var sa_xm uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        if len(vv) == 1 {
            sa_xm = uint32(vv[0].(asm.Register).ID())
        }
        Rm := uint32(0b00000)
        Rm |= sa_xm
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd_sp
        return p.setins(dp_2src(1, 0, Rm, 4, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for IRG")
}

// ISB instruction have one single form:
//
//   * ISB  {<option>|#<imm>}
//
func (self *Program) ISB(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ISB", 0, Operands {})
        case 1  : p = self.alloc("ISB", 1, Operands { vv[0] })
        default : panic("instruction ISB takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isOption(vv[0])) {
        sa_option := SY
        if len(vv) == 1 {
            sa_option = vv[0].(BarrierOption)
        }
        sa_imm := uint32(sa_option)
        return p.setins(barriers(sa_imm, 6, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ISB")
}

// LD1 instruction have 24 forms:
//
//   * LD1  { <Vt>.<T> }, [<Xn|SP>]
//   * LD1  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>]
//   * LD1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>]
//   * LD1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>]
//   * LD1  { <Vt>.<T> }, [<Xn|SP>], <imm>
//   * LD1  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <imm>
//   * LD1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <imm>
//   * LD1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <imm>
//   * LD1  { <Vt>.<T> }, [<Xn|SP>], <Xm>
//   * LD1  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <Xm>
//   * LD1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <Xm>
//   * LD1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <Xm>
//   * LD1  { <Vt>.B }[<index>], [<Xn|SP>]
//   * LD1  { <Vt>.D }[<index>], [<Xn|SP>]
//   * LD1  { <Vt>.H }[<index>], [<Xn|SP>]
//   * LD1  { <Vt>.S }[<index>], [<Xn|SP>]
//   * LD1  { <Vt>.B }[<index>], [<Xn|SP>], #1
//   * LD1  { <Vt>.B }[<index>], [<Xn|SP>], <Xm>
//   * LD1  { <Vt>.D }[<index>], [<Xn|SP>], #8
//   * LD1  { <Vt>.D }[<index>], [<Xn|SP>], <Xm>
//   * LD1  { <Vt>.H }[<index>], [<Xn|SP>], #2
//   * LD1  { <Vt>.H }[<index>], [<Xn|SP>], <Xm>
//   * LD1  { <Vt>.S }[<index>], [<Xn|SP>], #4
//   * LD1  { <Vt>.S }[<index>], [<Xn|SP>], <Xm>
//
func (self *Program) LD1(v0, v1 interface{}) *Instruction {
    p := self.alloc("LD1", 2, Operands { v0, v1 })
    // LD1  { <Vt>.<T> }, [<Xn|SP>]
    if isVec1(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 1, 7, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>]
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 1, 10, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>]
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 1, 6, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>]
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 1, 2, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.<T> }, [<Xn|SP>], <imm>
    if isVec1(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm := uint32(moffs(v1))
        if sa_imm != sa_t & 1 {
            panic("aarch64: invalid combination of operands for LD1")
        }
        return p.setins(asisdlsep(sa_imm, 1, 31, 7, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <imm>
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm_1 := uint32(moffs(v1))
        if sa_imm_1 != sa_t & 1 {
            panic("aarch64: invalid combination of operands for LD1")
        }
        return p.setins(asisdlsep(sa_imm_1, 1, 31, 10, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <imm>
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm_2 := uint32(moffs(v1))
        if sa_imm_2 != sa_t & 1 {
            panic("aarch64: invalid combination of operands for LD1")
        }
        return p.setins(asisdlsep(sa_imm_2, 1, 31, 6, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <imm>
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm_3 := uint32(moffs(v1))
        if sa_imm_3 != sa_t & 1 {
            panic("aarch64: invalid combination of operands for LD1")
        }
        return p.setins(asisdlsep(sa_imm_3, 1, 31, 2, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.<T> }, [<Xn|SP>], <Xm>
    if isVec1(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 1, sa_xm, 7, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <Xm>
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 1, sa_xm, 10, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <Xm>
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 1, sa_xm, 6, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <Xm>
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 1, sa_xm, 2, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.B }[<index>], [<Xn|SP>]
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index, 3, 1), 1, 0, 0, 0, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.D }[<index>], [<Xn|SP>]
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_index_1, 1, 0, 0, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.H }[<index>], [<Xn|SP>]
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(
            maskp(sa_index_2, 3, 1),
            1,
            0,
            0,
            2,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // LD1  { <Vt>.S }[<index>], [<Xn|SP>]
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index_3, 1, 1), 1, 0, 0, 4, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.B }[<index>], [<Xn|SP>], #1
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 1 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 1, 0, 31, 0, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.B }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 1, 0, sa_xm, 0, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.D }[<index>], [<Xn|SP>], #8
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 8 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 1, 0, 31, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.D }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 1, 0, sa_xm, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.H }[<index>], [<Xn|SP>], #2
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 2 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            1,
            0,
            31,
            2,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // LD1  { <Vt>.H }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            1,
            0,
            sa_xm,
            2,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // LD1  { <Vt>.S }[<index>], [<Xn|SP>], #4
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 4 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 1, 0, 31, 4, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // LD1  { <Vt>.S }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 1, 0, sa_xm, 4, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LD1")
}

// LD1R instruction have 3 forms:
//
//   * LD1R  { <Vt>.<T> }, [<Xn|SP>]
//   * LD1R  { <Vt>.<T> }, [<Xn|SP>], <imm>
//   * LD1R  { <Vt>.<T> }, [<Xn|SP>], <Xm>
//
func (self *Program) LD1R(v0, v1 interface{}) *Instruction {
    p := self.alloc("LD1R", 2, Operands { v0, v1 })
    // LD1R  { <Vt>.<T> }, [<Xn|SP>]
    if isVec1(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1R")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_t & 1, 1, 0, 0, 6, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD1R  { <Vt>.<T> }, [<Xn|SP>], <imm>
    if isVec1(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1R")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm := uint32(moffs(v1))
        if sa_imm != maskp(sa_t, 1, 2) {
            panic("aarch64: invalid combination of operands for LD1R")
        }
        return p.setins(asisdlsop(sa_t & 1, 1, 0, 31, 6, 0, sa_imm, sa_xn_sp, sa_vt))
    }
    // LD1R  { <Vt>.<T> }, [<Xn|SP>], <Xm>
    if isVec1(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD1R")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(sa_t & 1, 1, 0, sa_xm, 6, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LD1R")
}

// LD2 instruction have 15 forms:
//
//   * LD2  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>]
//   * LD2  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <imm>
//   * LD2  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <Xm>
//   * LD2  { <Vt>.B, <Vt2>.B }[<index>], [<Xn|SP>]
//   * LD2  { <Vt>.D, <Vt2>.D }[<index>], [<Xn|SP>]
//   * LD2  { <Vt>.H, <Vt2>.H }[<index>], [<Xn|SP>]
//   * LD2  { <Vt>.S, <Vt2>.S }[<index>], [<Xn|SP>]
//   * LD2  { <Vt>.B, <Vt2>.B }[<index>], [<Xn|SP>], #2
//   * LD2  { <Vt>.B, <Vt2>.B }[<index>], [<Xn|SP>], <Xm>
//   * LD2  { <Vt>.D, <Vt2>.D }[<index>], [<Xn|SP>], #16
//   * LD2  { <Vt>.D, <Vt2>.D }[<index>], [<Xn|SP>], <Xm>
//   * LD2  { <Vt>.H, <Vt2>.H }[<index>], [<Xn|SP>], #4
//   * LD2  { <Vt>.H, <Vt2>.H }[<index>], [<Xn|SP>], <Xm>
//   * LD2  { <Vt>.S, <Vt2>.S }[<index>], [<Xn|SP>], #8
//   * LD2  { <Vt>.S, <Vt2>.S }[<index>], [<Xn|SP>], <Xm>
//
func (self *Program) LD2(v0, v1 interface{}) *Instruction {
    p := self.alloc("LD2", 2, Operands { v0, v1 })
    // LD2  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>]
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD2")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 1, 8, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD2  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <imm>
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD2")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm := uint32(moffs(v1))
        if sa_imm != sa_t & 1 {
            panic("aarch64: invalid combination of operands for LD2")
        }
        return p.setins(asisdlsep(sa_imm, 1, 31, 8, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD2  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <Xm>
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD2")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 1, sa_xm, 8, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD2  { <Vt>.B, <Vt2>.B }[<index>], [<Xn|SP>]
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index, 3, 1), 1, 1, 0, 0, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // LD2  { <Vt>.D, <Vt2>.D }[<index>], [<Xn|SP>]
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_index_1, 1, 1, 0, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    // LD2  { <Vt>.H, <Vt2>.H }[<index>], [<Xn|SP>]
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(
            maskp(sa_index_2, 3, 1),
            1,
            1,
            0,
            2,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // LD2  { <Vt>.S, <Vt2>.S }[<index>], [<Xn|SP>]
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index_3, 1, 1), 1, 1, 0, 4, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // LD2  { <Vt>.B, <Vt2>.B }[<index>], [<Xn|SP>], #2
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 2 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 1, 1, 31, 0, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // LD2  { <Vt>.B, <Vt2>.B }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 1, 1, sa_xm, 0, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // LD2  { <Vt>.D, <Vt2>.D }[<index>], [<Xn|SP>], #16
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 16 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 1, 1, 31, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    // LD2  { <Vt>.D, <Vt2>.D }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 1, 1, sa_xm, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    // LD2  { <Vt>.H, <Vt2>.H }[<index>], [<Xn|SP>], #4
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 4 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            1,
            1,
            31,
            2,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // LD2  { <Vt>.H, <Vt2>.H }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            1,
            1,
            sa_xm,
            2,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // LD2  { <Vt>.S, <Vt2>.S }[<index>], [<Xn|SP>], #8
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 8 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 1, 1, 31, 4, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // LD2  { <Vt>.S, <Vt2>.S }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 1, 1, sa_xm, 4, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LD2")
}

// LD2R instruction have 3 forms:
//
//   * LD2R  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>]
//   * LD2R  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <imm>
//   * LD2R  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <Xm>
//
func (self *Program) LD2R(v0, v1 interface{}) *Instruction {
    p := self.alloc("LD2R", 2, Operands { v0, v1 })
    // LD2R  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>]
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD2R")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_t & 1, 1, 1, 0, 6, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD2R  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <imm>
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD2R")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm := uint32(moffs(v1))
        if sa_imm != maskp(sa_t, 1, 2) {
            panic("aarch64: invalid combination of operands for LD2R")
        }
        return p.setins(asisdlsop(sa_t & 1, 1, 1, 31, 6, 0, sa_imm, sa_xn_sp, sa_vt))
    }
    // LD2R  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <Xm>
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD2R")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(sa_t & 1, 1, 1, sa_xm, 6, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LD2R")
}

// LD3 instruction have 15 forms:
//
//   * LD3  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>]
//   * LD3  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <imm>
//   * LD3  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <Xm>
//   * LD3  { <Vt>.B, <Vt2>.B, <Vt3>.B }[<index>], [<Xn|SP>]
//   * LD3  { <Vt>.D, <Vt2>.D, <Vt3>.D }[<index>], [<Xn|SP>]
//   * LD3  { <Vt>.H, <Vt2>.H, <Vt3>.H }[<index>], [<Xn|SP>]
//   * LD3  { <Vt>.S, <Vt2>.S, <Vt3>.S }[<index>], [<Xn|SP>]
//   * LD3  { <Vt>.B, <Vt2>.B, <Vt3>.B }[<index>], [<Xn|SP>], #3
//   * LD3  { <Vt>.B, <Vt2>.B, <Vt3>.B }[<index>], [<Xn|SP>], <Xm>
//   * LD3  { <Vt>.D, <Vt2>.D, <Vt3>.D }[<index>], [<Xn|SP>], #24
//   * LD3  { <Vt>.D, <Vt2>.D, <Vt3>.D }[<index>], [<Xn|SP>], <Xm>
//   * LD3  { <Vt>.H, <Vt2>.H, <Vt3>.H }[<index>], [<Xn|SP>], #6
//   * LD3  { <Vt>.H, <Vt2>.H, <Vt3>.H }[<index>], [<Xn|SP>], <Xm>
//   * LD3  { <Vt>.S, <Vt2>.S, <Vt3>.S }[<index>], [<Xn|SP>], #12
//   * LD3  { <Vt>.S, <Vt2>.S, <Vt3>.S }[<index>], [<Xn|SP>], <Xm>
//
func (self *Program) LD3(v0, v1 interface{}) *Instruction {
    p := self.alloc("LD3", 2, Operands { v0, v1 })
    // LD3  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>]
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD3")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 1, 4, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD3  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <imm>
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD3")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm := uint32(moffs(v1))
        if sa_imm != sa_t & 1 {
            panic("aarch64: invalid combination of operands for LD3")
        }
        return p.setins(asisdlsep(sa_imm, 1, 31, 4, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD3  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <Xm>
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD3")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 1, sa_xm, 4, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD3  { <Vt>.B, <Vt2>.B, <Vt3>.B }[<index>], [<Xn|SP>]
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index, 3, 1), 1, 0, 0, 1, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // LD3  { <Vt>.D, <Vt2>.D, <Vt3>.D }[<index>], [<Xn|SP>]
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_index_1, 1, 0, 0, 5, 0, 1, sa_xn_sp, sa_vt))
    }
    // LD3  { <Vt>.H, <Vt2>.H, <Vt3>.H }[<index>], [<Xn|SP>]
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(
            maskp(sa_index_2, 3, 1),
            1,
            0,
            0,
            3,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // LD3  { <Vt>.S, <Vt2>.S, <Vt3>.S }[<index>], [<Xn|SP>]
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index_3, 1, 1), 1, 0, 0, 5, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // LD3  { <Vt>.B, <Vt2>.B, <Vt3>.B }[<index>], [<Xn|SP>], #3
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 3 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 1, 0, 31, 1, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // LD3  { <Vt>.B, <Vt2>.B, <Vt3>.B }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 1, 0, sa_xm, 1, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // LD3  { <Vt>.D, <Vt2>.D, <Vt3>.D }[<index>], [<Xn|SP>], #24
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 24 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 1, 0, 31, 5, 0, 1, sa_xn_sp, sa_vt))
    }
    // LD3  { <Vt>.D, <Vt2>.D, <Vt3>.D }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 1, 0, sa_xm, 5, 0, 1, sa_xn_sp, sa_vt))
    }
    // LD3  { <Vt>.H, <Vt2>.H, <Vt3>.H }[<index>], [<Xn|SP>], #6
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 6 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            1,
            0,
            31,
            3,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // LD3  { <Vt>.H, <Vt2>.H, <Vt3>.H }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            1,
            0,
            sa_xm,
            3,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // LD3  { <Vt>.S, <Vt2>.S, <Vt3>.S }[<index>], [<Xn|SP>], #12
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 12 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 1, 0, 31, 5, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // LD3  { <Vt>.S, <Vt2>.S, <Vt3>.S }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 1, 0, sa_xm, 5, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LD3")
}

// LD3R instruction have 3 forms:
//
//   * LD3R  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>]
//   * LD3R  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <imm>
//   * LD3R  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <Xm>
//
func (self *Program) LD3R(v0, v1 interface{}) *Instruction {
    p := self.alloc("LD3R", 2, Operands { v0, v1 })
    // LD3R  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>]
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD3R")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_t & 1, 1, 0, 0, 7, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD3R  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <imm>
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD3R")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm := uint32(moffs(v1))
        if sa_imm != maskp(sa_t, 1, 2) {
            panic("aarch64: invalid combination of operands for LD3R")
        }
        return p.setins(asisdlsop(sa_t & 1, 1, 0, 31, 7, 0, sa_imm, sa_xn_sp, sa_vt))
    }
    // LD3R  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <Xm>
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD3R")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(sa_t & 1, 1, 0, sa_xm, 7, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LD3R")
}

// LD4 instruction have 15 forms:
//
//   * LD4  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>]
//   * LD4  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <imm>
//   * LD4  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <Xm>
//   * LD4  { <Vt>.B, <Vt2>.B, <Vt3>.B, <Vt4>.B }[<index>], [<Xn|SP>]
//   * LD4  { <Vt>.D, <Vt2>.D, <Vt3>.D, <Vt4>.D }[<index>], [<Xn|SP>]
//   * LD4  { <Vt>.H, <Vt2>.H, <Vt3>.H, <Vt4>.H }[<index>], [<Xn|SP>]
//   * LD4  { <Vt>.S, <Vt2>.S, <Vt3>.S, <Vt4>.S }[<index>], [<Xn|SP>]
//   * LD4  { <Vt>.B, <Vt2>.B, <Vt3>.B, <Vt4>.B }[<index>], [<Xn|SP>], #4
//   * LD4  { <Vt>.B, <Vt2>.B, <Vt3>.B, <Vt4>.B }[<index>], [<Xn|SP>], <Xm>
//   * LD4  { <Vt>.D, <Vt2>.D, <Vt3>.D, <Vt4>.D }[<index>], [<Xn|SP>], #32
//   * LD4  { <Vt>.D, <Vt2>.D, <Vt3>.D, <Vt4>.D }[<index>], [<Xn|SP>], <Xm>
//   * LD4  { <Vt>.H, <Vt2>.H, <Vt3>.H, <Vt4>.H }[<index>], [<Xn|SP>], #8
//   * LD4  { <Vt>.H, <Vt2>.H, <Vt3>.H, <Vt4>.H }[<index>], [<Xn|SP>], <Xm>
//   * LD4  { <Vt>.S, <Vt2>.S, <Vt3>.S, <Vt4>.S }[<index>], [<Xn|SP>], #16
//   * LD4  { <Vt>.S, <Vt2>.S, <Vt3>.S, <Vt4>.S }[<index>], [<Xn|SP>], <Xm>
//
func (self *Program) LD4(v0, v1 interface{}) *Instruction {
    p := self.alloc("LD4", 2, Operands { v0, v1 })
    // LD4  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>]
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD4")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 1, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD4  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <imm>
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD4")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm := uint32(moffs(v1))
        if sa_imm != sa_t & 1 {
            panic("aarch64: invalid combination of operands for LD4")
        }
        return p.setins(asisdlsep(sa_imm, 1, 31, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD4  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <Xm>
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD4")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 1, sa_xm, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD4  { <Vt>.B, <Vt2>.B, <Vt3>.B, <Vt4>.B }[<index>], [<Xn|SP>]
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index, 3, 1), 1, 1, 0, 1, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // LD4  { <Vt>.D, <Vt2>.D, <Vt3>.D, <Vt4>.D }[<index>], [<Xn|SP>]
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_index_1, 1, 1, 0, 5, 0, 1, sa_xn_sp, sa_vt))
    }
    // LD4  { <Vt>.H, <Vt2>.H, <Vt3>.H, <Vt4>.H }[<index>], [<Xn|SP>]
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(
            maskp(sa_index_2, 3, 1),
            1,
            1,
            0,
            3,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // LD4  { <Vt>.S, <Vt2>.S, <Vt3>.S, <Vt4>.S }[<index>], [<Xn|SP>]
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index_3, 1, 1), 1, 1, 0, 5, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // LD4  { <Vt>.B, <Vt2>.B, <Vt3>.B, <Vt4>.B }[<index>], [<Xn|SP>], #4
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 4 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 1, 1, 31, 1, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // LD4  { <Vt>.B, <Vt2>.B, <Vt3>.B, <Vt4>.B }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 1, 1, sa_xm, 1, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // LD4  { <Vt>.D, <Vt2>.D, <Vt3>.D, <Vt4>.D }[<index>], [<Xn|SP>], #32
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 32 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 1, 1, 31, 5, 0, 1, sa_xn_sp, sa_vt))
    }
    // LD4  { <Vt>.D, <Vt2>.D, <Vt3>.D, <Vt4>.D }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 1, 1, sa_xm, 5, 0, 1, sa_xn_sp, sa_vt))
    }
    // LD4  { <Vt>.H, <Vt2>.H, <Vt3>.H, <Vt4>.H }[<index>], [<Xn|SP>], #8
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 8 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            1,
            1,
            31,
            3,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // LD4  { <Vt>.H, <Vt2>.H, <Vt3>.H, <Vt4>.H }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            1,
            1,
            sa_xm,
            3,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // LD4  { <Vt>.S, <Vt2>.S, <Vt3>.S, <Vt4>.S }[<index>], [<Xn|SP>], #16
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 16 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 1, 1, 31, 5, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // LD4  { <Vt>.S, <Vt2>.S, <Vt3>.S, <Vt4>.S }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 1, 1, sa_xm, 5, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LD4")
}

// LD4R instruction have 3 forms:
//
//   * LD4R  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>]
//   * LD4R  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <imm>
//   * LD4R  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <Xm>
//
func (self *Program) LD4R(v0, v1 interface{}) *Instruction {
    p := self.alloc("LD4R", 2, Operands { v0, v1 })
    // LD4R  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>]
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD4R")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_t & 1, 1, 1, 0, 7, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // LD4R  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <imm>
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD4R")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm := uint32(moffs(v1))
        if sa_imm != maskp(sa_t, 1, 2) {
            panic("aarch64: invalid combination of operands for LD4R")
        }
        return p.setins(asisdlsop(sa_t & 1, 1, 1, 31, 7, 0, sa_imm, sa_xn_sp, sa_vt))
    }
    // LD4R  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <Xm>
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for LD4R")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(sa_t & 1, 1, 1, sa_xm, 7, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LD4R")
}

// LD64B instruction have one single form:
//
//   * LD64B  <Xt>, [<Xn|SP> {,#0}]
//
func (self *Program) LD64B(v0, v1 interface{}) *Instruction {
    p := self.alloc("LD64B", 2, Operands { v0, v1 })
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(memop(3, 0, 0, 0, 31, 1, 5, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LD64B")
}

// LDADD instruction have 2 forms:
//
//   * LDADD  <Ws>, <Wt>, [<Xn|SP>]
//   * LDADD  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDADD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADD", 3, Operands { v0, v1, v2 })
    // LDADD  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    // LDADD  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDADD")
}

// LDADDA instruction have 2 forms:
//
//   * LDADDA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDADDA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDADDA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDA", 3, Operands { v0, v1, v2 })
    // LDADDA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    // LDADDA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDA")
}

// LDADDAB instruction have one single form:
//
//   * LDADDAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDAB")
}

// LDADDAH instruction have one single form:
//
//   * LDADDAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDAH")
}

// LDADDAL instruction have 2 forms:
//
//   * LDADDAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDADDAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDADDAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDAL", 3, Operands { v0, v1, v2 })
    // LDADDAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    // LDADDAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDAL")
}

// LDADDALB instruction have one single form:
//
//   * LDADDALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDALB")
}

// LDADDALH instruction have one single form:
//
//   * LDADDALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDALH")
}

// LDADDB instruction have one single form:
//
//   * LDADDB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDB")
}

// LDADDH instruction have one single form:
//
//   * LDADDH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDH")
}

// LDADDL instruction have 2 forms:
//
//   * LDADDL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDADDL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDADDL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDL", 3, Operands { v0, v1, v2 })
    // LDADDL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    // LDADDL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDL")
}

// LDADDLB instruction have one single form:
//
//   * LDADDLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDLB")
}

// LDADDLH instruction have one single form:
//
//   * LDADDLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDLH")
}

// LDAP1 instruction have one single form:
//
//   * LDAP1  { <Vt>.D }[<index>], [<Xn|SP>]
//
func (self *Program) LDAP1(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAP1", 2, Operands { v0, v1 })
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_index, 1, 0, 1, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAP1")
}

// LDAPR instruction have 4 forms:
//
//   * LDAPR  <Wt>, [<Xn|SP>], #4
//   * LDAPR  <Wt>, [<Xn|SP> {,#0}]
//   * LDAPR  <Xt>, [<Xn|SP>], #8
//   * LDAPR  <Xt>, [<Xn|SP> {,#0}]
//
func (self *Program) LDAPR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPR", 2, Operands { v0, v1 })
    // LDAPR  <Wt>, [<Xn|SP>], #4
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 4 && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldapstl_writeback(2, 1, sa_xn_sp, sa_wt))
    }
    // LDAPR  <Wt>, [<Xn|SP> {,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(memop(2, 0, 1, 0, 31, 1, 4, sa_xn_sp, sa_wt))
    }
    // LDAPR  <Xt>, [<Xn|SP>], #8
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 8 && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldapstl_writeback(3, 1, sa_xn_sp, sa_xt))
    }
    // LDAPR  <Xt>, [<Xn|SP> {,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(memop(3, 0, 1, 0, 31, 1, 4, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPR")
}

// LDAPRB instruction have one single form:
//
//   * LDAPRB  <Wt>, [<Xn|SP> {,#0}]
//
func (self *Program) LDAPRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPRB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(memop(0, 0, 1, 0, 31, 1, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPRB")
}

// LDAPRH instruction have one single form:
//
//   * LDAPRH  <Wt>, [<Xn|SP> {,#0}]
//
func (self *Program) LDAPRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPRH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(memop(1, 0, 1, 0, 31, 1, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPRH")
}

// LDAPUR instruction have 7 forms:
//
//   * LDAPUR  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDAPUR  <Xt>, [<Xn|SP>{, #<simm>}]
//   * LDAPUR  <Bt>, [<Xn|SP>{, #<simm>}]
//   * LDAPUR  <Dt>, [<Xn|SP>{, #<simm>}]
//   * LDAPUR  <Ht>, [<Xn|SP>{, #<simm>}]
//   * LDAPUR  <Qt>, [<Xn|SP>{, #<simm>}]
//   * LDAPUR  <St>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDAPUR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPUR", 2, Operands { v0, v1 })
    // LDAPUR  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(2, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDAPUR  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(3, 1, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDAPUR  <Bt>, [<Xn|SP>{, #<simm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(0, 1, sa_simm, sa_xn_sp, sa_bt))
    }
    // LDAPUR  <Dt>, [<Xn|SP>{, #<simm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(3, 1, sa_simm, sa_xn_sp, sa_dt))
    }
    // LDAPUR  <Ht>, [<Xn|SP>{, #<simm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(1, 1, sa_simm, sa_xn_sp, sa_ht))
    }
    // LDAPUR  <Qt>, [<Xn|SP>{, #<simm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(0, 3, sa_simm, sa_xn_sp, sa_qt))
    }
    // LDAPUR  <St>, [<Xn|SP>{, #<simm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(2, 1, sa_simm, sa_xn_sp, sa_st))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPUR")
}

// LDAPURB instruction have one single form:
//
//   * LDAPURB  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDAPURB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPURB", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPURB")
}

// LDAPURH instruction have one single form:
//
//   * LDAPURH  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDAPURH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPURH", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(1, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPURH")
}

// LDAPURSB instruction have 2 forms:
//
//   * LDAPURSB  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDAPURSB  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDAPURSB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPURSB", 2, Operands { v0, v1 })
    // LDAPURSB  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDAPURSB  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPURSB")
}

// LDAPURSH instruction have 2 forms:
//
//   * LDAPURSH  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDAPURSH  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDAPURSH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPURSH", 2, Operands { v0, v1 })
    // LDAPURSH  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(1, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDAPURSH  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(1, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPURSH")
}

// LDAPURSW instruction have one single form:
//
//   * LDAPURSW  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDAPURSW(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPURSW", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(2, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPURSW")
}

// LDAR instruction have 2 forms:
//
//   * LDAR  <Wt>, [<Xn|SP>{,#0}]
//   * LDAR  <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDAR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAR", 2, Operands { v0, v1 })
    // LDAR  <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(2, 1, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    // LDAR  <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(3, 1, 31, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAR")
}

// LDARB instruction have one single form:
//
//   * LDARB  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDARB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDARB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(0, 1, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDARB")
}

// LDARH instruction have one single form:
//
//   * LDARH  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDARH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDARH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(1, 1, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDARH")
}

// LDAXP instruction have 2 forms:
//
//   * LDAXP  <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
//   * LDAXP  <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
//
func (self *Program) LDAXP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDAXP", 3, Operands { v0, v1, v2 })
    // LDAXP  <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclp(0, 1, 31, 1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDAXP  <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclp(1, 1, 31, 1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAXP")
}

// LDAXR instruction have 2 forms:
//
//   * LDAXR  <Wt>, [<Xn|SP>{,#0}]
//   * LDAXR  <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDAXR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAXR", 2, Operands { v0, v1 })
    // LDAXR  <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(2, 1, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    // LDAXR  <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(3, 1, 31, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAXR")
}

// LDAXRB instruction have one single form:
//
//   * LDAXRB  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDAXRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAXRB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(0, 1, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAXRB")
}

// LDAXRH instruction have one single form:
//
//   * LDAXRH  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDAXRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAXRH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(1, 1, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAXRH")
}

// LDCLR instruction have 2 forms:
//
//   * LDCLR  <Ws>, <Wt>, [<Xn|SP>]
//   * LDCLR  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDCLR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLR", 3, Operands { v0, v1, v2 })
    // LDCLR  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    // LDCLR  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 1, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLR")
}

// LDCLRA instruction have 2 forms:
//
//   * LDCLRA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDCLRA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDCLRA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRA", 3, Operands { v0, v1, v2 })
    // LDCLRA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    // LDCLRA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 1, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRA")
}

// LDCLRAB instruction have one single form:
//
//   * LDCLRAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRAB")
}

// LDCLRAH instruction have one single form:
//
//   * LDCLRAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRAH")
}

// LDCLRAL instruction have 2 forms:
//
//   * LDCLRAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDCLRAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDCLRAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRAL", 3, Operands { v0, v1, v2 })
    // LDCLRAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    // LDCLRAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 1, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRAL")
}

// LDCLRALB instruction have one single form:
//
//   * LDCLRALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRALB")
}

// LDCLRALH instruction have one single form:
//
//   * LDCLRALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRALH")
}

// LDCLRB instruction have one single form:
//
//   * LDCLRB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRB")
}

// LDCLRH instruction have one single form:
//
//   * LDCLRH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRH")
}

// LDCLRL instruction have 2 forms:
//
//   * LDCLRL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDCLRL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDCLRL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRL", 3, Operands { v0, v1, v2 })
    // LDCLRL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    // LDCLRL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 1, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRL")
}

// LDCLRLB instruction have one single form:
//
//   * LDCLRLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRLB")
}

// LDCLRLH instruction have one single form:
//
//   * LDCLRLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRLH")
}

// LDCLRP instruction have one single form:
//
//   * LDCLRP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDCLRP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 0, sa_xt2, 0, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRP")
}

// LDCLRPA instruction have one single form:
//
//   * LDCLRPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDCLRPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 0, sa_xt2, 0, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRPA")
}

// LDCLRPAL instruction have one single form:
//
//   * LDCLRPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDCLRPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 1, sa_xt2, 0, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRPAL")
}

// LDCLRPL instruction have one single form:
//
//   * LDCLRPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDCLRPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 1, sa_xt2, 0, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRPL")
}

// LDEOR instruction have 2 forms:
//
//   * LDEOR  <Ws>, <Wt>, [<Xn|SP>]
//   * LDEOR  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDEOR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEOR", 3, Operands { v0, v1, v2 })
    // LDEOR  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    // LDEOR  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 2, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDEOR")
}

// LDEORA instruction have 2 forms:
//
//   * LDEORA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDEORA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDEORA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORA", 3, Operands { v0, v1, v2 })
    // LDEORA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    // LDEORA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 2, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORA")
}

// LDEORAB instruction have one single form:
//
//   * LDEORAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORAB")
}

// LDEORAH instruction have one single form:
//
//   * LDEORAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORAH")
}

// LDEORAL instruction have 2 forms:
//
//   * LDEORAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDEORAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDEORAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORAL", 3, Operands { v0, v1, v2 })
    // LDEORAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    // LDEORAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 2, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORAL")
}

// LDEORALB instruction have one single form:
//
//   * LDEORALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORALB")
}

// LDEORALH instruction have one single form:
//
//   * LDEORALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORALH")
}

// LDEORB instruction have one single form:
//
//   * LDEORB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORB")
}

// LDEORH instruction have one single form:
//
//   * LDEORH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORH")
}

// LDEORL instruction have 2 forms:
//
//   * LDEORL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDEORL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDEORL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORL", 3, Operands { v0, v1, v2 })
    // LDEORL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    // LDEORL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 2, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORL")
}

// LDEORLB instruction have one single form:
//
//   * LDEORLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORLB")
}

// LDEORLH instruction have one single form:
//
//   * LDEORLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORLH")
}

// LDG instruction have one single form:
//
//   * LDG  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDG(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDG", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt
        return p.setins(ldsttags(1, sa_simm, 0, Rn, Rt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDG")
}

// LDGM instruction have one single form:
//
//   * LDGM  <Xt>, [<Xn|SP>]
//
func (self *Program) LDGM(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDGM", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt
        return p.setins(ldsttags(3, 0, 0, Rn, Rt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDGM")
}

// LDIAPP instruction have 4 forms:
//
//   * LDIAPP  <Wt1>, <Wt2>, [<Xn|SP>], #8
//   * LDIAPP  <Wt1>, <Wt2>, [<Xn|SP>]
//   * LDIAPP  <Xt1>, <Xt2>, [<Xn|SP>], #16
//   * LDIAPP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDIAPP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDIAPP", 3, Operands { v0, v1, v2 })
    // LDIAPP  <Wt1>, <Wt2>, [<Xn|SP>], #8
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 8 &&
       mext(v2) == PostIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(2, 1, sa_wt2, 0, sa_xn_sp, sa_wt1))
    }
    // LDIAPP  <Wt1>, <Wt2>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(2, 1, sa_wt2, 1, sa_xn_sp, sa_wt1))
    }
    // LDIAPP  <Xt1>, <Xt2>, [<Xn|SP>], #16
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 16 &&
       mext(v2) == PostIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(3, 1, sa_xt2, 0, sa_xn_sp, sa_xt1))
    }
    // LDIAPP  <Xt1>, <Xt2>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(3, 1, sa_xt2, 1, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDIAPP")
}

// LDLAR instruction have 2 forms:
//
//   * LDLAR  <Wt>, [<Xn|SP>{,#0}]
//   * LDLAR  <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDLAR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDLAR", 2, Operands { v0, v1 })
    // LDLAR  <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(2, 1, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    // LDLAR  <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(3, 1, 31, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDLAR")
}

// LDLARB instruction have one single form:
//
//   * LDLARB  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDLARB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDLARB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(0, 1, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDLARB")
}

// LDLARH instruction have one single form:
//
//   * LDLARH  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDLARH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDLARH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(1, 1, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDLARH")
}

// LDNP instruction have 5 forms:
//
//   * LDNP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
//   * LDNP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * LDNP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
//   * LDNP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
//   * LDNP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
//
func (self *Program) LDNP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDNP", 3, Operands { v0, v1, v2 })
    // LDNP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(0, 0, 1, sa_imm, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDNP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(2, 0, 1, sa_imm_1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDNP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(1, 1, 1, sa_imm, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // LDNP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(2, 1, 1, sa_imm_1, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // LDNP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(0, 1, 1, sa_imm_2, sa_st2, sa_xn_sp, sa_st1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDNP")
}

// LDP instruction have 15 forms:
//
//   * LDP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <Wt1>, <Wt2>, [<Xn|SP>], #<imm>
//   * LDP  <Wt1>, <Wt2>, [<Xn|SP>, #<imm>]!
//   * LDP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
//   * LDP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
//   * LDP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <Dt1>, <Dt2>, [<Xn|SP>], #<imm>
//   * LDP  <Dt1>, <Dt2>, [<Xn|SP>, #<imm>]!
//   * LDP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <Qt1>, <Qt2>, [<Xn|SP>], #<imm>
//   * LDP  <Qt1>, <Qt2>, [<Xn|SP>, #<imm>]!
//   * LDP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <St1>, <St2>, [<Xn|SP>], #<imm>
//   * LDP  <St1>, <St2>, [<Xn|SP>, #<imm>]!
//
func (self *Program) LDP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDP", 3, Operands { v0, v1, v2 })
    // LDP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(0, 0, 1, sa_imm, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDP  <Wt1>, <Wt2>, [<Xn|SP>], #<imm>
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(0, 0, 1, sa_imm_1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDP  <Wt1>, <Wt2>, [<Xn|SP>, #<imm>]!
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(0, 0, 1, sa_imm_1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstpair_off(2, 0, 1, sa_imm_2, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_post(2, 0, 1, sa_imm_3, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(2, 0, 1, sa_imm_3, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(1, 1, 1, sa_imm, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // LDP  <Dt1>, <Dt2>, [<Xn|SP>], #<imm>
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(1, 1, 1, sa_imm_1, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // LDP  <Dt1>, <Dt2>, [<Xn|SP>, #<imm>]!
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(1, 1, 1, sa_imm_1, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // LDP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstpair_off(2, 1, 1, sa_imm_2, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // LDP  <Qt1>, <Qt2>, [<Xn|SP>], #<imm>
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_post(2, 1, 1, sa_imm_3, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // LDP  <Qt1>, <Qt2>, [<Xn|SP>, #<imm>]!
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(2, 1, 1, sa_imm_3, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // LDP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_4 := uint32(moffs(v2))
        return p.setins(ldstpair_off(0, 1, 1, sa_imm_4, sa_st2, sa_xn_sp, sa_st1))
    }
    // LDP  <St1>, <St2>, [<Xn|SP>], #<imm>
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_5 := uint32(moffs(v2))
        return p.setins(ldstpair_post(0, 1, 1, sa_imm_5, sa_st2, sa_xn_sp, sa_st1))
    }
    // LDP  <St1>, <St2>, [<Xn|SP>, #<imm>]!
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_5 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(0, 1, 1, sa_imm_5, sa_st2, sa_xn_sp, sa_st1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDP")
}

// LDPSW instruction have 3 forms:
//
//   * LDPSW  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * LDPSW  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
//   * LDPSW  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
//
func (self *Program) LDPSW(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDPSW", 3, Operands { v0, v1, v2 })
    // LDPSW  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(1, 0, 1, sa_imm, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDPSW  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(1, 0, 1, sa_imm_1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDPSW  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(1, 0, 1, sa_imm_1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDPSW")
}

// LDR instruction have 34 forms:
//
//   * LDR  <Wt>, [<Xn|SP>], #<simm>
//   * LDR  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Wt>, <label>
//   * LDR  <Xt>, [<Xn|SP>], #<simm>
//   * LDR  <Xt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Xt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Xt>, <label>
//   * LDR  <Bt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * LDR  <Bt>, [<Xn|SP>], #<simm>
//   * LDR  <Bt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Bt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Bt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * LDR  <Dt>, [<Xn|SP>], #<simm>
//   * LDR  <Dt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Dt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Dt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Dt>, <label>
//   * LDR  <Ht>, [<Xn|SP>], #<simm>
//   * LDR  <Ht>, [<Xn|SP>, #<simm>]!
//   * LDR  <Ht>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Ht>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Qt>, [<Xn|SP>], #<simm>
//   * LDR  <Qt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Qt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Qt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Qt>, <label>
//   * LDR  <St>, [<Xn|SP>], #<simm>
//   * LDR  <St>, [<Xn|SP>, #<simm>]!
//   * LDR  <St>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <St>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <St>, <label>
//
func (self *Program) LDR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDR", 2, Operands { v0, v1 })
    // LDR  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDR  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDR  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 0, 1, sa_pimm, sa_xn_sp, sa_wt))
    }
    // LDR  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 0, 1, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDR  <Wt>, <label>
    if isWr(v0) && isLabel(v1) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(0, 0, uint32(sa_label.RelativeTo(pc)), sa_wt) })
    }
    // LDR  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(3, 0, 1, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDR  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(3, 0, 1, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDR  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_1 := uint32(moffs(v1))
        return p.setins(ldst_pos(3, 0, 1, sa_pimm_1, sa_xn_sp, sa_xt))
    }
    // LDR  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_1 uint32
        var sa_extend uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount_1 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(3, 0, 1, sa_xm, sa_extend, sa_amount_1, sa_xn_sp, sa_xt))
    }
    // LDR  <Xt>, <label>
    if isXr(v0) && isLabel(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(1, 0, uint32(sa_label.RelativeTo(pc)), sa_xt) })
    }
    // LDR  <Bt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isBr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 1, 1, sa_xm, 3, sa_amount, sa_xn_sp, sa_bt))
    }
    // LDR  <Bt>, [<Xn|SP>], #<simm>
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 1, 1, sa_simm, sa_xn_sp, sa_bt))
    }
    // LDR  <Bt>, [<Xn|SP>, #<simm>]!
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 1, 1, sa_simm, sa_xn_sp, sa_bt))
    }
    // LDR  <Bt>, [<Xn|SP>{, #<pimm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 1, 1, sa_pimm, sa_xn_sp, sa_bt))
    }
    // LDR  <Bt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 1, 1, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_bt))
    }
    // LDR  <Dt>, [<Xn|SP>], #<simm>
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(3, 1, 1, sa_simm, sa_xn_sp, sa_dt))
    }
    // LDR  <Dt>, [<Xn|SP>, #<simm>]!
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(3, 1, 1, sa_simm, sa_xn_sp, sa_dt))
    }
    // LDR  <Dt>, [<Xn|SP>{, #<pimm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_1 := uint32(moffs(v1))
        return p.setins(ldst_pos(3, 1, 1, sa_pimm_1, sa_xn_sp, sa_dt))
    }
    // LDR  <Dt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isDr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_1 uint32
        var sa_extend_1 uint32
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_1 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(3, 1, 1, sa_xm, sa_extend_1, sa_amount_1, sa_xn_sp, sa_dt))
    }
    // LDR  <Dt>, <label>
    if isDr(v0) && isLabel(v1) {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(1, 1, uint32(sa_label.RelativeTo(pc)), sa_dt) })
    }
    // LDR  <Ht>, [<Xn|SP>], #<simm>
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 1, 1, sa_simm, sa_xn_sp, sa_ht))
    }
    // LDR  <Ht>, [<Xn|SP>, #<simm>]!
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 1, 1, sa_simm, sa_xn_sp, sa_ht))
    }
    // LDR  <Ht>, [<Xn|SP>{, #<pimm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_2 := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 1, 1, sa_pimm_2, sa_xn_sp, sa_ht))
    }
    // LDR  <Ht>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isHr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_2 uint32
        var sa_extend_1 uint32
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_2 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 1, 1, sa_xm, sa_extend_1, sa_amount_2, sa_xn_sp, sa_ht))
    }
    // LDR  <Qt>, [<Xn|SP>], #<simm>
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 1, 3, sa_simm, sa_xn_sp, sa_qt))
    }
    // LDR  <Qt>, [<Xn|SP>, #<simm>]!
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 1, 3, sa_simm, sa_xn_sp, sa_qt))
    }
    // LDR  <Qt>, [<Xn|SP>{, #<pimm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_3 := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 1, 3, sa_pimm_3, sa_xn_sp, sa_qt))
    }
    // LDR  <Qt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isQr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_3 uint32
        var sa_extend_1 uint32
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_3 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 1, 3, sa_xm, sa_extend_1, sa_amount_3, sa_xn_sp, sa_qt))
    }
    // LDR  <Qt>, <label>
    if isQr(v0) && isLabel(v1) {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(2, 1, uint32(sa_label.RelativeTo(pc)), sa_qt) })
    }
    // LDR  <St>, [<Xn|SP>], #<simm>
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 1, 1, sa_simm, sa_xn_sp, sa_st))
    }
    // LDR  <St>, [<Xn|SP>, #<simm>]!
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 1, 1, sa_simm, sa_xn_sp, sa_st))
    }
    // LDR  <St>, [<Xn|SP>{, #<pimm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_4 := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 1, 1, sa_pimm_4, sa_xn_sp, sa_st))
    }
    // LDR  <St>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isSr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_4 uint32
        var sa_extend_1 uint32
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_4 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 1, 1, sa_xm, sa_extend_1, sa_amount_4, sa_xn_sp, sa_st))
    }
    // LDR  <St>, <label>
    if isSr(v0) && isLabel(v1) {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(0, 1, uint32(sa_label.RelativeTo(pc)), sa_st) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDR")
}

// LDRAA instruction have 2 forms:
//
//   * LDRAA  <Xt>, [<Xn|SP>{, #<simm>}]!
//   * LDRAA  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDRAA(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRAA", 2, Operands { v0, v1 })
    // LDRAA  <Xt>, [<Xn|SP>{, #<simm>}]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_pac(3, 0, 0, maskp(sa_simm, 9, 1), mask(sa_simm, 9), 1, sa_xn_sp, sa_xt))
    }
    // LDRAA  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_pac(3, 0, 0, maskp(sa_simm, 9, 1), mask(sa_simm, 9), 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRAA")
}

// LDRAB instruction have 2 forms:
//
//   * LDRAB  <Xt>, [<Xn|SP>{, #<simm>}]!
//   * LDRAB  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDRAB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRAB", 2, Operands { v0, v1 })
    // LDRAB  <Xt>, [<Xn|SP>{, #<simm>}]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_pac(3, 0, 1, maskp(sa_simm, 9, 1), mask(sa_simm, 9), 1, sa_xn_sp, sa_xt))
    }
    // LDRAB  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_pac(3, 0, 1, maskp(sa_simm, 9, 1), mask(sa_simm, 9), 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRAB")
}

// LDRB instruction have 5 forms:
//
//   * LDRB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * LDRB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * LDRB  <Wt>, [<Xn|SP>], #<simm>
//   * LDRB  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDRB  <Wt>, [<Xn|SP>{, #<pimm>}]
//
func (self *Program) LDRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRB", 2, Operands { v0, v1 })
    // LDRB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 0, 1, sa_xm, 3, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 0, 1, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRB  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRB  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRB  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 0, 1, sa_pimm, sa_xn_sp, sa_wt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRB")
}

// LDRH instruction have 4 forms:
//
//   * LDRH  <Wt>, [<Xn|SP>], #<simm>
//   * LDRH  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDRH  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * LDRH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//
func (self *Program) LDRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRH", 2, Operands { v0, v1 })
    // LDRH  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRH  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRH  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 0, 1, sa_pimm, sa_xn_sp, sa_wt))
    }
    // LDRH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 0, 1, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRH")
}

// LDRSB instruction have 10 forms:
//
//   * LDRSB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * LDRSB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * LDRSB  <Wt>, [<Xn|SP>], #<simm>
//   * LDRSB  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDRSB  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * LDRSB  <Xt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * LDRSB  <Xt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * LDRSB  <Xt>, [<Xn|SP>], #<simm>
//   * LDRSB  <Xt>, [<Xn|SP>, #<simm>]!
//   * LDRSB  <Xt>, [<Xn|SP>{, #<pimm>}]
//
func (self *Program) LDRSB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRSB", 2, Operands { v0, v1 })
    // LDRSB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 0, 3, sa_xm, 3, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 0, 3, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 0, 3, sa_pimm, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Xt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 0, 2, sa_xm, 3, sa_amount, sa_xn_sp, sa_xt))
    }
    // LDRSB  <Xt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 0, 2, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_xt))
    }
    // LDRSB  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSB  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSB  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 0, 2, sa_pimm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRSB")
}

// LDRSH instruction have 8 forms:
//
//   * LDRSH  <Wt>, [<Xn|SP>], #<simm>
//   * LDRSH  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDRSH  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * LDRSH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDRSH  <Xt>, [<Xn|SP>], #<simm>
//   * LDRSH  <Xt>, [<Xn|SP>, #<simm>]!
//   * LDRSH  <Xt>, [<Xn|SP>{, #<pimm>}]
//   * LDRSH  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//
func (self *Program) LDRSH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRSH", 2, Operands { v0, v1 })
    // LDRSH  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRSH  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRSH  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 0, 3, sa_pimm, sa_xn_sp, sa_wt))
    }
    // LDRSH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 0, 3, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRSH  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSH  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSH  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 0, 2, sa_pimm, sa_xn_sp, sa_xt))
    }
    // LDRSH  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 0, 2, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRSH")
}

// LDRSW instruction have 5 forms:
//
//   * LDRSW  <Xt>, [<Xn|SP>], #<simm>
//   * LDRSW  <Xt>, [<Xn|SP>, #<simm>]!
//   * LDRSW  <Xt>, [<Xn|SP>{, #<pimm>}]
//   * LDRSW  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDRSW  <Xt>, <label>
//
func (self *Program) LDRSW(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRSW", 2, Operands { v0, v1 })
    // LDRSW  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSW  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSW  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 0, 2, sa_pimm, sa_xn_sp, sa_xt))
    }
    // LDRSW  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 0, 2, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_xt))
    }
    // LDRSW  <Xt>, <label>
    if isXr(v0) && isLabel(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(2, 0, uint32(sa_label.RelativeTo(pc)), sa_xt) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRSW")
}

// LDSET instruction have 2 forms:
//
//   * LDSET  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSET  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSET", 3, Operands { v0, v1, v2 })
    // LDSET  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    // LDSET  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 3, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSET")
}

// LDSETA instruction have 2 forms:
//
//   * LDSETA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSETA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSETA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETA", 3, Operands { v0, v1, v2 })
    // LDSETA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    // LDSETA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 3, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETA")
}

// LDSETAB instruction have one single form:
//
//   * LDSETAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETAB")
}

// LDSETAH instruction have one single form:
//
//   * LDSETAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETAH")
}

// LDSETAL instruction have 2 forms:
//
//   * LDSETAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSETAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSETAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETAL", 3, Operands { v0, v1, v2 })
    // LDSETAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    // LDSETAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 3, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETAL")
}

// LDSETALB instruction have one single form:
//
//   * LDSETALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETALB")
}

// LDSETALH instruction have one single form:
//
//   * LDSETALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETALH")
}

// LDSETB instruction have one single form:
//
//   * LDSETB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETB")
}

// LDSETH instruction have one single form:
//
//   * LDSETH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETH")
}

// LDSETL instruction have 2 forms:
//
//   * LDSETL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSETL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSETL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETL", 3, Operands { v0, v1, v2 })
    // LDSETL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    // LDSETL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 3, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETL")
}

// LDSETLB instruction have one single form:
//
//   * LDSETLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETLB")
}

// LDSETLH instruction have one single form:
//
//   * LDSETLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETLH")
}

// LDSETP instruction have one single form:
//
//   * LDSETP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDSETP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 0, sa_xt2, 0, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETP")
}

// LDSETPA instruction have one single form:
//
//   * LDSETPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDSETPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 0, sa_xt2, 0, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETPA")
}

// LDSETPAL instruction have one single form:
//
//   * LDSETPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDSETPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 1, sa_xt2, 0, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETPAL")
}

// LDSETPL instruction have one single form:
//
//   * LDSETPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDSETPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 1, sa_xt2, 0, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETPL")
}

// LDSMAX instruction have 2 forms:
//
//   * LDSMAX  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMAX  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMAX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAX", 3, Operands { v0, v1, v2 })
    // LDSMAX  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    // LDSMAX  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 4, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAX")
}

// LDSMAXA instruction have 2 forms:
//
//   * LDSMAXA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMAXA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMAXA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXA", 3, Operands { v0, v1, v2 })
    // LDSMAXA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    // LDSMAXA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 4, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXA")
}

// LDSMAXAB instruction have one single form:
//
//   * LDSMAXAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXAB")
}

// LDSMAXAH instruction have one single form:
//
//   * LDSMAXAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXAH")
}

// LDSMAXAL instruction have 2 forms:
//
//   * LDSMAXAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMAXAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMAXAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXAL", 3, Operands { v0, v1, v2 })
    // LDSMAXAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    // LDSMAXAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 4, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXAL")
}

// LDSMAXALB instruction have one single form:
//
//   * LDSMAXALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXALB")
}

// LDSMAXALH instruction have one single form:
//
//   * LDSMAXALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXALH")
}

// LDSMAXB instruction have one single form:
//
//   * LDSMAXB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXB")
}

// LDSMAXH instruction have one single form:
//
//   * LDSMAXH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXH")
}

// LDSMAXL instruction have 2 forms:
//
//   * LDSMAXL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMAXL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMAXL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXL", 3, Operands { v0, v1, v2 })
    // LDSMAXL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    // LDSMAXL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 4, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXL")
}

// LDSMAXLB instruction have one single form:
//
//   * LDSMAXLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXLB")
}

// LDSMAXLH instruction have one single form:
//
//   * LDSMAXLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXLH")
}

// LDSMIN instruction have 2 forms:
//
//   * LDSMIN  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMIN  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMIN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMIN", 3, Operands { v0, v1, v2 })
    // LDSMIN  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    // LDSMIN  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 5, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMIN")
}

// LDSMINA instruction have 2 forms:
//
//   * LDSMINA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMINA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMINA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINA", 3, Operands { v0, v1, v2 })
    // LDSMINA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    // LDSMINA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 5, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINA")
}

// LDSMINAB instruction have one single form:
//
//   * LDSMINAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINAB")
}

// LDSMINAH instruction have one single form:
//
//   * LDSMINAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINAH")
}

// LDSMINAL instruction have 2 forms:
//
//   * LDSMINAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMINAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMINAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINAL", 3, Operands { v0, v1, v2 })
    // LDSMINAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    // LDSMINAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 5, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINAL")
}

// LDSMINALB instruction have one single form:
//
//   * LDSMINALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINALB")
}

// LDSMINALH instruction have one single form:
//
//   * LDSMINALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINALH")
}

// LDSMINB instruction have one single form:
//
//   * LDSMINB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINB")
}

// LDSMINH instruction have one single form:
//
//   * LDSMINH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINH")
}

// LDSMINL instruction have 2 forms:
//
//   * LDSMINL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMINL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMINL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINL", 3, Operands { v0, v1, v2 })
    // LDSMINL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    // LDSMINL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 5, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINL")
}

// LDSMINLB instruction have one single form:
//
//   * LDSMINLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINLB")
}

// LDSMINLH instruction have one single form:
//
//   * LDSMINLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINLH")
}

// LDTR instruction have 2 forms:
//
//   * LDTR  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDTR  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDTR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDTR", 2, Operands { v0, v1 })
    // LDTR  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(2, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDTR  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(3, 0, 1, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDTR")
}

// LDTRB instruction have one single form:
//
//   * LDTRB  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDTRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDTRB", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(0, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDTRB")
}

// LDTRH instruction have one single form:
//
//   * LDTRH  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDTRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDTRH", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(1, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDTRH")
}

// LDTRSB instruction have 2 forms:
//
//   * LDTRSB  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDTRSB  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDTRSB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDTRSB", 2, Operands { v0, v1 })
    // LDTRSB  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(0, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDTRSB  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(0, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDTRSB")
}

// LDTRSH instruction have 2 forms:
//
//   * LDTRSH  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDTRSH  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDTRSH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDTRSH", 2, Operands { v0, v1 })
    // LDTRSH  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(1, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDTRSH  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(1, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDTRSH")
}

// LDTRSW instruction have one single form:
//
//   * LDTRSW  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDTRSW(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDTRSW", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(2, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDTRSW")
}

// LDUMAX instruction have 2 forms:
//
//   * LDUMAX  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMAX  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMAX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAX", 3, Operands { v0, v1, v2 })
    // LDUMAX  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    // LDUMAX  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 6, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAX")
}

// LDUMAXA instruction have 2 forms:
//
//   * LDUMAXA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMAXA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMAXA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXA", 3, Operands { v0, v1, v2 })
    // LDUMAXA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    // LDUMAXA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 6, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXA")
}

// LDUMAXAB instruction have one single form:
//
//   * LDUMAXAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXAB")
}

// LDUMAXAH instruction have one single form:
//
//   * LDUMAXAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXAH")
}

// LDUMAXAL instruction have 2 forms:
//
//   * LDUMAXAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMAXAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMAXAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXAL", 3, Operands { v0, v1, v2 })
    // LDUMAXAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    // LDUMAXAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 6, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXAL")
}

// LDUMAXALB instruction have one single form:
//
//   * LDUMAXALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXALB")
}

// LDUMAXALH instruction have one single form:
//
//   * LDUMAXALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXALH")
}

// LDUMAXB instruction have one single form:
//
//   * LDUMAXB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXB")
}

// LDUMAXH instruction have one single form:
//
//   * LDUMAXH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXH")
}

// LDUMAXL instruction have 2 forms:
//
//   * LDUMAXL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMAXL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMAXL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXL", 3, Operands { v0, v1, v2 })
    // LDUMAXL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    // LDUMAXL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 6, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXL")
}

// LDUMAXLB instruction have one single form:
//
//   * LDUMAXLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXLB")
}

// LDUMAXLH instruction have one single form:
//
//   * LDUMAXLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXLH")
}

// LDUMIN instruction have 2 forms:
//
//   * LDUMIN  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMIN  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMIN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMIN", 3, Operands { v0, v1, v2 })
    // LDUMIN  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    // LDUMIN  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 7, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMIN")
}

// LDUMINA instruction have 2 forms:
//
//   * LDUMINA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMINA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMINA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINA", 3, Operands { v0, v1, v2 })
    // LDUMINA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    // LDUMINA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 7, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINA")
}

// LDUMINAB instruction have one single form:
//
//   * LDUMINAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINAB")
}

// LDUMINAH instruction have one single form:
//
//   * LDUMINAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINAH")
}

// LDUMINAL instruction have 2 forms:
//
//   * LDUMINAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMINAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMINAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINAL", 3, Operands { v0, v1, v2 })
    // LDUMINAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    // LDUMINAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 7, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINAL")
}

// LDUMINALB instruction have one single form:
//
//   * LDUMINALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINALB")
}

// LDUMINALH instruction have one single form:
//
//   * LDUMINALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINALH")
}

// LDUMINB instruction have one single form:
//
//   * LDUMINB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINB")
}

// LDUMINH instruction have one single form:
//
//   * LDUMINH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINH")
}

// LDUMINL instruction have 2 forms:
//
//   * LDUMINL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMINL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMINL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINL", 3, Operands { v0, v1, v2 })
    // LDUMINL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    // LDUMINL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 7, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINL")
}

// LDUMINLB instruction have one single form:
//
//   * LDUMINLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINLB")
}

// LDUMINLH instruction have one single form:
//
//   * LDUMINLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINLH")
}

// LDUR instruction have 7 forms:
//
//   * LDUR  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDUR  <Xt>, [<Xn|SP>{, #<simm>}]
//   * LDUR  <Bt>, [<Xn|SP>{, #<simm>}]
//   * LDUR  <Dt>, [<Xn|SP>{, #<simm>}]
//   * LDUR  <Ht>, [<Xn|SP>{, #<simm>}]
//   * LDUR  <Qt>, [<Xn|SP>{, #<simm>}]
//   * LDUR  <St>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDUR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDUR", 2, Operands { v0, v1 })
    // LDUR  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(2, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDUR  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(3, 0, 1, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDUR  <Bt>, [<Xn|SP>{, #<simm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 1, 1, sa_simm, sa_xn_sp, sa_bt))
    }
    // LDUR  <Dt>, [<Xn|SP>{, #<simm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(3, 1, 1, sa_simm, sa_xn_sp, sa_dt))
    }
    // LDUR  <Ht>, [<Xn|SP>{, #<simm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(1, 1, 1, sa_simm, sa_xn_sp, sa_ht))
    }
    // LDUR  <Qt>, [<Xn|SP>{, #<simm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 1, 3, sa_simm, sa_xn_sp, sa_qt))
    }
    // LDUR  <St>, [<Xn|SP>{, #<simm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(2, 1, 1, sa_simm, sa_xn_sp, sa_st))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUR")
}

// LDURB instruction have one single form:
//
//   * LDURB  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDURB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDURB", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDURB")
}

// LDURH instruction have one single form:
//
//   * LDURH  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDURH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDURH", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(1, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDURH")
}

// LDURSB instruction have 2 forms:
//
//   * LDURSB  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDURSB  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDURSB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDURSB", 2, Operands { v0, v1 })
    // LDURSB  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDURSB  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDURSB")
}

// LDURSH instruction have 2 forms:
//
//   * LDURSH  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDURSH  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDURSH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDURSH", 2, Operands { v0, v1 })
    // LDURSH  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(1, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDURSH  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(1, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDURSH")
}

// LDURSW instruction have one single form:
//
//   * LDURSW  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDURSW(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDURSW", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(2, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDURSW")
}

// LDXP instruction have 2 forms:
//
//   * LDXP  <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
//   * LDXP  <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
//
func (self *Program) LDXP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDXP", 3, Operands { v0, v1, v2 })
    // LDXP  <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclp(0, 1, 31, 0, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDXP  <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclp(1, 1, 31, 0, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDXP")
}

// LDXR instruction have 2 forms:
//
//   * LDXR  <Wt>, [<Xn|SP>{,#0}]
//   * LDXR  <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDXR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDXR", 2, Operands { v0, v1 })
    // LDXR  <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(2, 1, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    // LDXR  <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(3, 1, 31, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDXR")
}

// LDXRB instruction have one single form:
//
//   * LDXRB  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDXRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDXRB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(0, 1, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDXRB")
}

// LDXRH instruction have one single form:
//
//   * LDXRH  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDXRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDXRH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(1, 1, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDXRH")
}

// LSLV instruction have 2 forms:
//
//   * LSLV  <Wd>, <Wn>, <Wm>
//   * LSLV  <Xd>, <Xn>, <Xm>
//
func (self *Program) LSLV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LSLV", 3, Operands { v0, v1, v2 })
    // LSLV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 8, sa_wn, sa_wd))
    }
    // LSLV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 8, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LSLV")
}

// LSRV instruction have 2 forms:
//
//   * LSRV  <Wd>, <Wn>, <Wm>
//   * LSRV  <Xd>, <Xn>, <Xm>
//
func (self *Program) LSRV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LSRV", 3, Operands { v0, v1, v2 })
    // LSRV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 9, sa_wn, sa_wd))
    }
    // LSRV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 9, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LSRV")
}

// MADD instruction have 2 forms:
//
//   * MADD  <Wd>, <Wn>, <Wm>, <Wa>
//   * MADD  <Xd>, <Xn>, <Xm>, <Xa>
//
func (self *Program) MADD(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("MADD", 4, Operands { v0, v1, v2, v3 })
    // MADD  <Wd>, <Wn>, <Wm>, <Wa>
    if isWr(v0) && isWr(v1) && isWr(v2) && isWr(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_wa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(0, 0, 0, sa_wm, 0, sa_wa, sa_wn, sa_wd))
    }
    // MADD  <Xd>, <Xn>, <Xm>, <Xa>
    if isXr(v0) && isXr(v1) && isXr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 0, sa_xm, 0, sa_xa, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MADD")
}

// MLA instruction have 2 forms:
//
//   * MLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
//   * MLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) MLA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("MLA", 3, Operands { v0, v1, v2 })
    // MLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVri(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != maskp(sa_t, 1, 2) || maskp(sa_t, 1, 2) != sa_ts || sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for MLA")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for MLA")
        }
        return p.setins(asimdelem(
            sa_t & 1,
            1,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            0,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // MLA  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 18, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MLA")
}

// MLS instruction have 2 forms:
//
//   * MLS  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
//   * MLS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) MLS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("MLS", 3, Operands { v0, v1, v2 })
    // MLS  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVri(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != maskp(sa_t, 1, 2) || maskp(sa_t, 1, 2) != sa_ts || sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for MLS")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for MLS")
        }
        return p.setins(asimdelem(
            sa_t & 1,
            1,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            4,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // MLS  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 18, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MLS")
}

// MOVI instruction have 6 forms:
//
//   * MOVI  <Vd>.2D, #<imm>
//   * MOVI  <Dd>, #<imm>
//   * MOVI  <Vd>.<T>, #<imm8>{, LSL #<amount>}
//   * MOVI  <Vd>.<T>, #<imm8>{, LSL #<amount>}
//   * MOVI  <Vd>.<T>, #<imm8>, MSL #<amount>
//   * MOVI  <Vd>.<T>, #<imm8>{, LSL #0}
//
func (self *Program) MOVI(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("MOVI", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("MOVI", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction MOVI takes 2 or 3 operands")
    }
    // MOVI  <Vd>.2D, #<imm>
    if isVr(v0) && vfmt(v0) == Vec2D && isUimm8(v1) {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm8(v1)
        return p.setins(asimdimm(
            1,
            1,
            maskp(sa_imm, 7, 1),
            maskp(sa_imm, 6, 1),
            maskp(sa_imm, 5, 1),
            14,
            0,
            maskp(sa_imm, 4, 1),
            maskp(sa_imm, 3, 1),
            maskp(sa_imm, 2, 1),
            maskp(sa_imm, 1, 1),
            sa_imm & 1,
            sa_vd,
        ))
    }
    // MOVI  <Dd>, #<imm>
    if isDr(v0) && isUimm8(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm8(v1)
        return p.setins(asimdimm(
            0,
            1,
            maskp(sa_imm, 7, 1),
            maskp(sa_imm, 6, 1),
            maskp(sa_imm, 5, 1),
            14,
            0,
            maskp(sa_imm, 4, 1),
            maskp(sa_imm, 3, 1),
            maskp(sa_imm, 2, 1),
            maskp(sa_imm, 1, 1),
            sa_imm & 1,
            sa_dd,
        ))
    }
    // MOVI  <Vd>.<T>, #<imm8>{, LSL #<amount>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isUimm8(v1) &&
       (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_amount uint32
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        if len(vv) == 1 {
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        cmode := uint32(0b1000)
        switch sa_amount {
            case 0: cmode |= 0b0 << 1
            case 8: cmode |= 0b1 << 1
            default: panic("aarch64: invalid combination of operands for MOVI")
        }
        return p.setins(asimdimm(
            sa_t,
            0,
            maskp(sa_imm8, 7, 1),
            maskp(sa_imm8, 6, 1),
            maskp(sa_imm8, 5, 1),
            cmode,
            0,
            maskp(sa_imm8, 4, 1),
            maskp(sa_imm8, 3, 1),
            maskp(sa_imm8, 2, 1),
            maskp(sa_imm8, 1, 1),
            sa_imm8 & 1,
            sa_vd,
        ))
    }
    // MOVI  <Vd>.<T>, #<imm8>{, LSL #<amount>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isUimm8(v1) &&
       (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_amount_1 uint32
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t_1 = 0b0
            case Vec4S: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        if len(vv) == 1 {
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        cmode := uint32(0b0000)
        switch sa_amount_1 {
            case 0: cmode |= 0b00 << 1
            case 8: cmode |= 0b01 << 1
            case 16: cmode |= 0b10 << 1
            case 24: cmode |= 0b11 << 1
            default: panic("aarch64: invalid combination of operands for MOVI")
        }
        return p.setins(asimdimm(
            sa_t_1,
            0,
            maskp(sa_imm8, 7, 1),
            maskp(sa_imm8, 6, 1),
            maskp(sa_imm8, 5, 1),
            cmode,
            0,
            maskp(sa_imm8, 4, 1),
            maskp(sa_imm8, 3, 1),
            maskp(sa_imm8, 2, 1),
            maskp(sa_imm8, 1, 1),
            sa_imm8 & 1,
            sa_vd,
        ))
    }
    // MOVI  <Vd>.<T>, #<imm8>, MSL #<amount>
    if len(vv) == 1 && isVr(v0) && isVfmt(v0, Vec2S, Vec4S) && isUimm8(v1) && isSameMod(vv[0], MSL(0)) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t_1 = 0b0
            case Vec4S: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        sa_amount_2 := uint32(vv[0].(Modifier).Amount())
        cmode := uint32(0b1100)
        switch sa_amount_2 {
            case 8: cmode |= 0b0 << 0
            case 16: cmode |= 0b1 << 0
            default: panic("aarch64: invalid combination of operands for MOVI")
        }
        return p.setins(asimdimm(
            sa_t_1,
            0,
            maskp(sa_imm8, 7, 1),
            maskp(sa_imm8, 6, 1),
            maskp(sa_imm8, 5, 1),
            cmode,
            0,
            maskp(sa_imm8, 4, 1),
            maskp(sa_imm8, 3, 1),
            maskp(sa_imm8, 2, 1),
            maskp(sa_imm8, 1, 1),
            sa_imm8 & 1,
            sa_vd,
        ))
    }
    // MOVI  <Vd>.<T>, #<imm8>{, LSL #0}
    if (len(vv) == 0 || len(vv) == 1) &&
       isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isUimm8(v1) &&
       (len(vv) == 0 || isSameMod(vv[0], LSL(0)) && modn(vv[0]) == 0) {
        var sa_t_2 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t_2 = 0b0
            case Vec16B: sa_t_2 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        return p.setins(asimdimm(
            sa_t_2,
            0,
            maskp(sa_imm8, 7, 1),
            maskp(sa_imm8, 6, 1),
            maskp(sa_imm8, 5, 1),
            14,
            0,
            maskp(sa_imm8, 4, 1),
            maskp(sa_imm8, 3, 1),
            maskp(sa_imm8, 2, 1),
            maskp(sa_imm8, 1, 1),
            sa_imm8 & 1,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MOVI")
}

// MOVK instruction have 2 forms:
//
//   * MOVK  <Wd>, #<imm>{, LSL #<shift>}
//   * MOVK  <Xd>, #<imm>{, LSL #<shift>}
//
func (self *Program) MOVK(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("MOVK", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("MOVK", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction MOVK takes 2 or 3 operands")
    }
    // MOVK  <Wd>, #<imm>{, LSL #<shift>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isUimm16(v1) && (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm16(v1)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(movewide(0, 3, sa_shift, sa_imm, sa_wd))
    }
    // MOVK  <Xd>, #<imm>{, LSL #<shift>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isUimm16(v1) && (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_shift_1 uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm16(v1)
        if len(vv) == 1 {
            sa_shift_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(movewide(1, 3, sa_shift_1, sa_imm, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MOVK")
}

// MOVN instruction have 2 forms:
//
//   * MOVN  <Wd>, #<imm>{, LSL #<shift>}
//   * MOVN  <Xd>, #<imm>{, LSL #<shift>}
//
func (self *Program) MOVN(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("MOVN", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("MOVN", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction MOVN takes 2 or 3 operands")
    }
    // MOVN  <Wd>, #<imm>{, LSL #<shift>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isUimm16(v1) && (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm16(v1)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(movewide(0, 0, sa_shift, sa_imm, sa_wd))
    }
    // MOVN  <Xd>, #<imm>{, LSL #<shift>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isUimm16(v1) && (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_shift_1 uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm16(v1)
        if len(vv) == 1 {
            sa_shift_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(movewide(1, 0, sa_shift_1, sa_imm, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MOVN")
}

// MOVZ instruction have 2 forms:
//
//   * MOVZ  <Wd>, #<imm>{, LSL #<shift>}
//   * MOVZ  <Xd>, #<imm>{, LSL #<shift>}
//
func (self *Program) MOVZ(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("MOVZ", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("MOVZ", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction MOVZ takes 2 or 3 operands")
    }
    // MOVZ  <Wd>, #<imm>{, LSL #<shift>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isUimm16(v1) && (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm16(v1)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(movewide(0, 2, sa_shift, sa_imm, sa_wd))
    }
    // MOVZ  <Xd>, #<imm>{, LSL #<shift>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isUimm16(v1) && (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_shift_1 uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm16(v1)
        if len(vv) == 1 {
            sa_shift_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(movewide(1, 2, sa_shift_1, sa_imm, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MOVZ")
}

// MRRS instruction have one single form:
//
//   * MRRS  <Xt>, <Xt+1>, (<systemreg>|S<op0>_<op1>_<Cn>_<Cm>_<op2>)
//
func (self *Program) MRRS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("MRRS", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXr(v1) && isNextReg(v1, v0, 1) && isSysReg(v2) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_systemreg := uint32(v2.(SystemRegister))
        return p.setins(systemmovepr(
            1,
            maskp(sa_systemreg, 6, 1),
            maskp(sa_systemreg, 3, 3),
            maskp(sa_systemreg, 7, 4),
            maskp(sa_systemreg, 11, 4),
            mask(sa_systemreg, 3),
            sa_xt,
        ))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for MRRS")
}

// MRS instruction have one single form:
//
//   * MRS  <Xt>, (<systemreg>|S<op0>_<op1>_<Cn>_<Cm>_<op2>)
//
func (self *Program) MRS(v0, v1 interface{}) *Instruction {
    p := self.alloc("MRS", 2, Operands { v0, v1 })
    if isXr(v0) && isSysReg(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_systemreg := uint32(v1.(SystemRegister))
        return p.setins(systemmove(
            1,
            maskp(sa_systemreg, 6, 1),
            maskp(sa_systemreg, 3, 3),
            maskp(sa_systemreg, 7, 4),
            maskp(sa_systemreg, 11, 4),
            mask(sa_systemreg, 3),
            sa_xt,
        ))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for MRS")
}

// MSR instruction have 2 forms:
//
//   * MSR  <pstatefield>, #<imm>
//   * MSR  (<systemreg>|S<op0>_<op1>_<Cn>_<Cm>_<op2>), <Xt>
//
func (self *Program) MSR(v0, v1 interface{}) *Instruction {
    p := self.alloc("MSR", 2, Operands { v0, v1 })
    // MSR  <pstatefield>, #<imm>
    if isPState(v0) && isUimm4(v1) {
        sa_pstatefield := uint32(v0.(PStateField))
        sa_imm := asUimm4(v1)
        if sa_imm != mask(sa_pstatefield, 4) {
            panic("aarch64: invalid combination of operands for MSR")
        }
        return p.setins(pstate(maskp(sa_pstatefield, 7, 3), sa_imm, maskp(sa_pstatefield, 4, 3), 31))
    }
    // MSR  (<systemreg>|S<op0>_<op1>_<Cn>_<Cm>_<op2>), <Xt>
    if isSysReg(v0) && isXr(v1) {
        sa_systemreg := uint32(v0.(SystemRegister))
        sa_xt := uint32(v1.(asm.Register).ID())
        return p.setins(systemmove(
            0,
            maskp(sa_systemreg, 6, 1),
            maskp(sa_systemreg, 3, 3),
            maskp(sa_systemreg, 7, 4),
            maskp(sa_systemreg, 11, 4),
            mask(sa_systemreg, 3),
            sa_xt,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MSR")
}

// MSRR instruction have one single form:
//
//   * MSRR  (<systemreg>|S<op0>_<op1>_<Cn>_<Cm>_<op2>), <Xt>, <Xt+1>
//
func (self *Program) MSRR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("MSRR", 3, Operands { v0, v1, v2 })
    if isSysReg(v0) && isXr(v1) && isXr(v2) && isNextReg(v2, v1, 1) {
        sa_systemreg := uint32(v0.(SystemRegister))
        sa_xt := uint32(v1.(asm.Register).ID())
        return p.setins(systemmovepr(
            0,
            maskp(sa_systemreg, 6, 1),
            maskp(sa_systemreg, 3, 3),
            maskp(sa_systemreg, 7, 4),
            maskp(sa_systemreg, 11, 4),
            mask(sa_systemreg, 3),
            sa_xt,
        ))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for MSRR")
}

// MSUB instruction have 2 forms:
//
//   * MSUB  <Wd>, <Wn>, <Wm>, <Wa>
//   * MSUB  <Xd>, <Xn>, <Xm>, <Xa>
//
func (self *Program) MSUB(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("MSUB", 4, Operands { v0, v1, v2, v3 })
    // MSUB  <Wd>, <Wn>, <Wm>, <Wa>
    if isWr(v0) && isWr(v1) && isWr(v2) && isWr(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_wa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(0, 0, 0, sa_wm, 1, sa_wa, sa_wn, sa_wd))
    }
    // MSUB  <Xd>, <Xn>, <Xm>, <Xa>
    if isXr(v0) && isXr(v1) && isXr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 0, sa_xm, 1, sa_xa, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MSUB")
}

// MUL instruction have 2 forms:
//
//   * MUL  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
//   * MUL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) MUL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("MUL", 3, Operands { v0, v1, v2 })
    // MUL  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVri(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != maskp(sa_t, 1, 2) || maskp(sa_t, 1, 2) != sa_ts || sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for MUL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for MUL")
        }
        return p.setins(asimdelem(
            sa_t & 1,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            8,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // MUL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 19, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MUL")
}

// MVNI instruction have 3 forms:
//
//   * MVNI  <Vd>.<T>, #<imm8>{, LSL #<amount>}
//   * MVNI  <Vd>.<T>, #<imm8>{, LSL #<amount>}
//   * MVNI  <Vd>.<T>, #<imm8>, MSL #<amount>
//
func (self *Program) MVNI(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("MVNI", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("MVNI", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction MVNI takes 2 or 3 operands")
    }
    // MVNI  <Vd>.<T>, #<imm8>{, LSL #<amount>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isUimm8(v1) &&
       (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_amount uint32
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        if len(vv) == 1 {
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        cmode := uint32(0b1000)
        switch sa_amount {
            case 0: cmode |= 0b0 << 1
            case 8: cmode |= 0b1 << 1
            default: panic("aarch64: invalid combination of operands for MVNI")
        }
        return p.setins(asimdimm(
            sa_t,
            1,
            maskp(sa_imm8, 7, 1),
            maskp(sa_imm8, 6, 1),
            maskp(sa_imm8, 5, 1),
            cmode,
            0,
            maskp(sa_imm8, 4, 1),
            maskp(sa_imm8, 3, 1),
            maskp(sa_imm8, 2, 1),
            maskp(sa_imm8, 1, 1),
            sa_imm8 & 1,
            sa_vd,
        ))
    }
    // MVNI  <Vd>.<T>, #<imm8>{, LSL #<amount>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isUimm8(v1) &&
       (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_amount_1 uint32
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t_1 = 0b0
            case Vec4S: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        if len(vv) == 1 {
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        cmode := uint32(0b0000)
        switch sa_amount_1 {
            case 0: cmode |= 0b00 << 1
            case 8: cmode |= 0b01 << 1
            case 16: cmode |= 0b10 << 1
            case 24: cmode |= 0b11 << 1
            default: panic("aarch64: invalid combination of operands for MVNI")
        }
        return p.setins(asimdimm(
            sa_t_1,
            1,
            maskp(sa_imm8, 7, 1),
            maskp(sa_imm8, 6, 1),
            maskp(sa_imm8, 5, 1),
            cmode,
            0,
            maskp(sa_imm8, 4, 1),
            maskp(sa_imm8, 3, 1),
            maskp(sa_imm8, 2, 1),
            maskp(sa_imm8, 1, 1),
            sa_imm8 & 1,
            sa_vd,
        ))
    }
    // MVNI  <Vd>.<T>, #<imm8>, MSL #<amount>
    if len(vv) == 1 && isVr(v0) && isVfmt(v0, Vec2S, Vec4S) && isUimm8(v1) && isSameMod(vv[0], MSL(0)) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t_1 = 0b0
            case Vec4S: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        sa_amount_2 := uint32(vv[0].(Modifier).Amount())
        cmode := uint32(0b1100)
        switch sa_amount_2 {
            case 8: cmode |= 0b0 << 0
            case 16: cmode |= 0b1 << 0
            default: panic("aarch64: invalid combination of operands for MVNI")
        }
        return p.setins(asimdimm(
            sa_t_1,
            1,
            maskp(sa_imm8, 7, 1),
            maskp(sa_imm8, 6, 1),
            maskp(sa_imm8, 5, 1),
            cmode,
            0,
            maskp(sa_imm8, 4, 1),
            maskp(sa_imm8, 3, 1),
            maskp(sa_imm8, 2, 1),
            maskp(sa_imm8, 1, 1),
            sa_imm8 & 1,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MVNI")
}

// NEG instruction have 2 forms:
//
//   * NEG  <Vd>.<T>, <Vn>.<T>
//   * NEG  <V><d>, <V><n>
//
func (self *Program) NEG(v0, v1 interface{}) *Instruction {
    p := self.alloc("NEG", 2, Operands { v0, v1 })
    // NEG  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 1, maskp(sa_t, 1, 2), 11, sa_vn, sa_vd))
    }
    // NEG  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for NEG")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmisc(1, sa_v, 11, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for NEG")
}

// NOP instruction have one single form:
//
//   * NOP
//
func (self *Program) NOP() *Instruction {
    p := self.alloc("NOP", 0, Operands {})
    return p.setins(hints(0, 0))
}

// NOT instruction have one single form:
//
//   * NOT  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) NOT(v0, v1 interface{}) *Instruction {
    p := self.alloc("NOT", 2, Operands { v0, v1 })
    if isVr(v0) && isVfmt(v0, Vec8B, Vec16B) && isVr(v1) && isVfmt(v1, Vec8B, Vec16B) && vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t, 1, 0, 5, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for NOT")
}

// ORN instruction have 3 forms:
//
//   * ORN  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ORN  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//   * ORN  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) ORN(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ORN", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ORN", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ORN takes 3 or 4 operands")
    }
    // ORN  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 1, sa_shift, 1, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ORN  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 1, sa_shift, 1, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // ORN  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t, 0, 3, sa_vm, 3, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ORN")
}

// ORR instruction have 7 forms:
//
//   * ORR  <Wd|WSP>, <Wn>, #<imm>
//   * ORR  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ORR  <Xd|SP>, <Xn>, #<imm>
//   * ORR  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//   * ORR  <Vd>.<T>, #<imm8>{, LSL #<amount>}
//   * ORR  <Vd>.<T>, #<imm8>{, LSL #<amount>}
//   * ORR  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) ORR(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ORR", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("ORR", 3, Operands { v0, v1, vv[0] })
        case 2  : p = self.alloc("ORR", 4, Operands { v0, v1, vv[0], vv[1] })
        default : panic("instruction ORR takes 2 or 3 or 4 operands")
    }
    // ORR  <Wd|WSP>, <Wn>, #<imm>
    if len(vv) == 1 && isWrOrWSP(v0) && isWr(v1) && isMask32(vv[0]) {
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_imm := asMaskOp(vv[0])
        return p.setins(log_imm(0, 1, 0, maskp(sa_imm, 6, 6), mask(sa_imm, 6), sa_wn, sa_wd_wsp))
    }
    // ORR  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 1 || len(vv) == 2) && isWr(v0) && isWr(v1) && isWr(vv[0]) && (len(vv) == 0 || isShift(vv[1])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(vv[0].(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[1].(ShiftType).ShiftType())
            sa_amount = uint32(vv[1].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 1, sa_shift, 0, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ORR  <Xd|SP>, <Xn>, #<imm>
    if len(vv) == 1 && isXrOrSP(v0) && isXr(v1) && isMask64(vv[0]) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_imm_1 := asMaskOp(vv[0])
        return p.setins(log_imm(1, 1, maskp(sa_imm_1, 12, 1), maskp(sa_imm_1, 6, 6), mask(sa_imm_1, 6), sa_xn, sa_xd_sp))
    }
    // ORR  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 1 || len(vv) == 2) && isXr(v0) && isXr(v1) && isXr(vv[0]) && (len(vv) == 0 || isShift(vv[1])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(vv[0].(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[1].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[1].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 1, sa_shift, 0, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // ORR  <Vd>.<T>, #<imm8>{, LSL #<amount>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H) &&
       isUimm8(v1) &&
       (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_amount uint32
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b0
            case Vec8H: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        if len(vv) == 1 {
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        cmode := uint32(0b1001)
        switch sa_amount {
            case 0: cmode |= 0b0 << 1
            case 8: cmode |= 0b1 << 1
            default: panic("aarch64: invalid combination of operands for ORR")
        }
        return p.setins(asimdimm(
            sa_t,
            0,
            maskp(sa_imm8, 7, 1),
            maskp(sa_imm8, 6, 1),
            maskp(sa_imm8, 5, 1),
            cmode,
            0,
            maskp(sa_imm8, 4, 1),
            maskp(sa_imm8, 3, 1),
            maskp(sa_imm8, 2, 1),
            maskp(sa_imm8, 1, 1),
            sa_imm8 & 1,
            sa_vd,
        ))
    }
    // ORR  <Vd>.<T>, #<imm8>{, LSL #<amount>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isUimm8(v1) &&
       (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_amount_1 uint32
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t_1 = 0b0
            case Vec4S: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm8 := asUimm8(v1)
        if len(vv) == 1 {
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        cmode := uint32(0b0001)
        switch sa_amount_1 {
            case 0: cmode |= 0b00 << 1
            case 8: cmode |= 0b01 << 1
            case 16: cmode |= 0b10 << 1
            case 24: cmode |= 0b11 << 1
            default: panic("aarch64: invalid combination of operands for ORR")
        }
        return p.setins(asimdimm(
            sa_t_1,
            0,
            maskp(sa_imm8, 7, 1),
            maskp(sa_imm8, 6, 1),
            maskp(sa_imm8, 5, 1),
            cmode,
            0,
            maskp(sa_imm8, 4, 1),
            maskp(sa_imm8, 3, 1),
            maskp(sa_imm8, 2, 1),
            maskp(sa_imm8, 1, 1),
            sa_imm8 & 1,
            sa_vd,
        ))
    }
    // ORR  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(vv[0]) &&
       isVfmt(vv[0], Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(vv[0]) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(vv[0].(asm.Register).ID())
        return p.setins(asimdsame(sa_t, 0, 2, sa_vm, 3, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ORR")
}

// PACDA instruction have one single form:
//
//   * PACDA  <Xd>, <Xn|SP>
//
func (self *Program) PACDA(v0, v1 interface{}) *Instruction {
    p := self.alloc("PACDA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 2, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACDA")
}

// PACDB instruction have one single form:
//
//   * PACDB  <Xd>, <Xn|SP>
//
func (self *Program) PACDB(v0, v1 interface{}) *Instruction {
    p := self.alloc("PACDB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 3, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACDB")
}

// PACDZA instruction have one single form:
//
//   * PACDZA  <Xd>
//
func (self *Program) PACDZA(v0 interface{}) *Instruction {
    p := self.alloc("PACDZA", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 10, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACDZA")
}

// PACDZB instruction have one single form:
//
//   * PACDZB  <Xd>
//
func (self *Program) PACDZB(v0 interface{}) *Instruction {
    p := self.alloc("PACDZB", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 11, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACDZB")
}

// PACGA instruction have one single form:
//
//   * PACGA  <Xd>, <Xn>, <Xm|SP>
//
func (self *Program) PACGA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("PACGA", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXr(v1) && isXrOrSP(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm_sp := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm_sp, 12, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACGA")
}

// PACIA instruction have one single form:
//
//   * PACIA  <Xd>, <Xn|SP>
//
func (self *Program) PACIA(v0, v1 interface{}) *Instruction {
    p := self.alloc("PACIA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 0, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACIA")
}

// PACIA1716 instruction have one single form:
//
//   * PACIA1716
//
func (self *Program) PACIA1716() *Instruction {
    p := self.alloc("PACIA1716", 0, Operands {})
    return p.setins(hints(1, 0))
}

// PACIASP instruction have one single form:
//
//   * PACIASP
//
func (self *Program) PACIASP() *Instruction {
    p := self.alloc("PACIASP", 0, Operands {})
    return p.setins(hints(3, 1))
}

// PACIAZ instruction have one single form:
//
//   * PACIAZ
//
func (self *Program) PACIAZ() *Instruction {
    p := self.alloc("PACIAZ", 0, Operands {})
    return p.setins(hints(3, 0))
}

// PACIB instruction have one single form:
//
//   * PACIB  <Xd>, <Xn|SP>
//
func (self *Program) PACIB(v0, v1 interface{}) *Instruction {
    p := self.alloc("PACIB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 1, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACIB")
}

// PACIB1716 instruction have one single form:
//
//   * PACIB1716
//
func (self *Program) PACIB1716() *Instruction {
    p := self.alloc("PACIB1716", 0, Operands {})
    return p.setins(hints(1, 2))
}

// PACIBSP instruction have one single form:
//
//   * PACIBSP
//
func (self *Program) PACIBSP() *Instruction {
    p := self.alloc("PACIBSP", 0, Operands {})
    return p.setins(hints(3, 3))
}

// PACIBZ instruction have one single form:
//
//   * PACIBZ
//
func (self *Program) PACIBZ() *Instruction {
    p := self.alloc("PACIBZ", 0, Operands {})
    return p.setins(hints(3, 2))
}

// PACIZA instruction have one single form:
//
//   * PACIZA  <Xd>
//
func (self *Program) PACIZA(v0 interface{}) *Instruction {
    p := self.alloc("PACIZA", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 8, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACIZA")
}

// PACIZB instruction have one single form:
//
//   * PACIZB  <Xd>
//
func (self *Program) PACIZB(v0 interface{}) *Instruction {
    p := self.alloc("PACIZB", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 9, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACIZB")
}

// PMUL instruction have one single form:
//
//   * PMUL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) PMUL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("PMUL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 19, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PMUL")
}

// PMULL instruction have one single form:
//
//   * PMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) PMULL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("PMULL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec1Q) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec1D, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec1D, Vec2D) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec1Q: sa_ta = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec1D: sa_tb = 0b110
            case Vec2D: sa_tb = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for PMULL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for PMULL")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 14, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PMULL")
}

// PMULL2 instruction have one single form:
//
//   * PMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) PMULL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("PMULL2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec1Q) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec1D, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec1D, Vec2D) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec1Q: sa_ta = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec1D: sa_tb = 0b110
            case Vec2D: sa_tb = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for PMULL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for PMULL2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 14, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PMULL2")
}

// PRFM instruction have 3 forms:
//
//   * PRFM  (<prfop>|#<imm5>), [<Xn|SP>{, #<pimm>}]
//   * PRFM  (<prfop>|#<imm5>), [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * PRFM  (<prfop>|#<imm5>), <label>
//
func (self *Program) PRFM(v0, v1 interface{}) *Instruction {
    p := self.alloc("PRFM", 2, Operands { v0, v1 })
    // PRFM  (<prfop>|#<imm5>), [<Xn|SP>{, #<pimm>}]
    if isBasicPrf(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_prfop := uint32(v0.(PrefetchOp))
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(3, 0, 2, sa_pimm, sa_xn_sp, sa_prfop))
    }
    // PRFM  (<prfop>|#<imm5>), [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isBasicPrf(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_prfop := uint32(v0.(PrefetchOp))
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(3, 0, 2, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_prfop))
    }
    // PRFM  (<prfop>|#<imm5>), <label>
    if isBasicPrf(v0) && isLabel(v1) {
        sa_prfop := uint32(v0.(PrefetchOp))
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(3, 0, uint32(sa_label.RelativeTo(pc)), sa_prfop) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for PRFM")
}

// PRFUM instruction have one single form:
//
//   * PRFUM (<prfop>|#<imm5>), [<Xn|SP>{, #<simm>}]
//
func (self *Program) PRFUM(v0, v1 interface{}) *Instruction {
    p := self.alloc("PRFUM", 2, Operands { v0, v1 })
    if isBasicPrf(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_prfop := uint32(v0.(PrefetchOp))
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(3, 0, 2, sa_simm, sa_xn_sp, sa_prfop))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PRFUM")
}

// PSB instruction have one single form:
//
//   * PSB CSYNC
//
func (self *Program) PSB(v0 interface{}) *Instruction {
    p := self.alloc("PSB", 1, Operands { v0 })
    if v0 == CSYNC {
        return p.setins(hints(2, 1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PSB")
}

// RADDHN instruction have one single form:
//
//   * RADDHN  <Vd>.<Tb>, <Vn>.<Ta>, <Vm>.<Ta>
//
func (self *Program) RADDHN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RADDHN", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8H, Vec4S, Vec2D) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for RADDHN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for RADDHN")
        }
        return p.setins(asimddiff(0, 1, sa_ta, sa_vm, 4, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RADDHN")
}

// RADDHN2 instruction have one single form:
//
//   * RADDHN2  <Vd>.<Tb>, <Vn>.<Ta>, <Vm>.<Ta>
//
func (self *Program) RADDHN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RADDHN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8H, Vec4S, Vec2D) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for RADDHN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for RADDHN2")
        }
        return p.setins(asimddiff(1, 1, sa_ta, sa_vm, 4, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RADDHN2")
}

// RAX1 instruction have one single form:
//
//   * RAX1  <Vd>.2D, <Vn>.2D, <Vm>.2D
//
func (self *Program) RAX1(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RAX1", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec2D && isVr(v1) && vfmt(v1) == Vec2D && isVr(v2) && vfmt(v2) == Vec2D {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha512_3(sa_vm, 0, 3, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RAX1")
}

// RBIT instruction have 3 forms:
//
//   * RBIT  <Wd>, <Wn>
//   * RBIT  <Xd>, <Xn>
//   * RBIT  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) RBIT(v0, v1 interface{}) *Instruction {
    p := self.alloc("RBIT", 2, Operands { v0, v1 })
    // RBIT  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 0, sa_wn, sa_wd))
    }
    // RBIT  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 0, sa_xn, sa_xd))
    }
    // RBIT  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec8B, Vec16B) && isVr(v1) && isVfmt(v1, Vec8B, Vec16B) && vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b0
            case Vec16B: sa_t = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t, 1, 1, 5, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for RBIT")
}

// RCWCAS instruction have one single form:
//
//   * RCWCAS  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCAS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCAS", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(0, 0, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCAS")
}

// RCWCASA instruction have one single form:
//
//   * RCWCASA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCASA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCASA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(0, 1, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASA")
}

// RCWCASAL instruction have one single form:
//
//   * RCWCASAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCASAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCASAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(0, 1, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASAL")
}

// RCWCASL instruction have one single form:
//
//   * RCWCASL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCASL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCASL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(0, 0, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASL")
}

// RCWCASP instruction have one single form:
//
//   * RCWCASP  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWCASP(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWCASP", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(0, 0, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASP")
}

// RCWCASPA instruction have one single form:
//
//   * RCWCASPA  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWCASPA(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWCASPA", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(0, 1, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASPA")
}

// RCWCASPAL instruction have one single form:
//
//   * RCWCASPAL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWCASPAL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWCASPAL", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(0, 1, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASPAL")
}

// RCWCASPL instruction have one single form:
//
//   * RCWCASPL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWCASPL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWCASPL", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(0, 0, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASPL")
}

// RCWCLR instruction have one single form:
//
//   * RCWCLR  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCLR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLR", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLR")
}

// RCWCLRA instruction have one single form:
//
//   * RCWCLRA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCLRA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRA")
}

// RCWCLRAL instruction have one single form:
//
//   * RCWCLRAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCLRAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRAL")
}

// RCWCLRL instruction have one single form:
//
//   * RCWCLRL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCLRL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRL")
}

// RCWCLRP instruction have one single form:
//
//   * RCWCLRP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWCLRP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 0, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRP")
}

// RCWCLRPA instruction have one single form:
//
//   * RCWCLRPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWCLRPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 0, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRPA")
}

// RCWCLRPAL instruction have one single form:
//
//   * RCWCLRPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWCLRPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 1, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRPAL")
}

// RCWCLRPL instruction have one single form:
//
//   * RCWCLRPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWCLRPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 1, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRPL")
}

// RCWSCAS instruction have one single form:
//
//   * RCWSCAS  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCAS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCAS", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(1, 0, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCAS")
}

// RCWSCASA instruction have one single form:
//
//   * RCWSCASA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCASA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCASA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(1, 1, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASA")
}

// RCWSCASAL instruction have one single form:
//
//   * RCWSCASAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCASAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCASAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(1, 1, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASAL")
}

// RCWSCASL instruction have one single form:
//
//   * RCWSCASL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCASL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCASL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(1, 0, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASL")
}

// RCWSCASP instruction have one single form:
//
//   * RCWSCASP  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWSCASP(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWSCASP", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(1, 0, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASP")
}

// RCWSCASPA instruction have one single form:
//
//   * RCWSCASPA  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWSCASPA(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWSCASPA", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(1, 1, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASPA")
}

// RCWSCASPAL instruction have one single form:
//
//   * RCWSCASPAL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWSCASPAL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWSCASPAL", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(1, 1, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASPAL")
}

// RCWSCASPL instruction have one single form:
//
//   * RCWSCASPL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWSCASPL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWSCASPL", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(1, 0, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASPL")
}

// RCWSCLR instruction have one single form:
//
//   * RCWSCLR  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCLR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLR", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLR")
}

// RCWSCLRA instruction have one single form:
//
//   * RCWSCLRA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCLRA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRA")
}

// RCWSCLRAL instruction have one single form:
//
//   * RCWSCLRAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCLRAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRAL")
}

// RCWSCLRL instruction have one single form:
//
//   * RCWSCLRL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCLRL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRL")
}

// RCWSCLRP instruction have one single form:
//
//   * RCWSCLRP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSCLRP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 0, 0, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRP")
}

// RCWSCLRPA instruction have one single form:
//
//   * RCWSCLRPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSCLRPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 1, 0, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRPA")
}

// RCWSCLRPAL instruction have one single form:
//
//   * RCWSCLRPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSCLRPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 1, 1, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRPAL")
}

// RCWSCLRPL instruction have one single form:
//
//   * RCWSCLRPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSCLRPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 0, 1, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRPL")
}

// RCWSET instruction have one single form:
//
//   * RCWSET  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSET", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSET")
}

// RCWSETA instruction have one single form:
//
//   * RCWSETA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSETA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETA")
}

// RCWSETAL instruction have one single form:
//
//   * RCWSETAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSETAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETAL")
}

// RCWSETL instruction have one single form:
//
//   * RCWSETL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSETL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETL")
}

// RCWSETP instruction have one single form:
//
//   * RCWSETP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSETP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 0, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETP")
}

// RCWSETPA instruction have one single form:
//
//   * RCWSETPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSETPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 0, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETPA")
}

// RCWSETPAL instruction have one single form:
//
//   * RCWSETPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSETPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 1, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETPAL")
}

// RCWSETPL instruction have one single form:
//
//   * RCWSETPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSETPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 1, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETPL")
}

// RCWSSET instruction have one single form:
//
//   * RCWSSET  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSET", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSET")
}

// RCWSSETA instruction have one single form:
//
//   * RCWSSETA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSETA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETA")
}

// RCWSSETAL instruction have one single form:
//
//   * RCWSSETAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSETAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETAL")
}

// RCWSSETL instruction have one single form:
//
//   * RCWSSETL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSETL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETL")
}

// RCWSSETP instruction have one single form:
//
//   * RCWSSETP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSETP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 0, 0, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETP")
}

// RCWSSETPA instruction have one single form:
//
//   * RCWSSETPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSETPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 1, 0, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETPA")
}

// RCWSSETPAL instruction have one single form:
//
//   * RCWSSETPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSETPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 1, 1, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETPAL")
}

// RCWSSETPL instruction have one single form:
//
//   * RCWSSETPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSETPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 0, 1, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETPL")
}

// RCWSSWP instruction have one single form:
//
//   * RCWSSWP  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSWP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWP")
}

// RCWSSWPA instruction have one single form:
//
//   * RCWSSWPA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSWPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPA")
}

// RCWSSWPAL instruction have one single form:
//
//   * RCWSSWPAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSWPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPAL")
}

// RCWSSWPL instruction have one single form:
//
//   * RCWSSWPL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSWPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPL")
}

// RCWSSWPP instruction have one single form:
//
//   * RCWSSWPP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSWPP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 0, 0, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPP")
}

// RCWSSWPPA instruction have one single form:
//
//   * RCWSSWPPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSWPPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 1, 0, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPPA")
}

// RCWSSWPPAL instruction have one single form:
//
//   * RCWSSWPPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSWPPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 1, 1, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPPAL")
}

// RCWSSWPPL instruction have one single form:
//
//   * RCWSSWPPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSWPPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 0, 1, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPPL")
}

// RCWSWP instruction have one single form:
//
//   * RCWSWP  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSWP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWP")
}

// RCWSWPA instruction have one single form:
//
//   * RCWSWPA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSWPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPA")
}

// RCWSWPAL instruction have one single form:
//
//   * RCWSWPAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSWPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPAL")
}

// RCWSWPL instruction have one single form:
//
//   * RCWSWPL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSWPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPL")
}

// RCWSWPP instruction have one single form:
//
//   * RCWSWPP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSWPP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 0, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPP")
}

// RCWSWPPA instruction have one single form:
//
//   * RCWSWPPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSWPPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 0, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPPA")
}

// RCWSWPPAL instruction have one single form:
//
//   * RCWSWPPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSWPPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 1, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPPAL")
}

// RCWSWPPL instruction have one single form:
//
//   * RCWSWPPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSWPPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 1, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPPL")
}

// RET instruction have one single form:
//
//   * RET  {<Xn>}
//
func (self *Program) RET(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("RET", 0, Operands {})
        case 1  : p = self.alloc("RET", 1, Operands { vv[0] })
        default : panic("instruction RET takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isXr(vv[0])) {
        var sa_xn uint32
        if len(vv) == 1 {
            sa_xn = uint32(vv[0].(asm.Register).ID())
        }
        return p.setins(branch_reg(2, 31, 0, sa_xn, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RET")
}

// RETAA instruction have one single form:
//
//   * RETAA
//
func (self *Program) RETAA() *Instruction {
    p := self.alloc("RETAA", 0, Operands {})
    return p.setins(branch_reg(2, 31, 2, 31, 31))
}

// RETAB instruction have one single form:
//
//   * RETAB
//
func (self *Program) RETAB() *Instruction {
    p := self.alloc("RETAB", 0, Operands {})
    return p.setins(branch_reg(2, 31, 3, 31, 31))
}

// REV instruction have 2 forms:
//
//   * REV  <Wd>, <Wn>
//   * REV  <Xd>, <Xn>
//
func (self *Program) REV(v0, v1 interface{}) *Instruction {
    p := self.alloc("REV", 2, Operands { v0, v1 })
    // REV  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 2, sa_wn, sa_wd))
    }
    // REV  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 3, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for REV")
}

// REV16 instruction have 3 forms:
//
//   * REV16  <Wd>, <Wn>
//   * REV16  <Xd>, <Xn>
//   * REV16  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) REV16(v0, v1 interface{}) *Instruction {
    p := self.alloc("REV16", 2, Operands { v0, v1 })
    // REV16  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 1, sa_wn, sa_wd))
    }
    // REV16  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 1, sa_xn, sa_xd))
    }
    // REV16  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec8B, Vec16B) && isVr(v1) && isVfmt(v1, Vec8B, Vec16B) && vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 0, maskp(sa_t, 1, 2), 1, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for REV16")
}

// REV32 instruction have 2 forms:
//
//   * REV32  <Xd>, <Xn>
//   * REV32  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) REV32(v0, v1 interface{}) *Instruction {
    p := self.alloc("REV32", 2, Operands { v0, v1 })
    // REV32  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 2, sa_xn, sa_xd))
    }
    // REV32  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 1, maskp(sa_t, 1, 2), 0, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for REV32")
}

// REV64 instruction have one single form:
//
//   * REV64  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) REV64(v0, v1 interface{}) *Instruction {
    p := self.alloc("REV64", 2, Operands { v0, v1 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 0, maskp(sa_t, 1, 2), 0, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for REV64")
}

// RMIF instruction have one single form:
//
//   * RMIF  <Xn>, #<shift>, #<mask>
//
func (self *Program) RMIF(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RMIF", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isUimm6(v1) && isUimm4(v2) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_shift := asUimm6(v1)
        sa_mask := asUimm4(v2)
        return p.setins(rmif(1, 0, 1, sa_shift, sa_xn, 0, sa_mask))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RMIF")
}

// RORV instruction have 2 forms:
//
//   * RORV  <Wd>, <Wn>, <Wm>
//   * RORV  <Xd>, <Xn>, <Xm>
//
func (self *Program) RORV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RORV", 3, Operands { v0, v1, v2 })
    // RORV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 11, sa_wn, sa_wd))
    }
    // RORV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 11, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for RORV")
}

// RPRFM instruction have one single form:
//
//   * RPRFM  (<rprfop>|#<imm6>), <Xm>, [<Xn|SP>]
//
func (self *Program) RPRFM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RPRFM", 3, Operands { v0, v1, v2 })
    if isRangePrf(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_rprfop := v0.(RangePrefetchOp).encode()
        sa_xm := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldst_regoff(
            3,
            0,
            2,
            sa_xm,
            mask(sa_rprfop, 3),
            maskp(sa_rprfop, 3, 1),
            sa_xn_sp,
            maskp(sa_rprfop, 4, 5),
        ))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RPRFM")
}

// RSHRN instruction have one single form:
//
//   * RSHRN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) RSHRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RSHRN", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for RSHRN")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for RSHRN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for RSHRN")
        }
        return p.setins(asimdshf(0, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 17, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RSHRN")
}

// RSHRN2 instruction have one single form:
//
//   * RSHRN2  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) RSHRN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RSHRN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for RSHRN2")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for RSHRN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for RSHRN2")
        }
        return p.setins(asimdshf(1, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 17, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RSHRN2")
}

// RSUBHN instruction have one single form:
//
//   * RSUBHN  <Vd>.<Tb>, <Vn>.<Ta>, <Vm>.<Ta>
//
func (self *Program) RSUBHN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RSUBHN", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8H, Vec4S, Vec2D) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for RSUBHN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for RSUBHN")
        }
        return p.setins(asimddiff(0, 1, sa_ta, sa_vm, 6, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RSUBHN")
}

// RSUBHN2 instruction have one single form:
//
//   * RSUBHN2  <Vd>.<Tb>, <Vn>.<Ta>, <Vm>.<Ta>
//
func (self *Program) RSUBHN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RSUBHN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8H, Vec4S, Vec2D) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for RSUBHN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for RSUBHN2")
        }
        return p.setins(asimddiff(1, 1, sa_ta, sa_vm, 6, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RSUBHN2")
}

// SABA instruction have one single form:
//
//   * SABA  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) SABA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SABA", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 15, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SABA")
}

// SABAL instruction have one single form:
//
//   * SABAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) SABAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SABAL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SABAL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SABAL")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 5, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SABAL")
}

// SABAL2 instruction have one single form:
//
//   * SABAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) SABAL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SABAL2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SABAL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SABAL2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 5, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SABAL2")
}

// SABD instruction have one single form:
//
//   * SABD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) SABD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SABD", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 14, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SABD")
}

// SABDL instruction have one single form:
//
//   * SABDL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) SABDL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SABDL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SABDL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SABDL")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 7, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SABDL")
}

// SABDL2 instruction have one single form:
//
//   * SABDL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) SABDL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SABDL2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SABDL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SABDL2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 7, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SABDL2")
}

// SADALP instruction have one single form:
//
//   * SADALP  <Vd>.<Ta>, <Vn>.<Tb>
//
func (self *Program) SADALP(v0, v1 interface{}) *Instruction {
    p := self.alloc("SADALP", 2, Operands { v0, v1 })
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S, Vec1D, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_ta = 0b000
            case Vec8H: sa_ta = 0b001
            case Vec2S: sa_ta = 0b010
            case Vec4S: sa_ta = 0b011
            case Vec1D: sa_ta = 0b100
            case Vec2D: sa_ta = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        if maskp(sa_ta, 1, 2) != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SADALP")
        }
        if sa_ta & 1 != sa_tb & 1 {
            panic("aarch64: invalid combination of operands for SADALP")
        }
        return p.setins(asimdmisc(sa_ta & 1, 0, maskp(sa_ta, 1, 2), 6, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SADALP")
}

// SADDL instruction have one single form:
//
//   * SADDL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) SADDL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SADDL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SADDL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SADDL")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 0, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SADDL")
}

// SADDL2 instruction have one single form:
//
//   * SADDL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) SADDL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SADDL2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SADDL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SADDL2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 0, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SADDL2")
}

// SADDLP instruction have one single form:
//
//   * SADDLP  <Vd>.<Ta>, <Vn>.<Tb>
//
func (self *Program) SADDLP(v0, v1 interface{}) *Instruction {
    p := self.alloc("SADDLP", 2, Operands { v0, v1 })
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S, Vec1D, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_ta = 0b000
            case Vec8H: sa_ta = 0b001
            case Vec2S: sa_ta = 0b010
            case Vec4S: sa_ta = 0b011
            case Vec1D: sa_ta = 0b100
            case Vec2D: sa_ta = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        if maskp(sa_ta, 1, 2) != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SADDLP")
        }
        if sa_ta & 1 != sa_tb & 1 {
            panic("aarch64: invalid combination of operands for SADDLP")
        }
        return p.setins(asimdmisc(sa_ta & 1, 0, maskp(sa_ta, 1, 2), 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SADDLP")
}

// SADDLV instruction have one single form:
//
//   * SADDLV  <V><d>, <Vn>.<T>
//
func (self *Program) SADDLV(v0, v1 interface{}) *Instruction {
    p := self.alloc("SADDLV", 2, Operands { v0, v1 })
    if isAdvSIMD(v0) && isVr(v1) && isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec4S) {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b00
            case SRegister: sa_v = 0b01
            case DRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for SADDLV")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        if maskp(sa_t, 1, 2) != sa_v {
            panic("aarch64: invalid combination of operands for SADDLV")
        }
        return p.setins(asimdall(sa_t & 1, 0, maskp(sa_t, 1, 2), 3, sa_vn, sa_d))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SADDLV")
}

// SADDW instruction have one single form:
//
//   * SADDW  <Vd>.<Ta>, <Vn>.<Ta>, <Vm>.<Tb>
//
func (self *Program) SADDW(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SADDW", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        switch vfmt(v2) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SADDW")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SADDW")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 1, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SADDW")
}

// SADDW2 instruction have one single form:
//
//   * SADDW2  <Vd>.<Ta>, <Vn>.<Ta>, <Vm>.<Tb>
//
func (self *Program) SADDW2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SADDW2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        switch vfmt(v2) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SADDW2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SADDW2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 1, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SADDW2")
}

// SB instruction have one single form:
//
//   * SB
//
func (self *Program) SB() *Instruction {
    p := self.alloc("SB", 0, Operands {})
    return p.setins(barriers(0, 7, 31))
}

// SBC instruction have 2 forms:
//
//   * SBC  <Wd>, <Wn>, <Wm>
//   * SBC  <Xd>, <Xn>, <Xm>
//
func (self *Program) SBC(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SBC", 3, Operands { v0, v1, v2 })
    // SBC  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(0, 1, 0, sa_wm, sa_wn, sa_wd))
    }
    // SBC  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(1, 1, 0, sa_xm, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SBC")
}

// SBCS instruction have 2 forms:
//
//   * SBCS  <Wd>, <Wn>, <Wm>
//   * SBCS  <Xd>, <Xn>, <Xm>
//
func (self *Program) SBCS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SBCS", 3, Operands { v0, v1, v2 })
    // SBCS  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(0, 1, 1, sa_wm, sa_wn, sa_wd))
    }
    // SBCS  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(1, 1, 1, sa_xm, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SBCS")
}

// SBFM instruction have 2 forms:
//
//   * SBFM  <Wd>, <Wn>, #<immr>, #<imms>
//   * SBFM  <Xd>, <Xn>, #<immr>, #<imms>
//
func (self *Program) SBFM(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("SBFM", 4, Operands { v0, v1, v2, v3 })
    // SBFM  <Wd>, <Wn>, #<immr>, #<imms>
    if isWr(v0) && isWr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_immr := asUimm6(v2)
        sa_imms := asUimm6(v3)
        return p.setins(bitfield(0, 0, 0, sa_immr, sa_imms, sa_wn, sa_wd))
    }
    // SBFM  <Xd>, <Xn>, #<immr>, #<imms>
    if isXr(v0) && isXr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_immr_1 := asUimm6(v2)
        sa_imms_1 := asUimm6(v3)
        return p.setins(bitfield(1, 0, 1, sa_immr_1, sa_imms_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SBFM")
}

// SCVTF instruction have 18 forms:
//
//   * SCVTF  <Dd>, <Wn>, #<fbits>
//   * SCVTF  <Dd>, <Wn>
//   * SCVTF  <Dd>, <Xn>, #<fbits>
//   * SCVTF  <Dd>, <Xn>
//   * SCVTF  <Hd>, <Wn>, #<fbits>
//   * SCVTF  <Hd>, <Wn>
//   * SCVTF  <Hd>, <Xn>, #<fbits>
//   * SCVTF  <Hd>, <Xn>
//   * SCVTF  <Sd>, <Wn>, #<fbits>
//   * SCVTF  <Sd>, <Wn>
//   * SCVTF  <Sd>, <Xn>, #<fbits>
//   * SCVTF  <Sd>, <Xn>
//   * SCVTF  <Vd>.<T>, <Vn>.<T>
//   * SCVTF  <Vd>.<T>, <Vn>.<T>
//   * SCVTF  <Vd>.<T>, <Vn>.<T>, #<fbits>
//   * SCVTF  <V><d>, <V><n>
//   * SCVTF  <Hd>, <Hn>
//   * SCVTF  <V><d>, <V><n>, #<fbits>
//
func (self *Program) SCVTF(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("SCVTF", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("SCVTF", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction SCVTF takes 2 or 3 operands")
    }
    // SCVTF  <Dd>, <Wn>, #<fbits>
    if len(vv) == 1 && isDr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 1, 0, 2, sa_fbits, sa_wn, sa_dd))
    }
    // SCVTF  <Dd>, <Wn>
    if isDr(v0) && isWr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 2, sa_wn, sa_dd))
    }
    // SCVTF  <Dd>, <Xn>, #<fbits>
    if len(vv) == 1 && isDr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 1, 0, 2, sa_fbits_1, sa_xn, sa_dd))
    }
    // SCVTF  <Dd>, <Xn>
    if isDr(v0) && isXr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 2, sa_xn, sa_dd))
    }
    // SCVTF  <Hd>, <Wn>, #<fbits>
    if len(vv) == 1 && isHr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 3, 0, 2, sa_fbits, sa_wn, sa_hd))
    }
    // SCVTF  <Hd>, <Wn>
    if isHr(v0) && isWr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 2, sa_wn, sa_hd))
    }
    // SCVTF  <Hd>, <Xn>, #<fbits>
    if len(vv) == 1 && isHr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 3, 0, 2, sa_fbits_1, sa_xn, sa_hd))
    }
    // SCVTF  <Hd>, <Xn>
    if isHr(v0) && isXr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 2, sa_xn, sa_hd))
    }
    // SCVTF  <Sd>, <Wn>, #<fbits>
    if len(vv) == 1 && isSr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 0, 0, 2, sa_fbits, sa_wn, sa_sd))
    }
    // SCVTF  <Sd>, <Wn>
    if isSr(v0) && isWr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 2, sa_wn, sa_sd))
    }
    // SCVTF  <Sd>, <Xn>, #<fbits>
    if len(vv) == 1 && isSr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 0, 0, 2, sa_fbits_1, sa_xn, sa_sd))
    }
    // SCVTF  <Sd>, <Xn>
    if isSr(v0) && isXr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 2, sa_xn, sa_sd))
    }
    // SCVTF  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 29, sa_vn, sa_vd))
    }
    // SCVTF  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 0, 0, 29, sa_vn, sa_vd))
    }
    // SCVTF  <Vd>.<T>, <Vn>.<T>, #<fbits>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(vv[0]) &&
       vfmt(v0) == vfmt(v1) {
        var sa_fbits uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0010: sa_fbits = 32 - uint32(asLit(vv[0]))
            case 0b0100: sa_fbits = 64 - uint32(asLit(vv[0]))
            case 0b1000: sa_fbits = 128 - uint32(asLit(vv[0]))
            default: panic("aarch64: invalid operand 'sa_fbits' for SCVTF")
        }
        if maskp(sa_fbits, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SCVTF")
        }
        return p.setins(asimdshf(sa_t & 1, 0, maskp(sa_fbits, 3, 4), mask(sa_fbits, 3), 28, sa_vn, sa_vd))
    }
    // SCVTF  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for SCVTF")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdmisc(0, size, 29, sa_n, sa_d))
    }
    // SCVTF  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(0, 0, 29, sa_hn, sa_hd))
    }
    // SCVTF  <V><d>, <V><n>, #<fbits>
    if len(vv) == 1 && isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(vv[0]) && isSameType(v0, v1) {
        var sa_fbits_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b0010
            case SRegister: sa_v = 0b0100
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SCVTF")
        }
        switch v0.(type) {
            case HRegister: sa_v__bit_mask = 0b1110
            case SRegister: sa_v__bit_mask = 0b1100
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SCVTF")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b0010: sa_fbits_1 = 32 - uint32(asLit(vv[0]))
            case 0b0100: sa_fbits_1 = 64 - uint32(asLit(vv[0]))
            case 0b1000: sa_fbits_1 = 128 - uint32(asLit(vv[0]))
            default: panic("aarch64: invalid operand 'sa_fbits_1' for SCVTF")
        }
        if maskp(sa_fbits_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for SCVTF")
        }
        return p.setins(asisdshf(0, maskp(sa_fbits_1, 3, 4), mask(sa_fbits_1, 3), 28, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SCVTF")
}

// SDIV instruction have 2 forms:
//
//   * SDIV  <Wd>, <Wn>, <Wm>
//   * SDIV  <Xd>, <Xn>, <Xm>
//
func (self *Program) SDIV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SDIV", 3, Operands { v0, v1, v2 })
    // SDIV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 3, sa_wn, sa_wd))
    }
    // SDIV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 3, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SDIV")
}

// SDOT instruction have 2 forms:
//
//   * SDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.4B[<index>]
//   * SDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) SDOT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SDOT", 3, Operands { v0, v1, v2 })
    // SDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.4B[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVri(v2) &&
       vmoder(v2) == Mode4B {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b0
            case Vec16B: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for SDOT")
        }
        return p.setins(asimdelem(
            sa_ta,
            0,
            2,
            sa_index & 1,
            maskp(sa_vm, 4, 1),
            mask(sa_vm, 4),
            14,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // SDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b0
            case Vec16B: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for SDOT")
        }
        return p.setins(asimdsame2(sa_ta, 0, 2, sa_vm, 2, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SDOT")
}

// SETE instruction have one single form:
//
//   * SETE  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETE", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 8, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETE")
}

// SETEN instruction have one single form:
//
//   * SETEN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETEN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETEN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 10, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETEN")
}

// SETET instruction have one single form:
//
//   * SETET  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETET", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 9, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETET")
}

// SETETN instruction have one single form:
//
//   * SETETN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETETN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETETN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 11, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETETN")
}

// SETF16 instruction have one single form:
//
//   * SETF16  <Wn>
//
func (self *Program) SETF16(v0 interface{}) *Instruction {
    p := self.alloc("SETF16", 1, Operands { v0 })
    if isWr(v0) {
        sa_wn := uint32(v0.(asm.Register).ID())
        return p.setins(setf(0, 0, 1, 0, 1, sa_wn, 0, 13))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETF16")
}

// SETF8 instruction have one single form:
//
//   * SETF8  <Wn>
//
func (self *Program) SETF8(v0 interface{}) *Instruction {
    p := self.alloc("SETF8", 1, Operands { v0 })
    if isWr(v0) {
        sa_wn := uint32(v0.(asm.Register).ID())
        return p.setins(setf(0, 0, 1, 0, 0, sa_wn, 0, 13))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETF8")
}

// SETGE instruction have one single form:
//
//   * SETGE  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGE", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs_1, 8, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGE")
}

// SETGEN instruction have one single form:
//
//   * SETGEN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGEN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGEN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs_1, 10, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGEN")
}

// SETGET instruction have one single form:
//
//   * SETGET  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGET", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs_1, 9, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGET")
}

// SETGETN instruction have one single form:
//
//   * SETGETN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGETN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGETN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs_1, 11, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGETN")
}

// SETGM instruction have one single form:
//
//   * SETGM  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGM", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 4, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGM")
}

// SETGMN instruction have one single form:
//
//   * SETGMN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGMN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGMN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 6, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGMN")
}

// SETGMT instruction have one single form:
//
//   * SETGMT  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGMT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGMT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 5, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGMT")
}

// SETGMTN instruction have one single form:
//
//   * SETGMTN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGMTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGMTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 7, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGMTN")
}

// SETGP instruction have one single form:
//
//   * SETGP  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGP", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 0, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGP")
}

// SETGPN instruction have one single form:
//
//   * SETGPN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGPN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGPN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 2, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGPN")
}

// SETGPT instruction have one single form:
//
//   * SETGPT  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGPT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGPT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 1, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGPT")
}

// SETGPTN instruction have one single form:
//
//   * SETGPTN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGPTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGPTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 3, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGPTN")
}

// SETM instruction have one single form:
//
//   * SETM  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETM", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 4, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETM")
}

// SETMN instruction have one single form:
//
//   * SETMN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETMN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETMN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 6, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETMN")
}

// SETMT instruction have one single form:
//
//   * SETMT  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETMT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETMT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 5, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETMT")
}

// SETMTN instruction have one single form:
//
//   * SETMTN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETMTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETMTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 7, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETMTN")
}

// SETP instruction have one single form:
//
//   * SETP  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETP", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 0, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETP")
}

// SETPN instruction have one single form:
//
//   * SETPN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETPN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETPN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 2, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETPN")
}

// SETPT instruction have one single form:
//
//   * SETPT  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETPT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETPT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 1, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETPT")
}

// SETPTN instruction have one single form:
//
//   * SETPTN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETPTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETPTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 3, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETPTN")
}

// SEV instruction have one single form:
//
//   * SEV
//
func (self *Program) SEV() *Instruction {
    p := self.alloc("SEV", 0, Operands {})
    return p.setins(hints(0, 4))
}

// SEVL instruction have one single form:
//
//   * SEVL
//
func (self *Program) SEVL() *Instruction {
    p := self.alloc("SEVL", 0, Operands {})
    return p.setins(hints(0, 5))
}

// SHA1C instruction have one single form:
//
//   * SHA1C  <Qd>, <Sn>, <Vm>.4S
//
func (self *Program) SHA1C(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHA1C", 3, Operands { v0, v1, v2 })
    if isQr(v0) && isSr(v1) && isVr(v2) && vfmt(v2) == Vec4S {
        sa_qd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha3(0, sa_vm, 0, sa_sn, sa_qd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA1C")
}

// SHA1H instruction have one single form:
//
//   * SHA1H  <Sd>, <Sn>
//
func (self *Program) SHA1H(v0, v1 interface{}) *Instruction {
    p := self.alloc("SHA1H", 2, Operands { v0, v1 })
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(cryptosha2(0, 0, sa_sn, sa_sd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA1H")
}

// SHA1M instruction have one single form:
//
//   * SHA1M  <Qd>, <Sn>, <Vm>.4S
//
func (self *Program) SHA1M(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHA1M", 3, Operands { v0, v1, v2 })
    if isQr(v0) && isSr(v1) && isVr(v2) && vfmt(v2) == Vec4S {
        sa_qd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha3(0, sa_vm, 2, sa_sn, sa_qd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA1M")
}

// SHA1P instruction have one single form:
//
//   * SHA1P  <Qd>, <Sn>, <Vm>.4S
//
func (self *Program) SHA1P(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHA1P", 3, Operands { v0, v1, v2 })
    if isQr(v0) && isSr(v1) && isVr(v2) && vfmt(v2) == Vec4S {
        sa_qd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha3(0, sa_vm, 1, sa_sn, sa_qd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA1P")
}

// SHA1SU0 instruction have one single form:
//
//   * SHA1SU0  <Vd>.4S, <Vn>.4S, <Vm>.4S
//
func (self *Program) SHA1SU0(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHA1SU0", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec4S && isVr(v2) && vfmt(v2) == Vec4S {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha3(0, sa_vm, 3, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA1SU0")
}

// SHA1SU1 instruction have one single form:
//
//   * SHA1SU1  <Vd>.4S, <Vn>.4S
//
func (self *Program) SHA1SU1(v0, v1 interface{}) *Instruction {
    p := self.alloc("SHA1SU1", 2, Operands { v0, v1 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec4S {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(cryptosha2(0, 1, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA1SU1")
}

// SHA256H instruction have one single form:
//
//   * SHA256H  <Qd>, <Qn>, <Vm>.4S
//
func (self *Program) SHA256H(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHA256H", 3, Operands { v0, v1, v2 })
    if isQr(v0) && isQr(v1) && isVr(v2) && vfmt(v2) == Vec4S {
        sa_qd := uint32(v0.(asm.Register).ID())
        sa_qn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha3(0, sa_vm, 4, sa_qn, sa_qd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA256H")
}

// SHA256H2 instruction have one single form:
//
//   * SHA256H2  <Qd>, <Qn>, <Vm>.4S
//
func (self *Program) SHA256H2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHA256H2", 3, Operands { v0, v1, v2 })
    if isQr(v0) && isQr(v1) && isVr(v2) && vfmt(v2) == Vec4S {
        sa_qd := uint32(v0.(asm.Register).ID())
        sa_qn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha3(0, sa_vm, 5, sa_qn, sa_qd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA256H2")
}

// SHA256SU0 instruction have one single form:
//
//   * SHA256SU0  <Vd>.4S, <Vn>.4S
//
func (self *Program) SHA256SU0(v0, v1 interface{}) *Instruction {
    p := self.alloc("SHA256SU0", 2, Operands { v0, v1 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec4S {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(cryptosha2(0, 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA256SU0")
}

// SHA256SU1 instruction have one single form:
//
//   * SHA256SU1  <Vd>.4S, <Vn>.4S, <Vm>.4S
//
func (self *Program) SHA256SU1(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHA256SU1", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec4S && isVr(v2) && vfmt(v2) == Vec4S {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha3(0, sa_vm, 6, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA256SU1")
}

// SHA512H instruction have one single form:
//
//   * SHA512H  <Qd>, <Qn>, <Vm>.2D
//
func (self *Program) SHA512H(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHA512H", 3, Operands { v0, v1, v2 })
    if isQr(v0) && isQr(v1) && isVr(v2) && vfmt(v2) == Vec2D {
        sa_qd := uint32(v0.(asm.Register).ID())
        sa_qn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha512_3(sa_vm, 0, 0, sa_qn, sa_qd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA512H")
}

// SHA512H2 instruction have one single form:
//
//   * SHA512H2  <Qd>, <Qn>, <Vm>.2D
//
func (self *Program) SHA512H2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHA512H2", 3, Operands { v0, v1, v2 })
    if isQr(v0) && isQr(v1) && isVr(v2) && vfmt(v2) == Vec2D {
        sa_qd := uint32(v0.(asm.Register).ID())
        sa_qn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha512_3(sa_vm, 0, 1, sa_qn, sa_qd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA512H2")
}

// SHA512SU0 instruction have one single form:
//
//   * SHA512SU0  <Vd>.2D, <Vn>.2D
//
func (self *Program) SHA512SU0(v0, v1 interface{}) *Instruction {
    p := self.alloc("SHA512SU0", 2, Operands { v0, v1 })
    if isVr(v0) && vfmt(v0) == Vec2D && isVr(v1) && vfmt(v1) == Vec2D {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(cryptosha512_2(0, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA512SU0")
}

// SHA512SU1 instruction have one single form:
//
//   * SHA512SU1  <Vd>.2D, <Vn>.2D, <Vm>.2D
//
func (self *Program) SHA512SU1(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHA512SU1", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec2D && isVr(v1) && vfmt(v1) == Vec2D && isVr(v2) && vfmt(v2) == Vec2D {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha512_3(sa_vm, 0, 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHA512SU1")
}

// SHADD instruction have one single form:
//
//   * SHADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) SHADD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHADD", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 0, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHADD")
}

// SHL instruction have 2 forms:
//
//   * SHL  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * SHL  <V><d>, <V><n>, #<shift>
//
func (self *Program) SHL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHL", 3, Operands { v0, v1, v2 })
    // SHL  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift = uint32(asLit(v2)) + 32
            case 0b1000: sa_shift = uint32(asLit(v2)) + 64
            default: panic("aarch64: invalid operand 'sa_shift' for SHL")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SHL")
        }
        return p.setins(asimdshf(sa_t & 1, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 10, sa_vn, sa_vd))
    }
    // SHL  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SHL")
        }
        switch v0.(type) {
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = uint32(asLit(v2)) + 64
            default: panic("aarch64: invalid operand 'sa_shift_1' for SHL")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for SHL")
        }
        return p.setins(asisdshf(0, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 10, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SHL")
}

// SHLL instruction have one single form:
//
//   * SHLL  <Vd>.<Ta>, <Vn>.<Tb>, #<shift>
//
func (self *Program) SHLL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHLL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isIntLit(v2, 8, 16, 32) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        switch asLit(v2) {
            case 8: sa_shift = 0b00
            case 16: sa_shift = 0b01
            case 32: sa_shift = 0b10
            default: panic("aarch64: invalid operand 'sa_shift' for SHLL")
        }
        if sa_shift != sa_ta || sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SHLL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SHLL")
        }
        return p.setins(asimdmisc(0, 1, sa_shift, 19, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHLL")
}

// SHLL2 instruction have one single form:
//
//   * SHLL2  <Vd>.<Ta>, <Vn>.<Tb>, #<shift>
//
func (self *Program) SHLL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHLL2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isIntLit(v2, 8, 16, 32) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        switch asLit(v2) {
            case 8: sa_shift = 0b00
            case 16: sa_shift = 0b01
            case 32: sa_shift = 0b10
            default: panic("aarch64: invalid operand 'sa_shift' for SHLL2")
        }
        if sa_shift != sa_ta || sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SHLL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SHLL2")
        }
        return p.setins(asimdmisc(1, 1, sa_shift, 19, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHLL2")
}

// SHRN instruction have one single form:
//
//   * SHRN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) SHRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHRN", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SHRN")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SHRN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SHRN")
        }
        return p.setins(asimdshf(0, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 16, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHRN")
}

// SHRN2 instruction have one single form:
//
//   * SHRN2  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) SHRN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHRN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SHRN2")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SHRN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SHRN2")
        }
        return p.setins(asimdshf(1, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 16, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHRN2")
}

// SHSUB instruction have one single form:
//
//   * SHSUB  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) SHSUB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SHSUB", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 4, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SHSUB")
}

// SLI instruction have 2 forms:
//
//   * SLI  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * SLI  <V><d>, <V><n>, #<shift>
//
func (self *Program) SLI(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SLI", 3, Operands { v0, v1, v2 })
    // SLI  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift = uint32(asLit(v2)) + 32
            case 0b1000: sa_shift = uint32(asLit(v2)) + 64
            default: panic("aarch64: invalid operand 'sa_shift' for SLI")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SLI")
        }
        return p.setins(asimdshf(sa_t & 1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 10, sa_vn, sa_vd))
    }
    // SLI  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SLI")
        }
        switch v0.(type) {
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SLI")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = uint32(asLit(v2)) + 64
            default: panic("aarch64: invalid operand 'sa_shift_1' for SLI")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for SLI")
        }
        return p.setins(asisdshf(1, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 10, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SLI")
}

// SM3PARTW1 instruction have one single form:
//
//   * SM3PARTW1  <Vd>.4S, <Vn>.4S, <Vm>.4S
//
func (self *Program) SM3PARTW1(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SM3PARTW1", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec4S && isVr(v2) && vfmt(v2) == Vec4S {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha512_3(sa_vm, 1, 0, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SM3PARTW1")
}

// SM3PARTW2 instruction have one single form:
//
//   * SM3PARTW2  <Vd>.4S, <Vn>.4S, <Vm>.4S
//
func (self *Program) SM3PARTW2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SM3PARTW2", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec4S && isVr(v2) && vfmt(v2) == Vec4S {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha512_3(sa_vm, 1, 1, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SM3PARTW2")
}

// SM3SS1 instruction have one single form:
//
//   * SM3SS1  <Vd>.4S, <Vn>.4S, <Vm>.4S, <Va>.4S
//
func (self *Program) SM3SS1(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("SM3SS1", 4, Operands { v0, v1, v2, v3 })
    if isVr(v0) &&
       vfmt(v0) == Vec4S &&
       isVr(v1) &&
       vfmt(v1) == Vec4S &&
       isVr(v2) &&
       vfmt(v2) == Vec4S &&
       isVr(v3) &&
       vfmt(v3) == Vec4S {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        sa_va := uint32(v3.(asm.Register).ID())
        return p.setins(crypto4(2, sa_vm, sa_va, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SM3SS1")
}

// SM3TT1A instruction have one single form:
//
//   * SM3TT1A  <Vd>.4S, <Vn>.4S, <Vm>.S[<imm2>]
//
func (self *Program) SM3TT1A(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SM3TT1A", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec4S && isVri(v2) && vmoder(v2) == ModeS {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_imm2 := uint32(vidxr(v2))
        return p.setins(crypto3_imm2(sa_vm, sa_imm2, 0, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SM3TT1A")
}

// SM3TT1B instruction have one single form:
//
//   * SM3TT1B  <Vd>.4S, <Vn>.4S, <Vm>.S[<imm2>]
//
func (self *Program) SM3TT1B(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SM3TT1B", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec4S && isVri(v2) && vmoder(v2) == ModeS {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_imm2 := uint32(vidxr(v2))
        return p.setins(crypto3_imm2(sa_vm, sa_imm2, 1, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SM3TT1B")
}

// SM3TT2A instruction have one single form:
//
//   * SM3TT2A  <Vd>.4S, <Vn>.4S, <Vm>.S[<imm2>]
//
func (self *Program) SM3TT2A(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SM3TT2A", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec4S && isVri(v2) && vmoder(v2) == ModeS {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_imm2 := uint32(vidxr(v2))
        return p.setins(crypto3_imm2(sa_vm, sa_imm2, 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SM3TT2A")
}

// SM3TT2B instruction have one single form:
//
//   * SM3TT2B  <Vd>.4S, <Vn>.4S, <Vm>.S[<imm2>]
//
func (self *Program) SM3TT2B(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SM3TT2B", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec4S && isVri(v2) && vmoder(v2) == ModeS {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_imm2 := uint32(vidxr(v2))
        return p.setins(crypto3_imm2(sa_vm, sa_imm2, 3, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SM3TT2B")
}

// SM4E instruction have one single form:
//
//   * SM4E  <Vd>.4S, <Vn>.4S
//
func (self *Program) SM4E(v0, v1 interface{}) *Instruction {
    p := self.alloc("SM4E", 2, Operands { v0, v1 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec4S {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(cryptosha512_2(1, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SM4E")
}

// SM4EKEY instruction have one single form:
//
//   * SM4EKEY  <Vd>.4S, <Vn>.4S, <Vm>.4S
//
func (self *Program) SM4EKEY(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SM4EKEY", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec4S && isVr(v2) && vfmt(v2) == Vec4S {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(cryptosha512_3(sa_vm, 1, 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SM4EKEY")
}

// SMADDL instruction have one single form:
//
//   * SMADDL  <Xd>, <Wn>, <Wm>, <Xa>
//
func (self *Program) SMADDL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("SMADDL", 4, Operands { v0, v1, v2, v3 })
    if isXr(v0) && isWr(v1) && isWr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 1, sa_wm, 0, sa_xa, sa_wn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMADDL")
}

// SMAX instruction have 5 forms:
//
//   * SMAX  <Wd>, <Wn>, <Wm>
//   * SMAX  <Wd>, <Wn>, #<simm>
//   * SMAX  <Xd>, <Xn>, <Xm>
//   * SMAX  <Xd>, <Xn>, #<simm>
//   * SMAX  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) SMAX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMAX", 3, Operands { v0, v1, v2 })
    // SMAX  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 24, sa_wn, sa_wd))
    }
    // SMAX  <Wd>, <Wn>, #<simm>
    if isWr(v0) && isWr(v1) && isFpImm8(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_simm := asFpImm8(v2)
        return p.setins(minmax_imm(0, 0, 0, 0, sa_simm, sa_wn, sa_wd))
    }
    // SMAX  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 24, sa_xn, sa_xd))
    }
    // SMAX  <Xd>, <Xn>, #<simm>
    if isXr(v0) && isXr(v1) && isFpImm8(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_simm := asFpImm8(v2)
        return p.setins(minmax_imm(1, 0, 0, 0, sa_simm, sa_xn, sa_xd))
    }
    // SMAX  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 12, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SMAX")
}

// SMAXP instruction have one single form:
//
//   * SMAXP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) SMAXP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMAXP", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 20, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMAXP")
}

// SMAXV instruction have one single form:
//
//   * SMAXV  <V><d>, <Vn>.<T>
//
func (self *Program) SMAXV(v0, v1 interface{}) *Instruction {
    p := self.alloc("SMAXV", 2, Operands { v0, v1 })
    if isAdvSIMD(v0) && isVr(v1) && isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec4S) {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for SMAXV")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        if maskp(sa_t, 1, 2) != sa_v {
            panic("aarch64: invalid combination of operands for SMAXV")
        }
        return p.setins(asimdall(sa_t & 1, 0, maskp(sa_t, 1, 2), 10, sa_vn, sa_d))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMAXV")
}

// SMC instruction have one single form:
//
//   * SMC  #<imm>
//
func (self *Program) SMC(v0 interface{}) *Instruction {
    p := self.alloc("SMC", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(0, sa_imm, 0, 3))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMC")
}

// SMIN instruction have 5 forms:
//
//   * SMIN  <Wd>, <Wn>, <Wm>
//   * SMIN  <Wd>, <Wn>, #<simm>
//   * SMIN  <Xd>, <Xn>, <Xm>
//   * SMIN  <Xd>, <Xn>, #<simm>
//   * SMIN  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) SMIN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMIN", 3, Operands { v0, v1, v2 })
    // SMIN  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 26, sa_wn, sa_wd))
    }
    // SMIN  <Wd>, <Wn>, #<simm>
    if isWr(v0) && isWr(v1) && isFpImm8(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_simm := asFpImm8(v2)
        return p.setins(minmax_imm(0, 0, 0, 2, sa_simm, sa_wn, sa_wd))
    }
    // SMIN  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 26, sa_xn, sa_xd))
    }
    // SMIN  <Xd>, <Xn>, #<simm>
    if isXr(v0) && isXr(v1) && isFpImm8(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_simm := asFpImm8(v2)
        return p.setins(minmax_imm(1, 0, 0, 2, sa_simm, sa_xn, sa_xd))
    }
    // SMIN  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 13, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SMIN")
}

// SMINP instruction have one single form:
//
//   * SMINP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) SMINP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMINP", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 21, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMINP")
}

// SMINV instruction have one single form:
//
//   * SMINV  <V><d>, <Vn>.<T>
//
func (self *Program) SMINV(v0, v1 interface{}) *Instruction {
    p := self.alloc("SMINV", 2, Operands { v0, v1 })
    if isAdvSIMD(v0) && isVr(v1) && isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec4S) {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for SMINV")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        if maskp(sa_t, 1, 2) != sa_v {
            panic("aarch64: invalid combination of operands for SMINV")
        }
        return p.setins(asimdall(sa_t & 1, 0, maskp(sa_t, 1, 2), 26, sa_vn, sa_d))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMINV")
}

// SMLAL instruction have 2 forms:
//
//   * SMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * SMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) SMLAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMLAL", 3, Operands { v0, v1, v2 })
    // SMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SMLAL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SMLAL")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 8, sa_vn, sa_vd))
    }
    // SMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SMLAL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SMLAL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SMLAL")
        }
        return p.setins(asimdelem(
            0,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            2,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SMLAL")
}

// SMLAL2 instruction have 2 forms:
//
//   * SMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * SMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) SMLAL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMLAL2", 3, Operands { v0, v1, v2 })
    // SMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SMLAL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SMLAL2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 8, sa_vn, sa_vd))
    }
    // SMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SMLAL2")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SMLAL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SMLAL2")
        }
        return p.setins(asimdelem(
            1,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            2,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SMLAL2")
}

// SMLSL instruction have 2 forms:
//
//   * SMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * SMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) SMLSL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMLSL", 3, Operands { v0, v1, v2 })
    // SMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SMLSL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SMLSL")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 10, sa_vn, sa_vd))
    }
    // SMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SMLSL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SMLSL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SMLSL")
        }
        return p.setins(asimdelem(
            0,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            6,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SMLSL")
}

// SMLSL2 instruction have 2 forms:
//
//   * SMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * SMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) SMLSL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMLSL2", 3, Operands { v0, v1, v2 })
    // SMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SMLSL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SMLSL2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 10, sa_vn, sa_vd))
    }
    // SMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SMLSL2")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SMLSL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SMLSL2")
        }
        return p.setins(asimdelem(
            1,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            6,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SMLSL2")
}

// SMMLA instruction have one single form:
//
//   * SMMLA  <Vd>.4S, <Vn>.16B, <Vm>.16B
//
func (self *Program) SMMLA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMMLA", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec16B && isVr(v2) && vfmt(v2) == Vec16B {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame2(1, 0, 2, sa_vm, 4, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMMLA")
}

// SMOV instruction have 2 forms:
//
//   * SMOV  <Wd>, <Vn>.<Ts>[<index>]
//   * SMOV  <Xd>, <Vn>.<Ts>[<index>]
//
func (self *Program) SMOV(v0, v1 interface{}) *Instruction {
    p := self.alloc("SMOV", 2, Operands { v0, v1 })
    // SMOV  <Wd>, <Vn>.<Ts>[<index>]
    if isWr(v0) && isVri(v1) {
        var sa_ts uint32
        var sa_ts__bit_mask uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(VidxRegister).ID())
        switch vmoder(v1) {
            case ModeB: sa_ts = 0b00001
            case ModeH: sa_ts = 0b00010
            default: panic("aarch64: unreachable")
        }
        switch vmoder(v1) {
            case ModeB: sa_ts__bit_mask = 0b00001
            case ModeH: sa_ts__bit_mask = 0b00011
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v1))
        if sa_index != sa_ts & sa_ts__bit_mask {
            panic("aarch64: invalid combination of operands for SMOV")
        }
        return p.setins(asimdins(0, 0, sa_index, 5, sa_vn, sa_wd))
    }
    // SMOV  <Xd>, <Vn>.<Ts>[<index>]
    if isXr(v0) && isVri(v1) {
        var sa_ts_1 uint32
        var sa_ts_1__bit_mask uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(VidxRegister).ID())
        switch vmoder(v1) {
            case ModeB: sa_ts_1 = 0b00001
            case ModeH: sa_ts_1 = 0b00010
            case ModeS: sa_ts_1 = 0b00100
            default: panic("aarch64: unreachable")
        }
        switch vmoder(v1) {
            case ModeB: sa_ts_1__bit_mask = 0b00001
            case ModeH: sa_ts_1__bit_mask = 0b00011
            case ModeS: sa_ts_1__bit_mask = 0b00111
            default: panic("aarch64: unreachable")
        }
        sa_index_1 := uint32(vidxr(v1))
        if sa_index_1 != sa_ts_1 & sa_ts_1__bit_mask {
            panic("aarch64: invalid combination of operands for SMOV")
        }
        return p.setins(asimdins(1, 0, sa_index_1, 5, sa_vn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SMOV")
}

// SMSUBL instruction have one single form:
//
//   * SMSUBL  <Xd>, <Wn>, <Wm>, <Xa>
//
func (self *Program) SMSUBL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("SMSUBL", 4, Operands { v0, v1, v2, v3 })
    if isXr(v0) && isWr(v1) && isWr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 1, sa_wm, 1, sa_xa, sa_wn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMSUBL")
}

// SMULH instruction have one single form:
//
//   * SMULH  <Xd>, <Xn>, <Xm>
//
func (self *Program) SMULH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMULH", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 2, sa_xm, 0, 31, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMULH")
}

// SMULL instruction have 2 forms:
//
//   * SMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * SMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) SMULL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMULL", 3, Operands { v0, v1, v2 })
    // SMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SMULL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SMULL")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 12, sa_vn, sa_vd))
    }
    // SMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SMULL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SMULL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SMULL")
        }
        return p.setins(asimdelem(
            0,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            10,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SMULL")
}

// SMULL2 instruction have 2 forms:
//
//   * SMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * SMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) SMULL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMULL2", 3, Operands { v0, v1, v2 })
    // SMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SMULL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SMULL2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 12, sa_vn, sa_vd))
    }
    // SMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SMULL2")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SMULL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SMULL2")
        }
        return p.setins(asimdelem(
            1,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            10,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SMULL2")
}

// SQABS instruction have 2 forms:
//
//   * SQABS  <Vd>.<T>, <Vn>.<T>
//   * SQABS  <V><d>, <V><n>
//
func (self *Program) SQABS(v0, v1 interface{}) *Instruction {
    p := self.alloc("SQABS", 2, Operands { v0, v1 })
    // SQABS  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 0, maskp(sa_t, 1, 2), 7, sa_vn, sa_vd))
    }
    // SQABS  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for SQABS")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmisc(0, sa_v, 7, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQABS")
}

// SQADD instruction have 2 forms:
//
//   * SQADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * SQADD  <V><d>, <V><n>, <V><m>
//
func (self *Program) SQADD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQADD", 3, Operands { v0, v1, v2 })
    // SQADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 1, sa_vn, sa_vd))
    }
    // SQADD  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for SQADD")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(0, sa_v, sa_m, 1, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQADD")
}

// SQDMLAL instruction have 4 forms:
//
//   * SQDMLAL  <Va><d>, <Vb><n>, <Vb><m>
//   * SQDMLAL  <Va><d>, <Vb><n>, <Vm>.<Ts>[<index>]
//   * SQDMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * SQDMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) SQDMLAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQDMLAL", 3, Operands { v0, v1, v2 })
    // SQDMLAL  <Va><d>, <Vb><n>, <Vb><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v1, v2) {
        var sa_va uint32
        var sa_vb uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_va = 0b01
            case DRegister: sa_va = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMLAL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_vb = 0b01
            case SRegister: sa_vb = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMLAL")
        }
        sa_m := uint32(v2.(asm.Register).ID())
        if sa_va != sa_vb {
            panic("aarch64: invalid combination of operands for SQDMLAL")
        }
        return p.setins(asisddiff(0, sa_va, sa_m, 9, sa_n, sa_d))
    }
    // SQDMLAL  <Va><d>, <Vb><n>, <Vm>.<Ts>[<index>]
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isVri(v2) {
        var sa_ts uint32
        var sa_va uint32
        var sa_vb uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_va = 0b01
            case DRegister: sa_va = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMLAL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_vb = 0b01
            case SRegister: sa_vb = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMLAL")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ts || sa_ts != sa_va || sa_va != sa_vb || sa_vb != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQDMLAL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQDMLAL")
        }
        return p.setins(asisdelem(
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            3,
            maskp(sa_index, 1, 1),
            sa_n,
            sa_d,
        ))
    }
    // SQDMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SQDMLAL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SQDMLAL")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 9, sa_vn, sa_vd))
    }
    // SQDMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQDMLAL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQDMLAL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SQDMLAL")
        }
        return p.setins(asimdelem(
            0,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            3,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQDMLAL")
}

// SQDMLAL2 instruction have 2 forms:
//
//   * SQDMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * SQDMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) SQDMLAL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQDMLAL2", 3, Operands { v0, v1, v2 })
    // SQDMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SQDMLAL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SQDMLAL2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 9, sa_vn, sa_vd))
    }
    // SQDMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQDMLAL2")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQDMLAL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SQDMLAL2")
        }
        return p.setins(asimdelem(
            1,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            3,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQDMLAL2")
}

// SQDMLSL instruction have 4 forms:
//
//   * SQDMLSL  <Va><d>, <Vb><n>, <Vb><m>
//   * SQDMLSL  <Va><d>, <Vb><n>, <Vm>.<Ts>[<index>]
//   * SQDMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * SQDMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) SQDMLSL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQDMLSL", 3, Operands { v0, v1, v2 })
    // SQDMLSL  <Va><d>, <Vb><n>, <Vb><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v1, v2) {
        var sa_va uint32
        var sa_vb uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_va = 0b01
            case DRegister: sa_va = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMLSL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_vb = 0b01
            case SRegister: sa_vb = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMLSL")
        }
        sa_m := uint32(v2.(asm.Register).ID())
        if sa_va != sa_vb {
            panic("aarch64: invalid combination of operands for SQDMLSL")
        }
        return p.setins(asisddiff(0, sa_va, sa_m, 11, sa_n, sa_d))
    }
    // SQDMLSL  <Va><d>, <Vb><n>, <Vm>.<Ts>[<index>]
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isVri(v2) {
        var sa_ts uint32
        var sa_va uint32
        var sa_vb uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_va = 0b01
            case DRegister: sa_va = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMLSL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_vb = 0b01
            case SRegister: sa_vb = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMLSL")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ts || sa_ts != sa_va || sa_va != sa_vb || sa_vb != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQDMLSL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQDMLSL")
        }
        return p.setins(asisdelem(
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            7,
            maskp(sa_index, 1, 1),
            sa_n,
            sa_d,
        ))
    }
    // SQDMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SQDMLSL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SQDMLSL")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 11, sa_vn, sa_vd))
    }
    // SQDMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQDMLSL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQDMLSL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SQDMLSL")
        }
        return p.setins(asimdelem(
            0,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            7,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQDMLSL")
}

// SQDMLSL2 instruction have 2 forms:
//
//   * SQDMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * SQDMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) SQDMLSL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQDMLSL2", 3, Operands { v0, v1, v2 })
    // SQDMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SQDMLSL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SQDMLSL2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 11, sa_vn, sa_vd))
    }
    // SQDMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQDMLSL2")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQDMLSL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SQDMLSL2")
        }
        return p.setins(asimdelem(
            1,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            7,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQDMLSL2")
}

// SQDMULH instruction have 4 forms:
//
//   * SQDMULH  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
//   * SQDMULH  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * SQDMULH  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
//   * SQDMULH  <V><d>, <V><n>, <V><m>
//
func (self *Program) SQDMULH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQDMULH", 3, Operands { v0, v1, v2 })
    // SQDMULH  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVri(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != maskp(sa_t, 1, 2) || maskp(sa_t, 1, 2) != sa_ts || sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQDMULH")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQDMULH")
        }
        return p.setins(asimdelem(
            sa_t & 1,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            12,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // SQDMULH  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 22, sa_vn, sa_vd))
    }
    // SQDMULH  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isVri(v2) && isSameType(v0, v1) {
        var sa_ts uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMULH")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ts || sa_ts != sa_v || sa_v != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQDMULH")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQDMULH")
        }
        return p.setins(asisdelem(
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            12,
            maskp(sa_index, 1, 1),
            sa_n,
            sa_d,
        ))
    }
    // SQDMULH  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMULH")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(0, sa_v, sa_m, 22, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQDMULH")
}

// SQDMULL instruction have 4 forms:
//
//   * SQDMULL  <Va><d>, <Vb><n>, <Vb><m>
//   * SQDMULL  <Va><d>, <Vb><n>, <Vm>.<Ts>[<index>]
//   * SQDMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * SQDMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) SQDMULL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQDMULL", 3, Operands { v0, v1, v2 })
    // SQDMULL  <Va><d>, <Vb><n>, <Vb><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v1, v2) {
        var sa_va uint32
        var sa_vb uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_va = 0b01
            case DRegister: sa_va = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMULL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_vb = 0b01
            case SRegister: sa_vb = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMULL")
        }
        sa_m := uint32(v2.(asm.Register).ID())
        if sa_va != sa_vb {
            panic("aarch64: invalid combination of operands for SQDMULL")
        }
        return p.setins(asisddiff(0, sa_va, sa_m, 13, sa_n, sa_d))
    }
    // SQDMULL  <Va><d>, <Vb><n>, <Vm>.<Ts>[<index>]
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isVri(v2) {
        var sa_ts uint32
        var sa_va uint32
        var sa_vb uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_va = 0b01
            case DRegister: sa_va = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMULL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_vb = 0b01
            case SRegister: sa_vb = 0b10
            default: panic("aarch64: invalid scalar operand size for SQDMULL")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ts || sa_ts != sa_va || sa_va != sa_vb || sa_vb != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQDMULL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQDMULL")
        }
        return p.setins(asisdelem(
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            11,
            maskp(sa_index, 1, 1),
            sa_n,
            sa_d,
        ))
    }
    // SQDMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SQDMULL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SQDMULL")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 13, sa_vn, sa_vd))
    }
    // SQDMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQDMULL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQDMULL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SQDMULL")
        }
        return p.setins(asimdelem(
            0,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            11,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQDMULL")
}

// SQDMULL2 instruction have 2 forms:
//
//   * SQDMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * SQDMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) SQDMULL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQDMULL2", 3, Operands { v0, v1, v2 })
    // SQDMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SQDMULL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SQDMULL2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 13, sa_vn, sa_vd))
    }
    // SQDMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQDMULL2")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQDMULL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SQDMULL2")
        }
        return p.setins(asimdelem(
            1,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            11,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQDMULL2")
}

// SQNEG instruction have 2 forms:
//
//   * SQNEG  <Vd>.<T>, <Vn>.<T>
//   * SQNEG  <V><d>, <V><n>
//
func (self *Program) SQNEG(v0, v1 interface{}) *Instruction {
    p := self.alloc("SQNEG", 2, Operands { v0, v1 })
    // SQNEG  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 1, maskp(sa_t, 1, 2), 7, sa_vn, sa_vd))
    }
    // SQNEG  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for SQNEG")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmisc(1, sa_v, 7, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQNEG")
}

// SQRDMLAH instruction have 4 forms:
//
//   * SQRDMLAH  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
//   * SQRDMLAH  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * SQRDMLAH  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
//   * SQRDMLAH  <V><d>, <V><n>, <V><m>
//
func (self *Program) SQRDMLAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQRDMLAH", 3, Operands { v0, v1, v2 })
    // SQRDMLAH  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVri(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != maskp(sa_t, 1, 2) || maskp(sa_t, 1, 2) != sa_ts || sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQRDMLAH")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQRDMLAH")
        }
        return p.setins(asimdelem(
            sa_t & 1,
            1,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            13,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // SQRDMLAH  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame2(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 0, sa_vn, sa_vd))
    }
    // SQRDMLAH  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isVri(v2) && isSameType(v0, v1) {
        var sa_ts uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for SQRDMLAH")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ts || sa_ts != sa_v || sa_v != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQRDMLAH")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQRDMLAH")
        }
        return p.setins(asisdelem(
            1,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            13,
            maskp(sa_index, 1, 1),
            sa_n,
            sa_d,
        ))
    }
    // SQRDMLAH  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for SQRDMLAH")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame2(1, sa_v, sa_m, 0, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQRDMLAH")
}

// SQRDMLSH instruction have 4 forms:
//
//   * SQRDMLSH  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
//   * SQRDMLSH  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * SQRDMLSH  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
//   * SQRDMLSH  <V><d>, <V><n>, <V><m>
//
func (self *Program) SQRDMLSH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQRDMLSH", 3, Operands { v0, v1, v2 })
    // SQRDMLSH  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVri(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != maskp(sa_t, 1, 2) || maskp(sa_t, 1, 2) != sa_ts || sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQRDMLSH")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQRDMLSH")
        }
        return p.setins(asimdelem(
            sa_t & 1,
            1,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            15,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // SQRDMLSH  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame2(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 1, sa_vn, sa_vd))
    }
    // SQRDMLSH  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isVri(v2) && isSameType(v0, v1) {
        var sa_ts uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for SQRDMLSH")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ts || sa_ts != sa_v || sa_v != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQRDMLSH")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQRDMLSH")
        }
        return p.setins(asisdelem(
            1,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            15,
            maskp(sa_index, 1, 1),
            sa_n,
            sa_d,
        ))
    }
    // SQRDMLSH  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for SQRDMLSH")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame2(1, sa_v, sa_m, 1, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQRDMLSH")
}

// SQRDMULH instruction have 4 forms:
//
//   * SQRDMULH  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
//   * SQRDMULH  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * SQRDMULH  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
//   * SQRDMULH  <V><d>, <V><n>, <V><m>
//
func (self *Program) SQRDMULH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQRDMULH", 3, Operands { v0, v1, v2 })
    // SQRDMULH  <Vd>.<T>, <Vn>.<T>, <Vm>.<Ts>[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVri(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != maskp(sa_t, 1, 2) || maskp(sa_t, 1, 2) != sa_ts || sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQRDMULH")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQRDMULH")
        }
        return p.setins(asimdelem(
            sa_t & 1,
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            13,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // SQRDMULH  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 22, sa_vn, sa_vd))
    }
    // SQRDMULH  <V><d>, <V><n>, <Vm>.<Ts>[<index>]
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isVri(v2) && isSameType(v0, v1) {
        var sa_ts uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for SQRDMULH")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ts || sa_ts != sa_v || sa_v != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for SQRDMULH")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for SQRDMULH")
        }
        return p.setins(asisdelem(
            0,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            13,
            maskp(sa_index, 1, 1),
            sa_n,
            sa_d,
        ))
    }
    // SQRDMULH  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for SQRDMULH")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(1, sa_v, sa_m, 22, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQRDMULH")
}

// SQRSHL instruction have 2 forms:
//
//   * SQRSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * SQRSHL  <V><d>, <V><n>, <V><m>
//
func (self *Program) SQRSHL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQRSHL", 3, Operands { v0, v1, v2 })
    // SQRSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 11, sa_vn, sa_vd))
    }
    // SQRSHL  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for SQRSHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(0, sa_v, sa_m, 11, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQRSHL")
}

// SQRSHRN instruction have 2 forms:
//
//   * SQRSHRN  <Vb><d>, <Va><n>, #<shift>
//   * SQRSHRN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) SQRSHRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQRSHRN", 3, Operands { v0, v1, v2 })
    // SQRSHRN  <Vb><d>, <Va><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) {
        var sa_shift_1 uint32
        var sa_va uint32
        var sa_va__bit_mask uint32
        var sa_vb uint32
        var sa_vb__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_vb = 0b0001
            case HRegister: sa_vb = 0b0010
            case SRegister: sa_vb = 0b0100
            default: panic("aarch64: invalid scalar operand size for SQRSHRN")
        }
        switch v0.(type) {
            case BRegister: sa_vb__bit_mask = 0b1111
            case HRegister: sa_vb__bit_mask = 0b1110
            case SRegister: sa_vb__bit_mask = 0b1100
            default: panic("aarch64: invalid scalar operand size for SQRSHRN")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_va = 0b0001
            case SRegister: sa_va = 0b0010
            case DRegister: sa_va = 0b0100
            default: panic("aarch64: invalid scalar operand size for SQRSHRN")
        }
        switch v1.(type) {
            case HRegister: sa_va__bit_mask = 0b1111
            case SRegister: sa_va__bit_mask = 0b1110
            case DRegister: sa_va__bit_mask = 0b1100
            default: panic("aarch64: invalid scalar operand size for SQRSHRN")
        }
        switch sa_vb {
            case 0b0001: sa_shift_1 = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift_1 = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift_1 = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for SQRSHRN")
        }
        if maskp(sa_shift_1, 3, 4) != sa_va & sa_va__bit_mask || sa_va & sa_va__bit_mask != sa_vb & sa_vb__bit_mask {
            panic("aarch64: invalid combination of operands for SQRSHRN")
        }
        return p.setins(asisdshf(0, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 19, sa_n, sa_d))
    }
    // SQRSHRN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SQRSHRN")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SQRSHRN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SQRSHRN")
        }
        return p.setins(asimdshf(0, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 19, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQRSHRN")
}

// SQRSHRN2 instruction have one single form:
//
//   * SQRSHRN2  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) SQRSHRN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQRSHRN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SQRSHRN2")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SQRSHRN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SQRSHRN2")
        }
        return p.setins(asimdshf(1, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 19, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SQRSHRN2")
}

// SQRSHRUN instruction have 2 forms:
//
//   * SQRSHRUN  <Vb><d>, <Va><n>, #<shift>
//   * SQRSHRUN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) SQRSHRUN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQRSHRUN", 3, Operands { v0, v1, v2 })
    // SQRSHRUN  <Vb><d>, <Va><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) {
        var sa_shift_1 uint32
        var sa_va uint32
        var sa_va__bit_mask uint32
        var sa_vb uint32
        var sa_vb__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_vb = 0b0001
            case HRegister: sa_vb = 0b0010
            case SRegister: sa_vb = 0b0100
            default: panic("aarch64: invalid scalar operand size for SQRSHRUN")
        }
        switch v0.(type) {
            case BRegister: sa_vb__bit_mask = 0b1111
            case HRegister: sa_vb__bit_mask = 0b1110
            case SRegister: sa_vb__bit_mask = 0b1100
            default: panic("aarch64: invalid scalar operand size for SQRSHRUN")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_va = 0b0001
            case SRegister: sa_va = 0b0010
            case DRegister: sa_va = 0b0100
            default: panic("aarch64: invalid scalar operand size for SQRSHRUN")
        }
        switch v1.(type) {
            case HRegister: sa_va__bit_mask = 0b1111
            case SRegister: sa_va__bit_mask = 0b1110
            case DRegister: sa_va__bit_mask = 0b1100
            default: panic("aarch64: invalid scalar operand size for SQRSHRUN")
        }
        switch sa_vb {
            case 0b0001: sa_shift_1 = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift_1 = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift_1 = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for SQRSHRUN")
        }
        if maskp(sa_shift_1, 3, 4) != sa_va & sa_va__bit_mask || sa_va & sa_va__bit_mask != sa_vb & sa_vb__bit_mask {
            panic("aarch64: invalid combination of operands for SQRSHRUN")
        }
        return p.setins(asisdshf(1, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 17, sa_n, sa_d))
    }
    // SQRSHRUN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SQRSHRUN")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SQRSHRUN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SQRSHRUN")
        }
        return p.setins(asimdshf(0, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 17, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQRSHRUN")
}

// SQRSHRUN2 instruction have one single form:
//
//   * SQRSHRUN2  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) SQRSHRUN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQRSHRUN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SQRSHRUN2")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SQRSHRUN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SQRSHRUN2")
        }
        return p.setins(asimdshf(1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 17, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SQRSHRUN2")
}

// SQSHL instruction have 4 forms:
//
//   * SQSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * SQSHL  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * SQSHL  <V><d>, <V><n>, <V><m>
//   * SQSHL  <V><d>, <V><n>, #<shift>
//
func (self *Program) SQSHL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQSHL", 3, Operands { v0, v1, v2 })
    // SQSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 9, sa_vn, sa_vd))
    }
    // SQSHL  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift = uint32(asLit(v2)) + 32
            case 0b1000: sa_shift = uint32(asLit(v2)) + 64
            default: panic("aarch64: invalid operand 'sa_shift' for SQSHL")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SQSHL")
        }
        return p.setins(asimdshf(sa_t & 1, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 14, sa_vn, sa_vd))
    }
    // SQSHL  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for SQSHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(0, sa_v, sa_m, 9, sa_n, sa_d))
    }
    // SQSHL  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b0001
            case HRegister: sa_v = 0b0010
            case SRegister: sa_v = 0b0100
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SQSHL")
        }
        switch v0.(type) {
            case BRegister: sa_v__bit_mask = 0b1111
            case HRegister: sa_v__bit_mask = 0b1110
            case SRegister: sa_v__bit_mask = 0b1100
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SQSHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b0001: sa_shift_1 = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift_1 = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift_1 = uint32(asLit(v2)) + 32
            case 0b1000: sa_shift_1 = uint32(asLit(v2)) + 64
            default: panic("aarch64: invalid operand 'sa_shift_1' for SQSHL")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for SQSHL")
        }
        return p.setins(asisdshf(0, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 14, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQSHL")
}

// SQSHLU instruction have 2 forms:
//
//   * SQSHLU  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * SQSHLU  <V><d>, <V><n>, #<shift>
//
func (self *Program) SQSHLU(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQSHLU", 3, Operands { v0, v1, v2 })
    // SQSHLU  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift = uint32(asLit(v2)) + 32
            case 0b1000: sa_shift = uint32(asLit(v2)) + 64
            default: panic("aarch64: invalid operand 'sa_shift' for SQSHLU")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SQSHLU")
        }
        return p.setins(asimdshf(sa_t & 1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 12, sa_vn, sa_vd))
    }
    // SQSHLU  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b0001
            case HRegister: sa_v = 0b0010
            case SRegister: sa_v = 0b0100
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SQSHLU")
        }
        switch v0.(type) {
            case BRegister: sa_v__bit_mask = 0b1111
            case HRegister: sa_v__bit_mask = 0b1110
            case SRegister: sa_v__bit_mask = 0b1100
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SQSHLU")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b0001: sa_shift_1 = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift_1 = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift_1 = uint32(asLit(v2)) + 32
            case 0b1000: sa_shift_1 = uint32(asLit(v2)) + 64
            default: panic("aarch64: invalid operand 'sa_shift_1' for SQSHLU")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for SQSHLU")
        }
        return p.setins(asisdshf(1, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 12, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQSHLU")
}

// SQSHRN instruction have 2 forms:
//
//   * SQSHRN  <Vb><d>, <Va><n>, #<shift>
//   * SQSHRN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) SQSHRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQSHRN", 3, Operands { v0, v1, v2 })
    // SQSHRN  <Vb><d>, <Va><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) {
        var sa_shift_1 uint32
        var sa_va uint32
        var sa_va__bit_mask uint32
        var sa_vb uint32
        var sa_vb__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_vb = 0b0001
            case HRegister: sa_vb = 0b0010
            case SRegister: sa_vb = 0b0100
            default: panic("aarch64: invalid scalar operand size for SQSHRN")
        }
        switch v0.(type) {
            case BRegister: sa_vb__bit_mask = 0b1111
            case HRegister: sa_vb__bit_mask = 0b1110
            case SRegister: sa_vb__bit_mask = 0b1100
            default: panic("aarch64: invalid scalar operand size for SQSHRN")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_va = 0b0001
            case SRegister: sa_va = 0b0010
            case DRegister: sa_va = 0b0100
            default: panic("aarch64: invalid scalar operand size for SQSHRN")
        }
        switch v1.(type) {
            case HRegister: sa_va__bit_mask = 0b1111
            case SRegister: sa_va__bit_mask = 0b1110
            case DRegister: sa_va__bit_mask = 0b1100
            default: panic("aarch64: invalid scalar operand size for SQSHRN")
        }
        switch sa_vb {
            case 0b0001: sa_shift_1 = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift_1 = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift_1 = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for SQSHRN")
        }
        if maskp(sa_shift_1, 3, 4) != sa_va & sa_va__bit_mask || sa_va & sa_va__bit_mask != sa_vb & sa_vb__bit_mask {
            panic("aarch64: invalid combination of operands for SQSHRN")
        }
        return p.setins(asisdshf(0, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 18, sa_n, sa_d))
    }
    // SQSHRN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SQSHRN")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SQSHRN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SQSHRN")
        }
        return p.setins(asimdshf(0, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 18, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQSHRN")
}

// SQSHRN2 instruction have one single form:
//
//   * SQSHRN2  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) SQSHRN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQSHRN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SQSHRN2")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SQSHRN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SQSHRN2")
        }
        return p.setins(asimdshf(1, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 18, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SQSHRN2")
}

// SQSHRUN instruction have 2 forms:
//
//   * SQSHRUN  <Vb><d>, <Va><n>, #<shift>
//   * SQSHRUN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) SQSHRUN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQSHRUN", 3, Operands { v0, v1, v2 })
    // SQSHRUN  <Vb><d>, <Va><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) {
        var sa_shift_1 uint32
        var sa_va uint32
        var sa_va__bit_mask uint32
        var sa_vb uint32
        var sa_vb__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_vb = 0b0001
            case HRegister: sa_vb = 0b0010
            case SRegister: sa_vb = 0b0100
            default: panic("aarch64: invalid scalar operand size for SQSHRUN")
        }
        switch v0.(type) {
            case BRegister: sa_vb__bit_mask = 0b1111
            case HRegister: sa_vb__bit_mask = 0b1110
            case SRegister: sa_vb__bit_mask = 0b1100
            default: panic("aarch64: invalid scalar operand size for SQSHRUN")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_va = 0b0001
            case SRegister: sa_va = 0b0010
            case DRegister: sa_va = 0b0100
            default: panic("aarch64: invalid scalar operand size for SQSHRUN")
        }
        switch v1.(type) {
            case HRegister: sa_va__bit_mask = 0b1111
            case SRegister: sa_va__bit_mask = 0b1110
            case DRegister: sa_va__bit_mask = 0b1100
            default: panic("aarch64: invalid scalar operand size for SQSHRUN")
        }
        switch sa_vb {
            case 0b0001: sa_shift_1 = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift_1 = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift_1 = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for SQSHRUN")
        }
        if maskp(sa_shift_1, 3, 4) != sa_va & sa_va__bit_mask || sa_va & sa_va__bit_mask != sa_vb & sa_vb__bit_mask {
            panic("aarch64: invalid combination of operands for SQSHRUN")
        }
        return p.setins(asisdshf(1, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 16, sa_n, sa_d))
    }
    // SQSHRUN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SQSHRUN")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SQSHRUN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SQSHRUN")
        }
        return p.setins(asimdshf(0, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 16, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQSHRUN")
}

// SQSHRUN2 instruction have one single form:
//
//   * SQSHRUN2  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) SQSHRUN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQSHRUN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SQSHRUN2")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SQSHRUN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SQSHRUN2")
        }
        return p.setins(asimdshf(1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 16, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SQSHRUN2")
}

// SQSUB instruction have 2 forms:
//
//   * SQSUB  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * SQSUB  <V><d>, <V><n>, <V><m>
//
func (self *Program) SQSUB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SQSUB", 3, Operands { v0, v1, v2 })
    // SQSUB  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 5, sa_vn, sa_vd))
    }
    // SQSUB  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for SQSUB")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(0, sa_v, sa_m, 5, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQSUB")
}

// SQXTN instruction have 2 forms:
//
//   * SQXTN  <Vb><d>, <Va><n>
//   * SQXTN  <Vd>.<Tb>, <Vn>.<Ta>
//
func (self *Program) SQXTN(v0, v1 interface{}) *Instruction {
    p := self.alloc("SQXTN", 2, Operands { v0, v1 })
    // SQXTN  <Vb><d>, <Va><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) {
        var sa_va uint32
        var sa_vb uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_vb = 0b00
            case HRegister: sa_vb = 0b01
            case SRegister: sa_vb = 0b10
            default: panic("aarch64: invalid scalar operand size for SQXTN")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_va = 0b00
            case SRegister: sa_va = 0b01
            case DRegister: sa_va = 0b10
            default: panic("aarch64: invalid scalar operand size for SQXTN")
        }
        if sa_va != sa_vb {
            panic("aarch64: invalid combination of operands for SQXTN")
        }
        return p.setins(asisdmisc(0, sa_va, 20, sa_n, sa_d))
    }
    // SQXTN  <Vd>.<Tb>, <Vn>.<Ta>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SQXTN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SQXTN")
        }
        return p.setins(asimdmisc(0, 0, sa_ta, 20, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQXTN")
}

// SQXTN2 instruction have one single form:
//
//   * SQXTN2  <Vd>.<Tb>, <Vn>.<Ta>
//
func (self *Program) SQXTN2(v0, v1 interface{}) *Instruction {
    p := self.alloc("SQXTN2", 2, Operands { v0, v1 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SQXTN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SQXTN2")
        }
        return p.setins(asimdmisc(1, 0, sa_ta, 20, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SQXTN2")
}

// SQXTUN instruction have 2 forms:
//
//   * SQXTUN  <Vb><d>, <Va><n>
//   * SQXTUN  <Vd>.<Tb>, <Vn>.<Ta>
//
func (self *Program) SQXTUN(v0, v1 interface{}) *Instruction {
    p := self.alloc("SQXTUN", 2, Operands { v0, v1 })
    // SQXTUN  <Vb><d>, <Va><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) {
        var sa_va uint32
        var sa_vb uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_vb = 0b00
            case HRegister: sa_vb = 0b01
            case SRegister: sa_vb = 0b10
            default: panic("aarch64: invalid scalar operand size for SQXTUN")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_va = 0b00
            case SRegister: sa_va = 0b01
            case DRegister: sa_va = 0b10
            default: panic("aarch64: invalid scalar operand size for SQXTUN")
        }
        if sa_va != sa_vb {
            panic("aarch64: invalid combination of operands for SQXTUN")
        }
        return p.setins(asisdmisc(1, sa_va, 18, sa_n, sa_d))
    }
    // SQXTUN  <Vd>.<Tb>, <Vn>.<Ta>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SQXTUN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SQXTUN")
        }
        return p.setins(asimdmisc(0, 1, sa_ta, 18, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SQXTUN")
}

// SQXTUN2 instruction have one single form:
//
//   * SQXTUN2  <Vd>.<Tb>, <Vn>.<Ta>
//
func (self *Program) SQXTUN2(v0, v1 interface{}) *Instruction {
    p := self.alloc("SQXTUN2", 2, Operands { v0, v1 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SQXTUN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SQXTUN2")
        }
        return p.setins(asimdmisc(1, 1, sa_ta, 18, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SQXTUN2")
}

// SRHADD instruction have one single form:
//
//   * SRHADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) SRHADD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SRHADD", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SRHADD")
}

// SRI instruction have 2 forms:
//
//   * SRI  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * SRI  <V><d>, <V><n>, #<shift>
//
func (self *Program) SRI(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SRI", 3, Operands { v0, v1, v2 })
    // SRI  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            case 0b1000: sa_shift = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SRI")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SRI")
        }
        return p.setins(asimdshf(sa_t & 1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 8, sa_vn, sa_vd))
    }
    // SRI  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SRI")
        }
        switch v0.(type) {
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SRI")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for SRI")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for SRI")
        }
        return p.setins(asisdshf(1, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 8, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SRI")
}

// SRSHL instruction have 2 forms:
//
//   * SRSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * SRSHL  <V><d>, <V><n>, <V><m>
//
func (self *Program) SRSHL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SRSHL", 3, Operands { v0, v1, v2 })
    // SRSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 10, sa_vn, sa_vd))
    }
    // SRSHL  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for SRSHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(0, sa_v, sa_m, 10, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SRSHL")
}

// SRSHR instruction have 2 forms:
//
//   * SRSHR  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * SRSHR  <V><d>, <V><n>, #<shift>
//
func (self *Program) SRSHR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SRSHR", 3, Operands { v0, v1, v2 })
    // SRSHR  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            case 0b1000: sa_shift = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SRSHR")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SRSHR")
        }
        return p.setins(asimdshf(sa_t & 1, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 4, sa_vn, sa_vd))
    }
    // SRSHR  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SRSHR")
        }
        switch v0.(type) {
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SRSHR")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for SRSHR")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for SRSHR")
        }
        return p.setins(asisdshf(0, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 4, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SRSHR")
}

// SRSRA instruction have 2 forms:
//
//   * SRSRA  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * SRSRA  <V><d>, <V><n>, #<shift>
//
func (self *Program) SRSRA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SRSRA", 3, Operands { v0, v1, v2 })
    // SRSRA  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            case 0b1000: sa_shift = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SRSRA")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SRSRA")
        }
        return p.setins(asimdshf(sa_t & 1, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 6, sa_vn, sa_vd))
    }
    // SRSRA  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SRSRA")
        }
        switch v0.(type) {
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SRSRA")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for SRSRA")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for SRSRA")
        }
        return p.setins(asisdshf(0, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 6, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SRSRA")
}

// SSHL instruction have 2 forms:
//
//   * SSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * SSHL  <V><d>, <V><n>, <V><m>
//
func (self *Program) SSHL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSHL", 3, Operands { v0, v1, v2 })
    // SSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 0, maskp(sa_t, 1, 2), sa_vm, 8, sa_vn, sa_vd))
    }
    // SSHL  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for SSHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(0, sa_v, sa_m, 8, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SSHL")
}

// SSHLL instruction have one single form:
//
//   * SSHLL  <Vd>.<Ta>, <Vn>.<Tb>, #<shift>
//
func (self *Program) SSHLL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSHLL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        switch sa_ta {
            case 0b0001: sa_shift = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift = uint32(asLit(v2)) + 32
            default: panic("aarch64: invalid operand 'sa_shift' for SSHLL")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SSHLL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SSHLL")
        }
        return p.setins(asimdshf(0, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 20, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SSHLL")
}

// SSHLL2 instruction have one single form:
//
//   * SSHLL2  <Vd>.<Ta>, <Vn>.<Tb>, #<shift>
//
func (self *Program) SSHLL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSHLL2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        switch sa_ta {
            case 0b0001: sa_shift = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift = uint32(asLit(v2)) + 32
            default: panic("aarch64: invalid operand 'sa_shift' for SSHLL2")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SSHLL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SSHLL2")
        }
        return p.setins(asimdshf(1, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 20, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SSHLL2")
}

// SSHR instruction have 2 forms:
//
//   * SSHR  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * SSHR  <V><d>, <V><n>, #<shift>
//
func (self *Program) SSHR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSHR", 3, Operands { v0, v1, v2 })
    // SSHR  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            case 0b1000: sa_shift = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SSHR")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SSHR")
        }
        return p.setins(asimdshf(sa_t & 1, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 0, sa_vn, sa_vd))
    }
    // SSHR  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SSHR")
        }
        switch v0.(type) {
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SSHR")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for SSHR")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for SSHR")
        }
        return p.setins(asisdshf(0, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 0, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SSHR")
}

// SSRA instruction have 2 forms:
//
//   * SSRA  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * SSRA  <V><d>, <V><n>, #<shift>
//
func (self *Program) SSRA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSRA", 3, Operands { v0, v1, v2 })
    // SSRA  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            case 0b1000: sa_shift = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for SSRA")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for SSRA")
        }
        return p.setins(asimdshf(sa_t & 1, 0, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 2, sa_vn, sa_vd))
    }
    // SSRA  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for SSRA")
        }
        switch v0.(type) {
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for SSRA")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for SSRA")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for SSRA")
        }
        return p.setins(asisdshf(0, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 2, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SSRA")
}

// SSUBL instruction have one single form:
//
//   * SSUBL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) SSUBL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSUBL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SSUBL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SSUBL")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SSUBL")
}

// SSUBL2 instruction have one single form:
//
//   * SSUBL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) SSUBL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSUBL2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SSUBL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SSUBL2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SSUBL2")
}

// SSUBW instruction have one single form:
//
//   * SSUBW  <Vd>.<Ta>, <Vn>.<Ta>, <Vm>.<Tb>
//
func (self *Program) SSUBW(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSUBW", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        switch vfmt(v2) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SSUBW")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SSUBW")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 3, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SSUBW")
}

// SSUBW2 instruction have one single form:
//
//   * SSUBW2  <Vd>.<Ta>, <Vn>.<Ta>, <Vm>.<Tb>
//
func (self *Program) SSUBW2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SSUBW2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        switch vfmt(v2) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SSUBW2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SSUBW2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 3, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SSUBW2")
}

// ST1 instruction have 24 forms:
//
//   * ST1  { <Vt>.<T> }, [<Xn|SP>]
//   * ST1  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>]
//   * ST1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>]
//   * ST1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>]
//   * ST1  { <Vt>.<T> }, [<Xn|SP>], <imm>
//   * ST1  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <imm>
//   * ST1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <imm>
//   * ST1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <imm>
//   * ST1  { <Vt>.<T> }, [<Xn|SP>], <Xm>
//   * ST1  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <Xm>
//   * ST1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <Xm>
//   * ST1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <Xm>
//   * ST1  { <Vt>.B }[<index>], [<Xn|SP>]
//   * ST1  { <Vt>.D }[<index>], [<Xn|SP>]
//   * ST1  { <Vt>.H }[<index>], [<Xn|SP>]
//   * ST1  { <Vt>.S }[<index>], [<Xn|SP>]
//   * ST1  { <Vt>.B }[<index>], [<Xn|SP>], #1
//   * ST1  { <Vt>.B }[<index>], [<Xn|SP>], <Xm>
//   * ST1  { <Vt>.D }[<index>], [<Xn|SP>], #8
//   * ST1  { <Vt>.D }[<index>], [<Xn|SP>], <Xm>
//   * ST1  { <Vt>.H }[<index>], [<Xn|SP>], #2
//   * ST1  { <Vt>.H }[<index>], [<Xn|SP>], <Xm>
//   * ST1  { <Vt>.S }[<index>], [<Xn|SP>], #4
//   * ST1  { <Vt>.S }[<index>], [<Xn|SP>], <Xm>
//
func (self *Program) ST1(v0, v1 interface{}) *Instruction {
    p := self.alloc("ST1", 2, Operands { v0, v1 })
    // ST1  { <Vt>.<T> }, [<Xn|SP>]
    if isVec1(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 0, 7, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>]
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 0, 10, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>]
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 0, 6, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>]
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 0, 2, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.<T> }, [<Xn|SP>], <imm>
    if isVec1(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm := uint32(moffs(v1))
        if sa_imm != sa_t & 1 {
            panic("aarch64: invalid combination of operands for ST1")
        }
        return p.setins(asisdlsep(sa_imm, 0, 31, 7, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <imm>
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm_1 := uint32(moffs(v1))
        if sa_imm_1 != sa_t & 1 {
            panic("aarch64: invalid combination of operands for ST1")
        }
        return p.setins(asisdlsep(sa_imm_1, 0, 31, 10, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <imm>
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm_2 := uint32(moffs(v1))
        if sa_imm_2 != sa_t & 1 {
            panic("aarch64: invalid combination of operands for ST1")
        }
        return p.setins(asisdlsep(sa_imm_2, 0, 31, 6, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <imm>
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm_3 := uint32(moffs(v1))
        if sa_imm_3 != sa_t & 1 {
            panic("aarch64: invalid combination of operands for ST1")
        }
        return p.setins(asisdlsep(sa_imm_3, 0, 31, 2, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.<T> }, [<Xn|SP>], <Xm>
    if isVec1(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 0, sa_xm, 7, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <Xm>
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 0, sa_xm, 10, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <Xm>
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 0, sa_xm, 6, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <Xm>
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec1D: sa_t = 0b110
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST1")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 0, sa_xm, 2, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.B }[<index>], [<Xn|SP>]
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index, 3, 1), 0, 0, 0, 0, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.D }[<index>], [<Xn|SP>]
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_index_1, 0, 0, 0, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.H }[<index>], [<Xn|SP>]
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(
            maskp(sa_index_2, 3, 1),
            0,
            0,
            0,
            2,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // ST1  { <Vt>.S }[<index>], [<Xn|SP>]
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index_3, 1, 1), 0, 0, 0, 4, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.B }[<index>], [<Xn|SP>], #1
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 1 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 0, 0, 31, 0, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.B }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 0, 0, sa_xm, 0, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.D }[<index>], [<Xn|SP>], #8
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 8 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 0, 0, 31, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.D }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 0, 0, sa_xm, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.H }[<index>], [<Xn|SP>], #2
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 2 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            0,
            0,
            31,
            2,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // ST1  { <Vt>.H }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            0,
            0,
            sa_xm,
            2,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // ST1  { <Vt>.S }[<index>], [<Xn|SP>], #4
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 4 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 0, 0, 31, 4, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // ST1  { <Vt>.S }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 0, 0, sa_xm, 4, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ST1")
}

// ST2 instruction have 15 forms:
//
//   * ST2  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>]
//   * ST2  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <imm>
//   * ST2  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <Xm>
//   * ST2  { <Vt>.B, <Vt2>.B }[<index>], [<Xn|SP>]
//   * ST2  { <Vt>.D, <Vt2>.D }[<index>], [<Xn|SP>]
//   * ST2  { <Vt>.H, <Vt2>.H }[<index>], [<Xn|SP>]
//   * ST2  { <Vt>.S, <Vt2>.S }[<index>], [<Xn|SP>]
//   * ST2  { <Vt>.B, <Vt2>.B }[<index>], [<Xn|SP>], #2
//   * ST2  { <Vt>.B, <Vt2>.B }[<index>], [<Xn|SP>], <Xm>
//   * ST2  { <Vt>.D, <Vt2>.D }[<index>], [<Xn|SP>], #16
//   * ST2  { <Vt>.D, <Vt2>.D }[<index>], [<Xn|SP>], <Xm>
//   * ST2  { <Vt>.H, <Vt2>.H }[<index>], [<Xn|SP>], #4
//   * ST2  { <Vt>.H, <Vt2>.H }[<index>], [<Xn|SP>], <Xm>
//   * ST2  { <Vt>.S, <Vt2>.S }[<index>], [<Xn|SP>], #8
//   * ST2  { <Vt>.S, <Vt2>.S }[<index>], [<Xn|SP>], <Xm>
//
func (self *Program) ST2(v0, v1 interface{}) *Instruction {
    p := self.alloc("ST2", 2, Operands { v0, v1 })
    // ST2  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>]
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST2")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 0, 8, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST2  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <imm>
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST2")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm := uint32(moffs(v1))
        if sa_imm != sa_t & 1 {
            panic("aarch64: invalid combination of operands for ST2")
        }
        return p.setins(asisdlsep(sa_imm, 0, 31, 8, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST2  { <Vt>.<T>, <Vt2>.<T> }, [<Xn|SP>], <Xm>
    if isVec2(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST2")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 0, sa_xm, 8, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST2  { <Vt>.B, <Vt2>.B }[<index>], [<Xn|SP>]
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index, 3, 1), 0, 1, 0, 0, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // ST2  { <Vt>.D, <Vt2>.D }[<index>], [<Xn|SP>]
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_index_1, 0, 1, 0, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    // ST2  { <Vt>.H, <Vt2>.H }[<index>], [<Xn|SP>]
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(
            maskp(sa_index_2, 3, 1),
            0,
            1,
            0,
            2,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // ST2  { <Vt>.S, <Vt2>.S }[<index>], [<Xn|SP>]
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index_3, 1, 1), 0, 1, 0, 4, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // ST2  { <Vt>.B, <Vt2>.B }[<index>], [<Xn|SP>], #2
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 2 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 0, 1, 31, 0, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // ST2  { <Vt>.B, <Vt2>.B }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 0, 1, sa_xm, 0, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // ST2  { <Vt>.D, <Vt2>.D }[<index>], [<Xn|SP>], #16
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 16 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 0, 1, 31, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    // ST2  { <Vt>.D, <Vt2>.D }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 0, 1, sa_xm, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    // ST2  { <Vt>.H, <Vt2>.H }[<index>], [<Xn|SP>], #4
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 4 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            0,
            1,
            31,
            2,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // ST2  { <Vt>.H, <Vt2>.H }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            0,
            1,
            sa_xm,
            2,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // ST2  { <Vt>.S, <Vt2>.S }[<index>], [<Xn|SP>], #8
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 8 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 0, 1, 31, 4, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // ST2  { <Vt>.S, <Vt2>.S }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec2(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 0, 1, sa_xm, 4, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ST2")
}

// ST2G instruction have 3 forms:
//
//   * ST2G  <Xt|SP>, [<Xn|SP>{, #<simm>}]
//   * ST2G  <Xt|SP>, [<Xn|SP>], #<simm>
//   * ST2G  <Xt|SP>, [<Xn|SP>, #<simm>]!
//
func (self *Program) ST2G(v0, v1 interface{}) *Instruction {
    p := self.alloc("ST2G", 2, Operands { v0, v1 })
    // ST2G  <Xt|SP>, [<Xn|SP>{, #<simm>}]
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(2, sa_simm, 2, Rn, Rt))
    }
    // ST2G  <Xt|SP>, [<Xn|SP>], #<simm>
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(2, sa_simm, 1, Rn, Rt))
    }
    // ST2G  <Xt|SP>, [<Xn|SP>, #<simm>]!
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(2, sa_simm, 3, Rn, Rt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ST2G")
}

// ST3 instruction have 15 forms:
//
//   * ST3  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>]
//   * ST3  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <imm>
//   * ST3  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <Xm>
//   * ST3  { <Vt>.B, <Vt2>.B, <Vt3>.B }[<index>], [<Xn|SP>]
//   * ST3  { <Vt>.D, <Vt2>.D, <Vt3>.D }[<index>], [<Xn|SP>]
//   * ST3  { <Vt>.H, <Vt2>.H, <Vt3>.H }[<index>], [<Xn|SP>]
//   * ST3  { <Vt>.S, <Vt2>.S, <Vt3>.S }[<index>], [<Xn|SP>]
//   * ST3  { <Vt>.B, <Vt2>.B, <Vt3>.B }[<index>], [<Xn|SP>], #3
//   * ST3  { <Vt>.B, <Vt2>.B, <Vt3>.B }[<index>], [<Xn|SP>], <Xm>
//   * ST3  { <Vt>.D, <Vt2>.D, <Vt3>.D }[<index>], [<Xn|SP>], #24
//   * ST3  { <Vt>.D, <Vt2>.D, <Vt3>.D }[<index>], [<Xn|SP>], <Xm>
//   * ST3  { <Vt>.H, <Vt2>.H, <Vt3>.H }[<index>], [<Xn|SP>], #6
//   * ST3  { <Vt>.H, <Vt2>.H, <Vt3>.H }[<index>], [<Xn|SP>], <Xm>
//   * ST3  { <Vt>.S, <Vt2>.S, <Vt3>.S }[<index>], [<Xn|SP>], #12
//   * ST3  { <Vt>.S, <Vt2>.S, <Vt3>.S }[<index>], [<Xn|SP>], <Xm>
//
func (self *Program) ST3(v0, v1 interface{}) *Instruction {
    p := self.alloc("ST3", 2, Operands { v0, v1 })
    // ST3  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>]
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST3")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 0, 4, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST3  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <imm>
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST3")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm := uint32(moffs(v1))
        if sa_imm != sa_t & 1 {
            panic("aarch64: invalid combination of operands for ST3")
        }
        return p.setins(asisdlsep(sa_imm, 0, 31, 4, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST3  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T> }, [<Xn|SP>], <Xm>
    if isVec3(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST3")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 0, sa_xm, 4, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST3  { <Vt>.B, <Vt2>.B, <Vt3>.B }[<index>], [<Xn|SP>]
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index, 3, 1), 0, 0, 0, 1, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // ST3  { <Vt>.D, <Vt2>.D, <Vt3>.D }[<index>], [<Xn|SP>]
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_index_1, 0, 0, 0, 5, 0, 1, sa_xn_sp, sa_vt))
    }
    // ST3  { <Vt>.H, <Vt2>.H, <Vt3>.H }[<index>], [<Xn|SP>]
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(
            maskp(sa_index_2, 3, 1),
            0,
            0,
            0,
            3,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // ST3  { <Vt>.S, <Vt2>.S, <Vt3>.S }[<index>], [<Xn|SP>]
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index_3, 1, 1), 0, 0, 0, 5, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // ST3  { <Vt>.B, <Vt2>.B, <Vt3>.B }[<index>], [<Xn|SP>], #3
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 3 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 0, 0, 31, 1, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // ST3  { <Vt>.B, <Vt2>.B, <Vt3>.B }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 0, 0, sa_xm, 1, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // ST3  { <Vt>.D, <Vt2>.D, <Vt3>.D }[<index>], [<Xn|SP>], #24
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 24 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 0, 0, 31, 5, 0, 1, sa_xn_sp, sa_vt))
    }
    // ST3  { <Vt>.D, <Vt2>.D, <Vt3>.D }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 0, 0, sa_xm, 5, 0, 1, sa_xn_sp, sa_vt))
    }
    // ST3  { <Vt>.H, <Vt2>.H, <Vt3>.H }[<index>], [<Xn|SP>], #6
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 6 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            0,
            0,
            31,
            3,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // ST3  { <Vt>.H, <Vt2>.H, <Vt3>.H }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            0,
            0,
            sa_xm,
            3,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // ST3  { <Vt>.S, <Vt2>.S, <Vt3>.S }[<index>], [<Xn|SP>], #12
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 12 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 0, 0, 31, 5, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // ST3  { <Vt>.S, <Vt2>.S, <Vt3>.S }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec3(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 0, 0, sa_xm, 5, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ST3")
}

// ST4 instruction have 15 forms:
//
//   * ST4  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>]
//   * ST4  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <imm>
//   * ST4  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <Xm>
//   * ST4  { <Vt>.B, <Vt2>.B, <Vt3>.B, <Vt4>.B }[<index>], [<Xn|SP>]
//   * ST4  { <Vt>.D, <Vt2>.D, <Vt3>.D, <Vt4>.D }[<index>], [<Xn|SP>]
//   * ST4  { <Vt>.H, <Vt2>.H, <Vt3>.H, <Vt4>.H }[<index>], [<Xn|SP>]
//   * ST4  { <Vt>.S, <Vt2>.S, <Vt3>.S, <Vt4>.S }[<index>], [<Xn|SP>]
//   * ST4  { <Vt>.B, <Vt2>.B, <Vt3>.B, <Vt4>.B }[<index>], [<Xn|SP>], #4
//   * ST4  { <Vt>.B, <Vt2>.B, <Vt3>.B, <Vt4>.B }[<index>], [<Xn|SP>], <Xm>
//   * ST4  { <Vt>.D, <Vt2>.D, <Vt3>.D, <Vt4>.D }[<index>], [<Xn|SP>], #32
//   * ST4  { <Vt>.D, <Vt2>.D, <Vt3>.D, <Vt4>.D }[<index>], [<Xn|SP>], <Xm>
//   * ST4  { <Vt>.H, <Vt2>.H, <Vt3>.H, <Vt4>.H }[<index>], [<Xn|SP>], #8
//   * ST4  { <Vt>.H, <Vt2>.H, <Vt3>.H, <Vt4>.H }[<index>], [<Xn|SP>], <Xm>
//   * ST4  { <Vt>.S, <Vt2>.S, <Vt3>.S, <Vt4>.S }[<index>], [<Xn|SP>], #16
//   * ST4  { <Vt>.S, <Vt2>.S, <Vt3>.S, <Vt4>.S }[<index>], [<Xn|SP>], <Xm>
//
func (self *Program) ST4(v0, v1 interface{}) *Instruction {
    p := self.alloc("ST4", 2, Operands { v0, v1 })
    // ST4  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>]
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST4")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlse(sa_t & 1, 0, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST4  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <imm>
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST4")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_imm := uint32(moffs(v1))
        if sa_imm != sa_t & 1 {
            panic("aarch64: invalid combination of operands for ST4")
        }
        return p.setins(asisdlsep(sa_imm, 0, 31, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST4  { <Vt>.<T>, <Vt2>.<T>, <Vt3>.<T>, <Vt4>.<T> }, [<Xn|SP>], <Xm>
    if isVec4(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isXr(midx(v1)) && mext(v1) == PostIndex {
        var sa_t uint32
        sa_vt := uint32(v0.(Vector).ID())
        switch velm(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: invalid vector arrangement for ST4")
        }
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsep(sa_t & 1, 0, sa_xm, 0, maskp(sa_t, 1, 2), sa_xn_sp, sa_vt))
    }
    // ST4  { <Vt>.B, <Vt2>.B, <Vt3>.B, <Vt4>.B }[<index>], [<Xn|SP>]
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index, 3, 1), 0, 1, 0, 1, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // ST4  { <Vt>.D, <Vt2>.D, <Vt3>.D, <Vt4>.D }[<index>], [<Xn|SP>]
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_index_1, 0, 1, 0, 5, 0, 1, sa_xn_sp, sa_vt))
    }
    // ST4  { <Vt>.H, <Vt2>.H, <Vt3>.H, <Vt4>.H }[<index>], [<Xn|SP>]
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(
            maskp(sa_index_2, 3, 1),
            0,
            1,
            0,
            3,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // ST4  { <Vt>.S, <Vt2>.S, <Vt3>.S, <Vt4>.S }[<index>], [<Xn|SP>]
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(maskp(sa_index_3, 1, 1), 0, 1, 0, 5, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // ST4  { <Vt>.B, <Vt2>.B, <Vt3>.B, <Vt4>.B }[<index>], [<Xn|SP>], #4
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 4 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 0, 1, 31, 1, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // ST4  { <Vt>.B, <Vt2>.B, <Vt3>.B, <Vt4>.B }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeB &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index, 3, 1), 0, 1, sa_xm, 1, maskp(sa_index, 2, 1), sa_index & 3, sa_xn_sp, sa_vt))
    }
    // ST4  { <Vt>.D, <Vt2>.D, <Vt3>.D, <Vt4>.D }[<index>], [<Xn|SP>], #32
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 32 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 0, 1, 31, 5, 0, 1, sa_xn_sp, sa_vt))
    }
    // ST4  { <Vt>.D, <Vt2>.D, <Vt3>.D, <Vt4>.D }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_1 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(sa_index_1, 0, 1, sa_xm, 5, 0, 1, sa_xn_sp, sa_vt))
    }
    // ST4  { <Vt>.H, <Vt2>.H, <Vt3>.H, <Vt4>.H }[<index>], [<Xn|SP>], #8
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 8 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            0,
            1,
            31,
            3,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // ST4  { <Vt>.H, <Vt2>.H, <Vt3>.H, <Vt4>.H }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeH &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_2 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(
            maskp(sa_index_2, 3, 1),
            0,
            1,
            sa_xm,
            3,
            maskp(sa_index_2, 2, 1),
            sa_index_2 & 3,
            sa_xn_sp,
            sa_vt,
        ))
    }
    // ST4  { <Vt>.S, <Vt2>.S, <Vt3>.S, <Vt4>.S }[<index>], [<Xn|SP>], #16
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 16 &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 0, 1, 31, 5, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // ST4  { <Vt>.S, <Vt2>.S, <Vt3>.S, <Vt4>.S }[<index>], [<Xn|SP>], <Xm>
    if isIdxVec4(v0) &&
       vmodei(v0) == ModeS &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       mext(v1) == PostIndex {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index_3 := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        return p.setins(asisdlsop(maskp(sa_index_3, 1, 1), 0, 1, sa_xm, 5, sa_index_3 & 1, 0, sa_xn_sp, sa_vt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ST4")
}

// ST64B instruction have one single form:
//
//   * ST64B  <Xt>, [<Xn|SP> {,#0}]
//
func (self *Program) ST64B(v0, v1 interface{}) *Instruction {
    p := self.alloc("ST64B", 2, Operands { v0, v1 })
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(memop(3, 0, 0, 0, 31, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ST64B")
}

// ST64BV instruction have one single form:
//
//   * ST64BV  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) ST64BV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ST64BV", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ST64BV")
}

// ST64BV0 instruction have one single form:
//
//   * ST64BV0  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) ST64BV0(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ST64BV0", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ST64BV0")
}

// STG instruction have 3 forms:
//
//   * STG  <Xt|SP>, [<Xn|SP>{, #<simm>}]
//   * STG  <Xt|SP>, [<Xn|SP>], #<simm>
//   * STG  <Xt|SP>, [<Xn|SP>, #<simm>]!
//
func (self *Program) STG(v0, v1 interface{}) *Instruction {
    p := self.alloc("STG", 2, Operands { v0, v1 })
    // STG  <Xt|SP>, [<Xn|SP>{, #<simm>}]
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(0, sa_simm, 2, Rn, Rt))
    }
    // STG  <Xt|SP>, [<Xn|SP>], #<simm>
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(0, sa_simm, 1, Rn, Rt))
    }
    // STG  <Xt|SP>, [<Xn|SP>, #<simm>]!
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(0, sa_simm, 3, Rn, Rt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STG")
}

// STGM instruction have one single form:
//
//   * STGM  <Xt>, [<Xn|SP>]
//
func (self *Program) STGM(v0, v1 interface{}) *Instruction {
    p := self.alloc("STGM", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt
        return p.setins(ldsttags(2, 0, 0, Rn, Rt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STGM")
}

// STGP instruction have 3 forms:
//
//   * STGP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * STGP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
//   * STGP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
//
func (self *Program) STGP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STGP", 3, Operands { v0, v1, v2 })
    // STGP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        imm7 := uint32(0b0000000)
        imm7 |= sa_imm
        Rt2 := uint32(0b00000)
        Rt2 |= sa_xt2
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt1
        return p.setins(ldstpair_off(1, 0, 0, imm7, Rt2, Rn, Rt))
    }
    // STGP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        imm7 := uint32(0b0000000)
        imm7 |= sa_imm_1
        Rt2 := uint32(0b00000)
        Rt2 |= sa_xt2
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt1
        return p.setins(ldstpair_post(1, 0, 0, imm7, Rt2, Rn, Rt))
    }
    // STGP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        imm7 := uint32(0b0000000)
        imm7 |= sa_imm_1
        Rt2 := uint32(0b00000)
        Rt2 |= sa_xt2
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt1
        return p.setins(ldstpair_pre(1, 0, 0, imm7, Rt2, Rn, Rt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STGP")
}

// STILP instruction have 4 forms:
//
//   * STILP  <Wt1>, <Wt2>, [<Xn|SP>, #-8]!
//   * STILP  <Wt1>, <Wt2>, [<Xn|SP>]
//   * STILP  <Xt1>, <Xt2>, [<Xn|SP>, #-16]!
//   * STILP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) STILP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STILP", 3, Operands { v0, v1, v2 })
    // STILP  <Wt1>, <Wt2>, [<Xn|SP>, #-8]!
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == -8 &&
       mext(v2) == PreIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(2, 0, sa_wt2, 0, sa_xn_sp, sa_wt1))
    }
    // STILP  <Wt1>, <Wt2>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(2, 0, sa_wt2, 1, sa_xn_sp, sa_wt1))
    }
    // STILP  <Xt1>, <Xt2>, [<Xn|SP>, #-16]!
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == -16 &&
       mext(v2) == PreIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(3, 0, sa_xt2, 0, sa_xn_sp, sa_xt1))
    }
    // STILP  <Xt1>, <Xt2>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(3, 0, sa_xt2, 1, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STILP")
}

// STL1 instruction have one single form:
//
//   * STL1  { <Vt>.D }[<index>], [<Xn|SP>]
//
func (self *Program) STL1(v0, v1 interface{}) *Instruction {
    p := self.alloc("STL1", 2, Operands { v0, v1 })
    if isIdxVec1(v0) &&
       vmodei(v0) == ModeD &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == nil {
        sa_vt := uint32(v0.(IndexedVector).ID())
        sa_index := uint32(v0.(IndexedVector).Index())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(asisdlso(sa_index, 0, 0, 1, 4, 0, 1, sa_xn_sp, sa_vt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STL1")
}

// STLLR instruction have 2 forms:
//
//   * STLLR  <Wt>, [<Xn|SP>{,#0}]
//   * STLLR  <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLLR(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLLR", 2, Operands { v0, v1 })
    // STLLR  <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(2, 0, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    // STLLR  <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(3, 0, 31, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STLLR")
}

// STLLRB instruction have one single form:
//
//   * STLLRB  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLLRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLLRB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(0, 0, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLLRB")
}

// STLLRH instruction have one single form:
//
//   * STLLRH  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLLRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLLRH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(1, 0, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLLRH")
}

// STLR instruction have 4 forms:
//
//   * STLR  <Wt>, [<Xn|SP>, #-4]!
//   * STLR  <Xt>, [<Xn|SP>, #-8]!
//   * STLR  <Wt>, [<Xn|SP>{,#0}]
//   * STLR  <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLR(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLR", 2, Operands { v0, v1 })
    // STLR  <Wt>, [<Xn|SP>, #-4]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == -4 && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldapstl_writeback(2, 0, sa_xn_sp, sa_wt))
    }
    // STLR  <Xt>, [<Xn|SP>, #-8]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == -8 && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldapstl_writeback(3, 0, sa_xn_sp, sa_xt))
    }
    // STLR  <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(2, 0, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    // STLR  <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(3, 0, 31, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STLR")
}

// STLRB instruction have one single form:
//
//   * STLRB  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLRB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(0, 0, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLRB")
}

// STLRH instruction have one single form:
//
//   * STLRH  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLRH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(1, 0, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLRH")
}

// STLUR instruction have 7 forms:
//
//   * STLUR  <Wt>, [<Xn|SP>{, #<simm>}]
//   * STLUR  <Xt>, [<Xn|SP>{, #<simm>}]
//   * STLUR  <Bt>, [<Xn|SP>{, #<simm>}]
//   * STLUR  <Dt>, [<Xn|SP>{, #<simm>}]
//   * STLUR  <Ht>, [<Xn|SP>{, #<simm>}]
//   * STLUR  <Qt>, [<Xn|SP>{, #<simm>}]
//   * STLUR  <St>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STLUR(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLUR", 2, Operands { v0, v1 })
    // STLUR  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(2, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STLUR  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(3, 0, sa_simm, sa_xn_sp, sa_xt))
    }
    // STLUR  <Bt>, [<Xn|SP>{, #<simm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(0, 0, sa_simm, sa_xn_sp, sa_bt))
    }
    // STLUR  <Dt>, [<Xn|SP>{, #<simm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(3, 0, sa_simm, sa_xn_sp, sa_dt))
    }
    // STLUR  <Ht>, [<Xn|SP>{, #<simm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(1, 0, sa_simm, sa_xn_sp, sa_ht))
    }
    // STLUR  <Qt>, [<Xn|SP>{, #<simm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(0, 2, sa_simm, sa_xn_sp, sa_qt))
    }
    // STLUR  <St>, [<Xn|SP>{, #<simm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(2, 0, sa_simm, sa_xn_sp, sa_st))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STLUR")
}

// STLURB instruction have one single form:
//
//   * STLURB  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STLURB(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLURB", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLURB")
}

// STLURH instruction have one single form:
//
//   * STLURH  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STLURH(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLURH", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(1, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLURH")
}

// STLXP instruction have 2 forms:
//
//   * STLXP  <Ws>, <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
//   * STLXP  <Ws>, <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
//
func (self *Program) STLXP(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("STLXP", 4, Operands { v0, v1, v2, v3 })
    // STLXP  <Ws>, <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isWr(v2) &&
       isMem(v3) &&
       isXrOrSP(mbase(v3)) &&
       midx(v3) == nil &&
       (moffs(v3) == 0 || moffs(v3) == 0) &&
       mext(v3) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt1 := uint32(v1.(asm.Register).ID())
        sa_wt2 := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v3).ID())
        return p.setins(ldstexclp(0, 0, sa_ws, 1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // STLXP  <Ws>, <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isXr(v1) &&
       isXr(v2) &&
       isMem(v3) &&
       isXrOrSP(mbase(v3)) &&
       midx(v3) == nil &&
       (moffs(v3) == 0 || moffs(v3) == 0) &&
       mext(v3) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_xt1 := uint32(v1.(asm.Register).ID())
        sa_xt2 := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v3).ID())
        return p.setins(ldstexclp(1, 0, sa_ws, 1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STLXP")
}

// STLXR instruction have 2 forms:
//
//   * STLXR  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * STLXR  <Ws>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLXR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STLXR", 3, Operands { v0, v1, v2 })
    // STLXR  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(2, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    // STLXR  <Ws>, <Xt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(3, 0, sa_ws, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STLXR")
}

// STLXRB instruction have one single form:
//
//   * STLXRB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLXRB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STLXRB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(0, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLXRB")
}

// STLXRH instruction have one single form:
//
//   * STLXRH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLXRH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STLXRH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(1, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLXRH")
}

// STNP instruction have 5 forms:
//
//   * STNP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
//   * STNP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * STNP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
//   * STNP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
//   * STNP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
//
func (self *Program) STNP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STNP", 3, Operands { v0, v1, v2 })
    // STNP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(0, 0, 0, sa_imm, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // STNP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(2, 0, 0, sa_imm_1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // STNP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(1, 1, 0, sa_imm, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // STNP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(2, 1, 0, sa_imm_1, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // STNP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(0, 1, 0, sa_imm_2, sa_st2, sa_xn_sp, sa_st1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STNP")
}

// STP instruction have 15 forms:
//
//   * STP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
//   * STP  <Wt1>, <Wt2>, [<Xn|SP>], #<imm>
//   * STP  <Wt1>, <Wt2>, [<Xn|SP>, #<imm>]!
//   * STP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * STP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
//   * STP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
//   * STP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
//   * STP  <Dt1>, <Dt2>, [<Xn|SP>], #<imm>
//   * STP  <Dt1>, <Dt2>, [<Xn|SP>, #<imm>]!
//   * STP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
//   * STP  <Qt1>, <Qt2>, [<Xn|SP>], #<imm>
//   * STP  <Qt1>, <Qt2>, [<Xn|SP>, #<imm>]!
//   * STP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
//   * STP  <St1>, <St2>, [<Xn|SP>], #<imm>
//   * STP  <St1>, <St2>, [<Xn|SP>, #<imm>]!
//
func (self *Program) STP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STP", 3, Operands { v0, v1, v2 })
    // STP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(0, 0, 0, sa_imm, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // STP  <Wt1>, <Wt2>, [<Xn|SP>], #<imm>
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(0, 0, 0, sa_imm_1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // STP  <Wt1>, <Wt2>, [<Xn|SP>, #<imm>]!
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(0, 0, 0, sa_imm_1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // STP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstpair_off(2, 0, 0, sa_imm_2, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // STP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_post(2, 0, 0, sa_imm_3, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // STP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(2, 0, 0, sa_imm_3, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // STP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(1, 1, 0, sa_imm, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // STP  <Dt1>, <Dt2>, [<Xn|SP>], #<imm>
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(1, 1, 0, sa_imm_1, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // STP  <Dt1>, <Dt2>, [<Xn|SP>, #<imm>]!
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(1, 1, 0, sa_imm_1, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // STP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstpair_off(2, 1, 0, sa_imm_2, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // STP  <Qt1>, <Qt2>, [<Xn|SP>], #<imm>
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_post(2, 1, 0, sa_imm_3, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // STP  <Qt1>, <Qt2>, [<Xn|SP>, #<imm>]!
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(2, 1, 0, sa_imm_3, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // STP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_4 := uint32(moffs(v2))
        return p.setins(ldstpair_off(0, 1, 0, sa_imm_4, sa_st2, sa_xn_sp, sa_st1))
    }
    // STP  <St1>, <St2>, [<Xn|SP>], #<imm>
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_5 := uint32(moffs(v2))
        return p.setins(ldstpair_post(0, 1, 0, sa_imm_5, sa_st2, sa_xn_sp, sa_st1))
    }
    // STP  <St1>, <St2>, [<Xn|SP>, #<imm>]!
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_5 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(0, 1, 0, sa_imm_5, sa_st2, sa_xn_sp, sa_st1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STP")
}

// STR instruction have 29 forms:
//
//   * STR  <Wt>, [<Xn|SP>], #<simm>
//   * STR  <Wt>, [<Xn|SP>, #<simm>]!
//   * STR  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * STR  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * STR  <Xt>, [<Xn|SP>], #<simm>
//   * STR  <Xt>, [<Xn|SP>, #<simm>]!
//   * STR  <Xt>, [<Xn|SP>{, #<pimm>}]
//   * STR  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * STR  <Bt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * STR  <Bt>, [<Xn|SP>], #<simm>
//   * STR  <Bt>, [<Xn|SP>, #<simm>]!
//   * STR  <Bt>, [<Xn|SP>{, #<pimm>}]
//   * STR  <Bt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * STR  <Dt>, [<Xn|SP>], #<simm>
//   * STR  <Dt>, [<Xn|SP>, #<simm>]!
//   * STR  <Dt>, [<Xn|SP>{, #<pimm>}]
//   * STR  <Dt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * STR  <Ht>, [<Xn|SP>], #<simm>
//   * STR  <Ht>, [<Xn|SP>, #<simm>]!
//   * STR  <Ht>, [<Xn|SP>{, #<pimm>}]
//   * STR  <Ht>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * STR  <Qt>, [<Xn|SP>], #<simm>
//   * STR  <Qt>, [<Xn|SP>, #<simm>]!
//   * STR  <Qt>, [<Xn|SP>{, #<pimm>}]
//   * STR  <Qt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * STR  <St>, [<Xn|SP>], #<simm>
//   * STR  <St>, [<Xn|SP>, #<simm>]!
//   * STR  <St>, [<Xn|SP>{, #<pimm>}]
//   * STR  <St>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//
func (self *Program) STR(v0, v1 interface{}) *Instruction {
    p := self.alloc("STR", 2, Operands { v0, v1 })
    // STR  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STR  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STR  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 0, 0, sa_pimm, sa_xn_sp, sa_wt))
    }
    // STR  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 0, 0, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // STR  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(3, 0, 0, sa_simm, sa_xn_sp, sa_xt))
    }
    // STR  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(3, 0, 0, sa_simm, sa_xn_sp, sa_xt))
    }
    // STR  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_1 := uint32(moffs(v1))
        return p.setins(ldst_pos(3, 0, 0, sa_pimm_1, sa_xn_sp, sa_xt))
    }
    // STR  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_1 uint32
        var sa_extend uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount_1 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(3, 0, 0, sa_xm, sa_extend, sa_amount_1, sa_xn_sp, sa_xt))
    }
    // STR  <Bt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isBr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 1, 0, sa_xm, 3, sa_amount, sa_xn_sp, sa_bt))
    }
    // STR  <Bt>, [<Xn|SP>], #<simm>
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 1, 0, sa_simm, sa_xn_sp, sa_bt))
    }
    // STR  <Bt>, [<Xn|SP>, #<simm>]!
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 1, 0, sa_simm, sa_xn_sp, sa_bt))
    }
    // STR  <Bt>, [<Xn|SP>{, #<pimm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 1, 0, sa_pimm, sa_xn_sp, sa_bt))
    }
    // STR  <Bt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 1, 0, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_bt))
    }
    // STR  <Dt>, [<Xn|SP>], #<simm>
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(3, 1, 0, sa_simm, sa_xn_sp, sa_dt))
    }
    // STR  <Dt>, [<Xn|SP>, #<simm>]!
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(3, 1, 0, sa_simm, sa_xn_sp, sa_dt))
    }
    // STR  <Dt>, [<Xn|SP>{, #<pimm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_1 := uint32(moffs(v1))
        return p.setins(ldst_pos(3, 1, 0, sa_pimm_1, sa_xn_sp, sa_dt))
    }
    // STR  <Dt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isDr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_1 uint32
        var sa_extend_1 uint32
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_1 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(3, 1, 0, sa_xm, sa_extend_1, sa_amount_1, sa_xn_sp, sa_dt))
    }
    // STR  <Ht>, [<Xn|SP>], #<simm>
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 1, 0, sa_simm, sa_xn_sp, sa_ht))
    }
    // STR  <Ht>, [<Xn|SP>, #<simm>]!
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 1, 0, sa_simm, sa_xn_sp, sa_ht))
    }
    // STR  <Ht>, [<Xn|SP>{, #<pimm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_2 := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 1, 0, sa_pimm_2, sa_xn_sp, sa_ht))
    }
    // STR  <Ht>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isHr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_2 uint32
        var sa_extend_1 uint32
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_2 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 1, 0, sa_xm, sa_extend_1, sa_amount_2, sa_xn_sp, sa_ht))
    }
    // STR  <Qt>, [<Xn|SP>], #<simm>
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 1, 2, sa_simm, sa_xn_sp, sa_qt))
    }
    // STR  <Qt>, [<Xn|SP>, #<simm>]!
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 1, 2, sa_simm, sa_xn_sp, sa_qt))
    }
    // STR  <Qt>, [<Xn|SP>{, #<pimm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_3 := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 1, 2, sa_pimm_3, sa_xn_sp, sa_qt))
    }
    // STR  <Qt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isQr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_3 uint32
        var sa_extend_1 uint32
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_3 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 1, 2, sa_xm, sa_extend_1, sa_amount_3, sa_xn_sp, sa_qt))
    }
    // STR  <St>, [<Xn|SP>], #<simm>
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 1, 0, sa_simm, sa_xn_sp, sa_st))
    }
    // STR  <St>, [<Xn|SP>, #<simm>]!
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 1, 0, sa_simm, sa_xn_sp, sa_st))
    }
    // STR  <St>, [<Xn|SP>{, #<pimm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_4 := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 1, 0, sa_pimm_4, sa_xn_sp, sa_st))
    }
    // STR  <St>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isSr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_4 uint32
        var sa_extend_1 uint32
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_4 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 1, 0, sa_xm, sa_extend_1, sa_amount_4, sa_xn_sp, sa_st))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STR")
}

// STRB instruction have 5 forms:
//
//   * STRB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * STRB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * STRB  <Wt>, [<Xn|SP>], #<simm>
//   * STRB  <Wt>, [<Xn|SP>, #<simm>]!
//   * STRB  <Wt>, [<Xn|SP>{, #<pimm>}]
//
func (self *Program) STRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("STRB", 2, Operands { v0, v1 })
    // STRB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 0, 0, sa_xm, 3, sa_amount, sa_xn_sp, sa_wt))
    }
    // STRB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 0, 0, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // STRB  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STRB  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STRB  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 0, 0, sa_pimm, sa_xn_sp, sa_wt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STRB")
}

// STRH instruction have 4 forms:
//
//   * STRH  <Wt>, [<Xn|SP>], #<simm>
//   * STRH  <Wt>, [<Xn|SP>, #<simm>]!
//   * STRH  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * STRH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//
func (self *Program) STRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("STRH", 2, Operands { v0, v1 })
    // STRH  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STRH  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STRH  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 0, 0, sa_pimm, sa_xn_sp, sa_wt))
    }
    // STRH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 0, 0, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STRH")
}

// STTR instruction have 2 forms:
//
//   * STTR  <Wt>, [<Xn|SP>{, #<simm>}]
//   * STTR  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STTR(v0, v1 interface{}) *Instruction {
    p := self.alloc("STTR", 2, Operands { v0, v1 })
    // STTR  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(2, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STTR  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(3, 0, 0, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STTR")
}

// STTRB instruction have one single form:
//
//   * STTRB  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STTRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("STTRB", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(0, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STTRB")
}

// STTRH instruction have one single form:
//
//   * STTRH  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STTRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("STTRH", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(1, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STTRH")
}

// STUR instruction have 7 forms:
//
//   * STUR  <Wt>, [<Xn|SP>{, #<simm>}]
//   * STUR  <Xt>, [<Xn|SP>{, #<simm>}]
//   * STUR  <Bt>, [<Xn|SP>{, #<simm>}]
//   * STUR  <Dt>, [<Xn|SP>{, #<simm>}]
//   * STUR  <Ht>, [<Xn|SP>{, #<simm>}]
//   * STUR  <Qt>, [<Xn|SP>{, #<simm>}]
//   * STUR  <St>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STUR(v0, v1 interface{}) *Instruction {
    p := self.alloc("STUR", 2, Operands { v0, v1 })
    // STUR  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(2, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STUR  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(3, 0, 0, sa_simm, sa_xn_sp, sa_xt))
    }
    // STUR  <Bt>, [<Xn|SP>{, #<simm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 1, 0, sa_simm, sa_xn_sp, sa_bt))
    }
    // STUR  <Dt>, [<Xn|SP>{, #<simm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(3, 1, 0, sa_simm, sa_xn_sp, sa_dt))
    }
    // STUR  <Ht>, [<Xn|SP>{, #<simm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(1, 1, 0, sa_simm, sa_xn_sp, sa_ht))
    }
    // STUR  <Qt>, [<Xn|SP>{, #<simm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 1, 2, sa_simm, sa_xn_sp, sa_qt))
    }
    // STUR  <St>, [<Xn|SP>{, #<simm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(2, 1, 0, sa_simm, sa_xn_sp, sa_st))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STUR")
}

// STURB instruction have one single form:
//
//   * STURB  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STURB(v0, v1 interface{}) *Instruction {
    p := self.alloc("STURB", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STURB")
}

// STURH instruction have one single form:
//
//   * STURH  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STURH(v0, v1 interface{}) *Instruction {
    p := self.alloc("STURH", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(1, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STURH")
}

// STXP instruction have 2 forms:
//
//   * STXP  <Ws>, <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
//   * STXP  <Ws>, <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
//
func (self *Program) STXP(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("STXP", 4, Operands { v0, v1, v2, v3 })
    // STXP  <Ws>, <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isWr(v2) &&
       isMem(v3) &&
       isXrOrSP(mbase(v3)) &&
       midx(v3) == nil &&
       (moffs(v3) == 0 || moffs(v3) == 0) &&
       mext(v3) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt1 := uint32(v1.(asm.Register).ID())
        sa_wt2 := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v3).ID())
        return p.setins(ldstexclp(0, 0, sa_ws, 0, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // STXP  <Ws>, <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isXr(v1) &&
       isXr(v2) &&
       isMem(v3) &&
       isXrOrSP(mbase(v3)) &&
       midx(v3) == nil &&
       (moffs(v3) == 0 || moffs(v3) == 0) &&
       mext(v3) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_xt1 := uint32(v1.(asm.Register).ID())
        sa_xt2 := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v3).ID())
        return p.setins(ldstexclp(1, 0, sa_ws, 0, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STXP")
}

// STXR instruction have 2 forms:
//
//   * STXR  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * STXR  <Ws>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) STXR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STXR", 3, Operands { v0, v1, v2 })
    // STXR  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(2, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    // STXR  <Ws>, <Xt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(3, 0, sa_ws, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STXR")
}

// STXRB instruction have one single form:
//
//   * STXRB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STXRB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STXRB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(0, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STXRB")
}

// STXRH instruction have one single form:
//
//   * STXRH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STXRH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STXRH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(1, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STXRH")
}

// STZ2G instruction have 3 forms:
//
//   * STZ2G  <Xt|SP>, [<Xn|SP>{, #<simm>}]
//   * STZ2G  <Xt|SP>, [<Xn|SP>], #<simm>
//   * STZ2G  <Xt|SP>, [<Xn|SP>, #<simm>]!
//
func (self *Program) STZ2G(v0, v1 interface{}) *Instruction {
    p := self.alloc("STZ2G", 2, Operands { v0, v1 })
    // STZ2G  <Xt|SP>, [<Xn|SP>{, #<simm>}]
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(3, sa_simm, 2, Rn, Rt))
    }
    // STZ2G  <Xt|SP>, [<Xn|SP>], #<simm>
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(3, sa_simm, 1, Rn, Rt))
    }
    // STZ2G  <Xt|SP>, [<Xn|SP>, #<simm>]!
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(3, sa_simm, 3, Rn, Rt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STZ2G")
}

// STZG instruction have 3 forms:
//
//   * STZG  <Xt|SP>, [<Xn|SP>{, #<simm>}]
//   * STZG  <Xt|SP>, [<Xn|SP>], #<simm>
//   * STZG  <Xt|SP>, [<Xn|SP>, #<simm>]!
//
func (self *Program) STZG(v0, v1 interface{}) *Instruction {
    p := self.alloc("STZG", 2, Operands { v0, v1 })
    // STZG  <Xt|SP>, [<Xn|SP>{, #<simm>}]
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(1, sa_simm, 2, Rn, Rt))
    }
    // STZG  <Xt|SP>, [<Xn|SP>], #<simm>
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(1, sa_simm, 1, Rn, Rt))
    }
    // STZG  <Xt|SP>, [<Xn|SP>, #<simm>]!
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(1, sa_simm, 3, Rn, Rt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STZG")
}

// STZGM instruction have one single form:
//
//   * STZGM  <Xt>, [<Xn|SP>]
//
func (self *Program) STZGM(v0, v1 interface{}) *Instruction {
    p := self.alloc("STZGM", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt
        return p.setins(ldsttags(0, 0, 0, Rn, Rt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STZGM")
}

// SUB instruction have 8 forms:
//
//   * SUB  <Wd|WSP>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
//   * SUB  <Wd|WSP>, <Wn|WSP>, #<imm>{, <shift>}
//   * SUB  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * SUB  <Xd|SP>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
//   * SUB  <Xd|SP>, <Xn|SP>, #<imm>{, <shift>}
//   * SUB  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//   * SUB  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * SUB  <V><d>, <V><n>, <V><m>
//
func (self *Program) SUB(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("SUB", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("SUB", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction SUB takes 3 or 4 operands")
    }
    // SUB  <Wd|WSP>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) &&
       isWrOrWSP(v0) &&
       isWrOrWSP(v1) &&
       isWr(v2) &&
       (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_extend = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_ext(0, 1, 0, 0, sa_wm, sa_extend, sa_amount, sa_wn_wsp, sa_wd_wsp))
    }
    // SUB  <Wd|WSP>, <Wn|WSP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isWrOrWSP(v0) &&
       isWrOrWSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(0, 1, 0, sa_shift, sa_imm, sa_wn_wsp, sa_wd_wsp))
    }
    // SUB  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(0, 1, 0, sa_shift, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // SUB  <Xd|SP>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) &&
       isXrOrSP(v0) &&
       isXrOrSP(v1) &&
       isWrOrXr(v2) &&
       (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend_1 uint32
        var sa_r [4]uint32
        var sa_r__bit_mask [4]uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        switch true {
            case isWr(v2): sa_r = [4]uint32{0b000, 0b010, 0b100, 0b110}
            case isXr(v2): sa_r = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        switch true {
            case isWr(v2): sa_r__bit_mask = [4]uint32{0b110, 0b111, 0b110, 0b111}
            case isXr(v2): sa_r__bit_mask = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        if len(vv) == 1 {
            sa_extend_1 = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        if !matchany(sa_extend_1, &sa_r[0], &sa_r__bit_mask[0], 4) {
            panic("aarch64: invalid combination of operands for SUB")
        }
        return p.setins(addsub_ext(1, 1, 0, 0, sa_m, sa_extend_1, sa_amount, sa_xn_sp, sa_xd_sp))
    }
    // SUB  <Xd|SP>, <Xn|SP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isXrOrSP(v0) &&
       isXrOrSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(1, 1, 0, sa_shift, sa_imm, sa_xn_sp, sa_xd_sp))
    }
    // SUB  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(1, 1, 0, sa_shift, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // SUB  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 16, sa_vn, sa_vd))
    }
    // SUB  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for SUB")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(1, sa_v, sa_m, 16, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SUB")
}

// SUBG instruction have one single form:
//
//   * SUBG  <Xd|SP>, <Xn|SP>, #<uimm6>, #<uimm4>
//
func (self *Program) SUBG(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("SUBG", 4, Operands { v0, v1, v2, v3 })
    if isXrOrSP(v0) && isXrOrSP(v1) && isUimm6(v2) && isUimm4(v3) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_uimm6 := asUimm6(v2)
        sa_uimm4 := asUimm4(v3)
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd_sp
        return p.setins(addsub_immtags(1, 1, 0, sa_uimm6, 0, sa_uimm4, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SUBG")
}

// SUBHN instruction have one single form:
//
//   * SUBHN  <Vd>.<Tb>, <Vn>.<Ta>, <Vm>.<Ta>
//
func (self *Program) SUBHN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SUBHN", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8H, Vec4S, Vec2D) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SUBHN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for SUBHN")
        }
        return p.setins(asimddiff(0, 0, sa_ta, sa_vm, 6, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SUBHN")
}

// SUBHN2 instruction have one single form:
//
//   * SUBHN2  <Vd>.<Tb>, <Vn>.<Ta>, <Vm>.<Ta>
//
func (self *Program) SUBHN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SUBHN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8H, Vec4S, Vec2D) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for SUBHN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for SUBHN2")
        }
        return p.setins(asimddiff(1, 0, sa_ta, sa_vm, 6, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SUBHN2")
}

// SUBP instruction have one single form:
//
//   * SUBP  <Xd>, <Xn|SP>, <Xm|SP>
//
func (self *Program) SUBP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SUBP", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXrOrSP(v1) && isXrOrSP(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_xm_sp := uint32(v2.(asm.Register).ID())
        Rm := uint32(0b00000)
        Rm |= sa_xm_sp
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd
        return p.setins(dp_2src(1, 0, Rm, 0, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SUBP")
}

// SUBPS instruction have one single form:
//
//   * SUBPS  <Xd>, <Xn|SP>, <Xm|SP>
//
func (self *Program) SUBPS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SUBPS", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXrOrSP(v1) && isXrOrSP(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_xm_sp := uint32(v2.(asm.Register).ID())
        Rm := uint32(0b00000)
        Rm |= sa_xm_sp
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd
        return p.setins(dp_2src(1, 1, Rm, 0, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SUBPS")
}

// SUBS instruction have 6 forms:
//
//   * SUBS  <Wd>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
//   * SUBS  <Wd>, <Wn|WSP>, #<imm>{, <shift>}
//   * SUBS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * SUBS  <Xd>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
//   * SUBS  <Xd>, <Xn|SP>, #<imm>{, <shift>}
//   * SUBS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) SUBS(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("SUBS", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("SUBS", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction SUBS takes 3 or 4 operands")
    }
    // SUBS  <Wd>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWrOrWSP(v1) && isWr(v2) && (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_extend = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_ext(0, 1, 1, 0, sa_wm, sa_extend, sa_amount, sa_wn_wsp, sa_wd))
    }
    // SUBS  <Wd>, <Wn|WSP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isWr(v0) &&
       isWrOrWSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(0, 1, 1, sa_shift, sa_imm, sa_wn_wsp, sa_wd))
    }
    // SUBS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(0, 1, 1, sa_shift, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // SUBS  <Xd>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXrOrSP(v1) && isWrOrXr(v2) && (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend_1 uint32
        var sa_r [4]uint32
        var sa_r__bit_mask [4]uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        switch true {
            case isWr(v2): sa_r = [4]uint32{0b000, 0b010, 0b100, 0b110}
            case isXr(v2): sa_r = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        switch true {
            case isWr(v2): sa_r__bit_mask = [4]uint32{0b110, 0b111, 0b110, 0b111}
            case isXr(v2): sa_r__bit_mask = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        if len(vv) == 1 {
            sa_extend_1 = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        if !matchany(sa_extend_1, &sa_r[0], &sa_r__bit_mask[0], 4) {
            panic("aarch64: invalid combination of operands for SUBS")
        }
        return p.setins(addsub_ext(1, 1, 1, 0, sa_m, sa_extend_1, sa_amount, sa_xn_sp, sa_xd))
    }
    // SUBS  <Xd>, <Xn|SP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isXr(v0) &&
       isXrOrSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(1, 1, 1, sa_shift, sa_imm, sa_xn_sp, sa_xd))
    }
    // SUBS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(1, 1, 1, sa_shift, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SUBS")
}

// SUDOT instruction have one single form:
//
//   * SUDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.4B[<index>]
//
func (self *Program) SUDOT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SUDOT", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVri(v2) &&
       vmoder(v2) == Mode4B {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b0
            case Vec16B: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for SUDOT")
        }
        return p.setins(asimdelem(
            sa_ta,
            0,
            0,
            sa_index & 1,
            maskp(sa_vm, 4, 1),
            mask(sa_vm, 4),
            15,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SUDOT")
}

// SUQADD instruction have 2 forms:
//
//   * SUQADD  <Vd>.<T>, <Vn>.<T>
//   * SUQADD  <V><d>, <V><n>
//
func (self *Program) SUQADD(v0, v1 interface{}) *Instruction {
    p := self.alloc("SUQADD", 2, Operands { v0, v1 })
    // SUQADD  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 0, maskp(sa_t, 1, 2), 3, sa_vn, sa_vd))
    }
    // SUQADD  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for SUQADD")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmisc(0, sa_v, 3, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SUQADD")
}

// SVC instruction have one single form:
//
//   * SVC  #<imm>
//
func (self *Program) SVC(v0 interface{}) *Instruction {
    p := self.alloc("SVC", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(0, sa_imm, 0, 1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SVC")
}

// SWP instruction have 2 forms:
//
//   * SWP  <Ws>, <Wt>, [<Xn|SP>]
//   * SWP  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) SWP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWP", 3, Operands { v0, v1, v2 })
    // SWP  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    // SWP  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 1, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SWP")
}

// SWPA instruction have 2 forms:
//
//   * SWPA  <Ws>, <Wt>, [<Xn|SP>]
//   * SWPA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) SWPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPA", 3, Operands { v0, v1, v2 })
    // SWPA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    // SWPA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 1, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SWPA")
}

// SWPAB instruction have one single form:
//
//   * SWPAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPAB")
}

// SWPAH instruction have one single form:
//
//   * SWPAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPAH")
}

// SWPAL instruction have 2 forms:
//
//   * SWPAL  <Ws>, <Wt>, [<Xn|SP>]
//   * SWPAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) SWPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPAL", 3, Operands { v0, v1, v2 })
    // SWPAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    // SWPAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 1, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SWPAL")
}

// SWPALB instruction have one single form:
//
//   * SWPALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPALB")
}

// SWPALH instruction have one single form:
//
//   * SWPALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPALH")
}

// SWPB instruction have one single form:
//
//   * SWPB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPB")
}

// SWPH instruction have one single form:
//
//   * SWPH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPH")
}

// SWPL instruction have 2 forms:
//
//   * SWPL  <Ws>, <Wt>, [<Xn|SP>]
//   * SWPL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) SWPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPL", 3, Operands { v0, v1, v2 })
    // SWPL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    // SWPL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 1, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SWPL")
}

// SWPLB instruction have one single form:
//
//   * SWPLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPLB")
}

// SWPLH instruction have one single form:
//
//   * SWPLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPLH")
}

// SWPP instruction have one single form:
//
//   * SWPP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) SWPP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 0, sa_xt2, 1, 0, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPP")
}

// SWPPA instruction have one single form:
//
//   * SWPPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) SWPPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 0, sa_xt2, 1, 0, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPPA")
}

// SWPPAL instruction have one single form:
//
//   * SWPPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) SWPPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 1, sa_xt2, 1, 0, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPPAL")
}

// SWPPL instruction have one single form:
//
//   * SWPPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) SWPPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 1, sa_xt2, 1, 0, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPPL")
}

// SYS instruction have one single form:
//
//   * SYS  #<op1>, <Cn>, <Cm>, #<op2>{, <Xt>}
//
func (self *Program) SYS(v0, v1, v2, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("SYS", 4, Operands { v0, v1, v2, v3 })
        case 1  : p = self.alloc("SYS", 5, Operands { v0, v1, v2, v3, vv[0] })
        default : panic("instruction SYS takes 4 or 5 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) &&
       isUimm3(v0) &&
       isUimm4(v1) &&
       isUimm4(v2) &&
       isUimm3(v3) &&
       (len(vv) == 0 || isXr(vv[0])) {
        var sa_xt uint32
        sa_op1 := asUimm3(v0)
        sa_cn := asUimm4(v1)
        sa_cm := asUimm4(v2)
        sa_op2 := asUimm3(v3)
        if len(vv) == 1 {
            sa_xt = uint32(vv[0].(asm.Register).ID())
        }
        return p.setins(systeminstrs(0, sa_op1, sa_cn, sa_cm, sa_op2, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SYS")
}

// SYSL instruction have one single form:
//
//   * SYSL  <Xt>, #<op1>, <Cn>, <Cm>, #<op2>
//
func (self *Program) SYSL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("SYSL", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) && isUimm3(v1) && isUimm4(v2) && isUimm4(v3) && isUimm3(v4) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_op1 := asUimm3(v1)
        sa_cn := asUimm4(v2)
        sa_cm := asUimm4(v3)
        sa_op2 := asUimm3(v4)
        return p.setins(systeminstrs(1, sa_op1, sa_cn, sa_cm, sa_op2, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SYSL")
}

// SYSP instruction have one single form:
//
//   * SYSP  #<op1>, <Cn>, <Cm>, #<op2>{, <Xt1>, <Xt2>}
//
func (self *Program) SYSP(v0, v1, v2, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("SYSP", 4, Operands { v0, v1, v2, v3 })
        case 2  : p = self.alloc("SYSP", 6, Operands { v0, v1, v2, v3, vv[0], vv[1] })
        default : panic("instruction SYSP takes 4 or 6 operands")
    }
    if (len(vv) == 0 || len(vv) == 2) &&
       isUimm3(v0) &&
       isUimm4(v1) &&
       isUimm4(v2) &&
       isUimm3(v3) &&
       (len(vv) == 0 || isXr(vv[0])) &&
       (len(vv) == 0 || isXr(vv[1])) {
        var sa_xt1 uint32
        var sa_xt2 uint32
        sa_op1 := asUimm3(v0)
        sa_cn := asUimm4(v1)
        sa_cm := asUimm4(v2)
        sa_op2 := asUimm3(v3)
        if len(vv) == 2 {
            sa_xt1 = uint32(vv[0].(asm.Register).ID())
            sa_xt2 = uint32(vv[1].(asm.Register).ID())
        }
        if sa_xt1 != sa_xt2 {
            panic("aarch64: invalid combination of operands for SYSP")
        }
        return p.setins(syspairinstrs(0, sa_op1, sa_cn, sa_cm, sa_op2, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SYSP")
}

// TBL instruction have 4 forms:
//
//   * TBL  <Vd>.<Ta>, { <Vn>.16B }, <Vm>.<Ta>
//   * TBL  <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B }, <Vm>.<Ta>
//   * TBL  <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B, <Vn+2>.16B }, <Vm>.<Ta>
//   * TBL  <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B, <Vn+2>.16B, <Vn+3>.16B }, <Vm>.<Ta>
//
func (self *Program) TBL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("TBL", 3, Operands { v0, v1, v2 })
    // TBL  <Vd>.<Ta>, { <Vn>.16B }, <Vm>.<Ta>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVec1(v1) &&
       velm(v1) == Vec16B &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v2) {
        var sa_ta uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_ta = 0b0
            case Vec16B: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(Vector).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdtbl(sa_ta, 0, sa_vm, 0, 0, sa_vn, sa_vd))
    }
    // TBL  <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B }, <Vm>.<Ta>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVec2(v1) &&
       velm(v1) == Vec16B &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v2) {
        var sa_ta uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_ta = 0b0
            case Vec16B: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn_1 := uint32(v1.(Vector).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdtbl(sa_ta, 0, sa_vm, 1, 0, sa_vn_1, sa_vd))
    }
    // TBL  <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B, <Vn+2>.16B }, <Vm>.<Ta>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVec3(v1) &&
       velm(v1) == Vec16B &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v2) {
        var sa_ta uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_ta = 0b0
            case Vec16B: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn_1 := uint32(v1.(Vector).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdtbl(sa_ta, 0, sa_vm, 2, 0, sa_vn_1, sa_vd))
    }
    // TBL  <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B, <Vn+2>.16B, <Vn+3>.16B }, <Vm>.<Ta>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVec4(v1) &&
       velm(v1) == Vec16B &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v2) {
        var sa_ta uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_ta = 0b0
            case Vec16B: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn_1 := uint32(v1.(Vector).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdtbl(sa_ta, 0, sa_vm, 3, 0, sa_vn_1, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for TBL")
}

// TBNZ instruction have one single form:
//
//   * TBNZ  <R><t>, #<imm>, <label>
//
func (self *Program) TBNZ(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("TBNZ", 3, Operands { v0, v1, v2 })
    if isWrOrXr(v0) && isUimm6(v1) && isLabel(v2) {
        var sa_r uint32
        sa_t := uint32(v0.(asm.Register).ID())
        switch true {
            case isWr(v0): sa_r = 0b0
            case isXr(v0): sa_r = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm := asUimm6(v1)
        sa_label := v2.(*asm.Label)
        if sa_imm & 1 != sa_r {
            panic("aarch64: invalid combination of operands for TBNZ")
        }
        return p.setenc(func(pc uintptr) uint32 {
            return testbranch(
                sa_imm & 1,
                1,
                maskp(sa_imm, 1, 5),
                uint32(sa_label.RelativeTo(pc)),
                sa_t,
            )
        })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TBNZ")
}

// TBX instruction have 4 forms:
//
//   * TBX  <Vd>.<Ta>, { <Vn>.16B }, <Vm>.<Ta>
//   * TBX  <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B }, <Vm>.<Ta>
//   * TBX  <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B, <Vn+2>.16B }, <Vm>.<Ta>
//   * TBX  <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B, <Vn+2>.16B, <Vn+3>.16B }, <Vm>.<Ta>
//
func (self *Program) TBX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("TBX", 3, Operands { v0, v1, v2 })
    // TBX  <Vd>.<Ta>, { <Vn>.16B }, <Vm>.<Ta>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVec1(v1) &&
       velm(v1) == Vec16B &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v2) {
        var sa_ta uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_ta = 0b0
            case Vec16B: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(Vector).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdtbl(sa_ta, 0, sa_vm, 0, 1, sa_vn, sa_vd))
    }
    // TBX  <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B }, <Vm>.<Ta>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVec2(v1) &&
       velm(v1) == Vec16B &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v2) {
        var sa_ta uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_ta = 0b0
            case Vec16B: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn_1 := uint32(v1.(Vector).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdtbl(sa_ta, 0, sa_vm, 1, 1, sa_vn_1, sa_vd))
    }
    // TBX  <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B, <Vn+2>.16B }, <Vm>.<Ta>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVec3(v1) &&
       velm(v1) == Vec16B &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v2) {
        var sa_ta uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_ta = 0b0
            case Vec16B: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn_1 := uint32(v1.(Vector).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdtbl(sa_ta, 0, sa_vm, 2, 1, sa_vn_1, sa_vd))
    }
    // TBX  <Vd>.<Ta>, { <Vn>.16B, <Vn+1>.16B, <Vn+2>.16B, <Vn+3>.16B }, <Vm>.<Ta>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B) &&
       isVec4(v1) &&
       velm(v1) == Vec16B &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v0) == vfmt(v2) {
        var sa_ta uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_ta = 0b0
            case Vec16B: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn_1 := uint32(v1.(Vector).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdtbl(sa_ta, 0, sa_vm, 3, 1, sa_vn_1, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for TBX")
}

// TBZ instruction have one single form:
//
//   * TBZ  <R><t>, #<imm>, <label>
//
func (self *Program) TBZ(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("TBZ", 3, Operands { v0, v1, v2 })
    if isWrOrXr(v0) && isUimm6(v1) && isLabel(v2) {
        var sa_r uint32
        sa_t := uint32(v0.(asm.Register).ID())
        switch true {
            case isWr(v0): sa_r = 0b0
            case isXr(v0): sa_r = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm := asUimm6(v1)
        sa_label := v2.(*asm.Label)
        if sa_imm & 1 != sa_r {
            panic("aarch64: invalid combination of operands for TBZ")
        }
        return p.setenc(func(pc uintptr) uint32 {
            return testbranch(
                sa_imm & 1,
                0,
                maskp(sa_imm, 1, 5),
                uint32(sa_label.RelativeTo(pc)),
                sa_t,
            )
        })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TBZ")
}

// TCANCEL instruction have one single form:
//
//   * TCANCEL  #<imm>
//
func (self *Program) TCANCEL(v0 interface{}) *Instruction {
    p := self.alloc("TCANCEL", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(3, sa_imm, 0, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TCANCEL")
}

// TCOMMIT instruction have one single form:
//
//   * TCOMMIT
//
func (self *Program) TCOMMIT() *Instruction {
    p := self.alloc("TCOMMIT", 0, Operands {})
    return p.setins(barriers(0, 3, 31))
}

// TRN1 instruction have one single form:
//
//   * TRN1  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) TRN1(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("TRN1", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdperm(sa_t & 1, maskp(sa_t, 1, 2), sa_vm, 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TRN1")
}

// TRN2 instruction have one single form:
//
//   * TRN2  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) TRN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("TRN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdperm(sa_t & 1, maskp(sa_t, 1, 2), sa_vm, 6, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TRN2")
}

// TSB instruction have one single form:
//
//   * TSB CSYNC
//
func (self *Program) TSB(v0 interface{}) *Instruction {
    p := self.alloc("TSB", 1, Operands { v0 })
    if v0 == CSYNC {
        return p.setins(hints(2, 2))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TSB")
}

// TSTART instruction have one single form:
//
//   * TSTART  <Xt>
//
func (self *Program) TSTART(v0 interface{}) *Instruction {
    p := self.alloc("TSTART", 1, Operands { v0 })
    if isXr(v0) {
        sa_xt := uint32(v0.(asm.Register).ID())
        return p.setins(systemresult(3, 3, 0, 3, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TSTART")
}

// TTEST instruction have one single form:
//
//   * TTEST  <Xt>
//
func (self *Program) TTEST(v0 interface{}) *Instruction {
    p := self.alloc("TTEST", 1, Operands { v0 })
    if isXr(v0) {
        sa_xt := uint32(v0.(asm.Register).ID())
        return p.setins(systemresult(3, 3, 1, 3, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TTEST")
}

// UABA instruction have one single form:
//
//   * UABA  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) UABA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UABA", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 15, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UABA")
}

// UABAL instruction have one single form:
//
//   * UABAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) UABAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UABAL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UABAL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UABAL")
        }
        return p.setins(asimddiff(0, 1, sa_ta, sa_vm, 5, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UABAL")
}

// UABAL2 instruction have one single form:
//
//   * UABAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) UABAL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UABAL2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UABAL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UABAL2")
        }
        return p.setins(asimddiff(1, 1, sa_ta, sa_vm, 5, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UABAL2")
}

// UABD instruction have one single form:
//
//   * UABD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) UABD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UABD", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 14, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UABD")
}

// UABDL instruction have one single form:
//
//   * UABDL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) UABDL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UABDL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UABDL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UABDL")
        }
        return p.setins(asimddiff(0, 1, sa_ta, sa_vm, 7, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UABDL")
}

// UABDL2 instruction have one single form:
//
//   * UABDL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) UABDL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UABDL2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UABDL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UABDL2")
        }
        return p.setins(asimddiff(1, 1, sa_ta, sa_vm, 7, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UABDL2")
}

// UADALP instruction have one single form:
//
//   * UADALP  <Vd>.<Ta>, <Vn>.<Tb>
//
func (self *Program) UADALP(v0, v1 interface{}) *Instruction {
    p := self.alloc("UADALP", 2, Operands { v0, v1 })
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S, Vec1D, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_ta = 0b000
            case Vec8H: sa_ta = 0b001
            case Vec2S: sa_ta = 0b010
            case Vec4S: sa_ta = 0b011
            case Vec1D: sa_ta = 0b100
            case Vec2D: sa_ta = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        if maskp(sa_ta, 1, 2) != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UADALP")
        }
        if sa_ta & 1 != sa_tb & 1 {
            panic("aarch64: invalid combination of operands for UADALP")
        }
        return p.setins(asimdmisc(sa_ta & 1, 1, maskp(sa_ta, 1, 2), 6, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UADALP")
}

// UADDL instruction have one single form:
//
//   * UADDL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) UADDL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UADDL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UADDL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UADDL")
        }
        return p.setins(asimddiff(0, 1, sa_ta, sa_vm, 0, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UADDL")
}

// UADDL2 instruction have one single form:
//
//   * UADDL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) UADDL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UADDL2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UADDL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UADDL2")
        }
        return p.setins(asimddiff(1, 1, sa_ta, sa_vm, 0, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UADDL2")
}

// UADDLP instruction have one single form:
//
//   * UADDLP  <Vd>.<Ta>, <Vn>.<Tb>
//
func (self *Program) UADDLP(v0, v1 interface{}) *Instruction {
    p := self.alloc("UADDLP", 2, Operands { v0, v1 })
    if isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S, Vec1D, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_ta = 0b000
            case Vec8H: sa_ta = 0b001
            case Vec2S: sa_ta = 0b010
            case Vec4S: sa_ta = 0b011
            case Vec1D: sa_ta = 0b100
            case Vec2D: sa_ta = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        if maskp(sa_ta, 1, 2) != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UADDLP")
        }
        if sa_ta & 1 != sa_tb & 1 {
            panic("aarch64: invalid combination of operands for UADDLP")
        }
        return p.setins(asimdmisc(sa_ta & 1, 1, maskp(sa_ta, 1, 2), 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UADDLP")
}

// UADDLV instruction have one single form:
//
//   * UADDLV  <V><d>, <Vn>.<T>
//
func (self *Program) UADDLV(v0, v1 interface{}) *Instruction {
    p := self.alloc("UADDLV", 2, Operands { v0, v1 })
    if isAdvSIMD(v0) && isVr(v1) && isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec4S) {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b00
            case SRegister: sa_v = 0b01
            case DRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for UADDLV")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        if maskp(sa_t, 1, 2) != sa_v {
            panic("aarch64: invalid combination of operands for UADDLV")
        }
        return p.setins(asimdall(sa_t & 1, 1, maskp(sa_t, 1, 2), 3, sa_vn, sa_d))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UADDLV")
}

// UADDW instruction have one single form:
//
//   * UADDW  <Vd>.<Ta>, <Vn>.<Ta>, <Vm>.<Tb>
//
func (self *Program) UADDW(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UADDW", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        switch vfmt(v2) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UADDW")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UADDW")
        }
        return p.setins(asimddiff(0, 1, sa_ta, sa_vm, 1, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UADDW")
}

// UADDW2 instruction have one single form:
//
//   * UADDW2  <Vd>.<Ta>, <Vn>.<Ta>, <Vm>.<Tb>
//
func (self *Program) UADDW2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UADDW2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        switch vfmt(v2) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UADDW2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UADDW2")
        }
        return p.setins(asimddiff(1, 1, sa_ta, sa_vm, 1, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UADDW2")
}

// UBFM instruction have 2 forms:
//
//   * UBFM  <Wd>, <Wn>, #<immr>, #<imms>
//   * UBFM  <Xd>, <Xn>, #<immr>, #<imms>
//
func (self *Program) UBFM(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("UBFM", 4, Operands { v0, v1, v2, v3 })
    // UBFM  <Wd>, <Wn>, #<immr>, #<imms>
    if isWr(v0) && isWr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_immr := asUimm6(v2)
        sa_imms := asUimm6(v3)
        return p.setins(bitfield(0, 2, 0, sa_immr, sa_imms, sa_wn, sa_wd))
    }
    // UBFM  <Xd>, <Xn>, #<immr>, #<imms>
    if isXr(v0) && isXr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_immr_1 := asUimm6(v2)
        sa_imms_1 := asUimm6(v3)
        return p.setins(bitfield(1, 2, 1, sa_immr_1, sa_imms_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UBFM")
}

// UCVTF instruction have 18 forms:
//
//   * UCVTF  <Dd>, <Wn>, #<fbits>
//   * UCVTF  <Dd>, <Wn>
//   * UCVTF  <Dd>, <Xn>, #<fbits>
//   * UCVTF  <Dd>, <Xn>
//   * UCVTF  <Hd>, <Wn>, #<fbits>
//   * UCVTF  <Hd>, <Wn>
//   * UCVTF  <Hd>, <Xn>, #<fbits>
//   * UCVTF  <Hd>, <Xn>
//   * UCVTF  <Sd>, <Wn>, #<fbits>
//   * UCVTF  <Sd>, <Wn>
//   * UCVTF  <Sd>, <Xn>, #<fbits>
//   * UCVTF  <Sd>, <Xn>
//   * UCVTF  <Vd>.<T>, <Vn>.<T>
//   * UCVTF  <Vd>.<T>, <Vn>.<T>
//   * UCVTF  <Vd>.<T>, <Vn>.<T>, #<fbits>
//   * UCVTF  <V><d>, <V><n>
//   * UCVTF  <Hd>, <Hn>
//   * UCVTF  <V><d>, <V><n>, #<fbits>
//
func (self *Program) UCVTF(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("UCVTF", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("UCVTF", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction UCVTF takes 2 or 3 operands")
    }
    // UCVTF  <Dd>, <Wn>, #<fbits>
    if len(vv) == 1 && isDr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 1, 0, 3, sa_fbits, sa_wn, sa_dd))
    }
    // UCVTF  <Dd>, <Wn>
    if isDr(v0) && isWr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 3, sa_wn, sa_dd))
    }
    // UCVTF  <Dd>, <Xn>, #<fbits>
    if len(vv) == 1 && isDr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 1, 0, 3, sa_fbits_1, sa_xn, sa_dd))
    }
    // UCVTF  <Dd>, <Xn>
    if isDr(v0) && isXr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 3, sa_xn, sa_dd))
    }
    // UCVTF  <Hd>, <Wn>, #<fbits>
    if len(vv) == 1 && isHr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 3, 0, 3, sa_fbits, sa_wn, sa_hd))
    }
    // UCVTF  <Hd>, <Wn>
    if isHr(v0) && isWr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 3, sa_wn, sa_hd))
    }
    // UCVTF  <Hd>, <Xn>, #<fbits>
    if len(vv) == 1 && isHr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 3, 0, 3, sa_fbits_1, sa_xn, sa_hd))
    }
    // UCVTF  <Hd>, <Xn>
    if isHr(v0) && isXr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 3, sa_xn, sa_hd))
    }
    // UCVTF  <Sd>, <Wn>, #<fbits>
    if len(vv) == 1 && isSr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 0, 0, 3, sa_fbits, sa_wn, sa_sd))
    }
    // UCVTF  <Sd>, <Wn>
    if isSr(v0) && isWr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 3, sa_wn, sa_sd))
    }
    // UCVTF  <Sd>, <Xn>, #<fbits>
    if len(vv) == 1 && isSr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 0, 0, 3, sa_fbits_1, sa_xn, sa_sd))
    }
    // UCVTF  <Sd>, <Xn>
    if isSr(v0) && isXr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 3, sa_xn, sa_sd))
    }
    // UCVTF  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            case Vec2D: sa_t = 0b11
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 29, sa_vn, sa_vd))
    }
    // UCVTF  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) && isVfmt(v0, Vec4H, Vec8H) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H) && vfmt(v0) == vfmt(v1) {
        var sa_t_1 uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t_1 = 0b0
            case Vec8H: sa_t_1 = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmiscfp16(sa_t_1, 1, 0, 29, sa_vn, sa_vd))
    }
    // UCVTF  <Vd>.<T>, <Vn>.<T>, #<fbits>
    if len(vv) == 1 &&
       isVr(v0) &&
       isVfmt(v0, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(vv[0]) &&
       vfmt(v0) == vfmt(v1) {
        var sa_fbits uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0010: sa_fbits = 32 - uint32(asLit(vv[0]))
            case 0b0100: sa_fbits = 64 - uint32(asLit(vv[0]))
            case 0b1000: sa_fbits = 128 - uint32(asLit(vv[0]))
            default: panic("aarch64: invalid operand 'sa_fbits' for UCVTF")
        }
        if maskp(sa_fbits, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for UCVTF")
        }
        return p.setins(asimdshf(sa_t & 1, 1, maskp(sa_fbits, 3, 4), mask(sa_fbits, 3), 28, sa_vn, sa_vd))
    }
    // UCVTF  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case SRegister: sa_v = 0b0
            case DRegister: sa_v = 0b1
            default: panic("aarch64: invalid scalar operand size for UCVTF")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        size := uint32(0b00)
        size |= sa_v
        return p.setins(asisdmisc(1, size, 29, sa_n, sa_d))
    }
    // UCVTF  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmiscfp16(1, 0, 29, sa_hn, sa_hd))
    }
    // UCVTF  <V><d>, <V><n>, #<fbits>
    if len(vv) == 1 && isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(vv[0]) && isSameType(v0, v1) {
        var sa_fbits_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case HRegister: sa_v = 0b0010
            case SRegister: sa_v = 0b0100
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for UCVTF")
        }
        switch v0.(type) {
            case HRegister: sa_v__bit_mask = 0b1110
            case SRegister: sa_v__bit_mask = 0b1100
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for UCVTF")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b0010: sa_fbits_1 = 32 - uint32(asLit(vv[0]))
            case 0b0100: sa_fbits_1 = 64 - uint32(asLit(vv[0]))
            case 0b1000: sa_fbits_1 = 128 - uint32(asLit(vv[0]))
            default: panic("aarch64: invalid operand 'sa_fbits_1' for UCVTF")
        }
        if maskp(sa_fbits_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for UCVTF")
        }
        return p.setins(asisdshf(1, maskp(sa_fbits_1, 3, 4), mask(sa_fbits_1, 3), 28, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UCVTF")
}

// UDF instruction have one single form:
//
//   * UDF  #<imm>
//
func (self *Program) UDF(v0 interface{}) *Instruction {
    p := self.alloc("UDF", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(perm_undef(sa_imm))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UDF")
}

// UDIV instruction have 2 forms:
//
//   * UDIV  <Wd>, <Wn>, <Wm>
//   * UDIV  <Xd>, <Xn>, <Xm>
//
func (self *Program) UDIV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UDIV", 3, Operands { v0, v1, v2 })
    // UDIV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 2, sa_wn, sa_wd))
    }
    // UDIV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 2, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UDIV")
}

// UDOT instruction have 2 forms:
//
//   * UDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.4B[<index>]
//   * UDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) UDOT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UDOT", 3, Operands { v0, v1, v2 })
    // UDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.4B[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVri(v2) &&
       vmoder(v2) == Mode4B {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b0
            case Vec16B: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for UDOT")
        }
        return p.setins(asimdelem(
            sa_ta,
            1,
            2,
            sa_index & 1,
            maskp(sa_vm, 4, 1),
            mask(sa_vm, 4),
            14,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // UDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b0
            case Vec16B: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for UDOT")
        }
        return p.setins(asimdsame2(sa_ta, 1, 2, sa_vm, 2, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UDOT")
}

// UHADD instruction have one single form:
//
//   * UHADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) UHADD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UHADD", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 0, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UHADD")
}

// UHSUB instruction have one single form:
//
//   * UHSUB  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) UHSUB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UHSUB", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 4, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UHSUB")
}

// UMADDL instruction have one single form:
//
//   * UMADDL  <Xd>, <Wn>, <Wm>, <Xa>
//
func (self *Program) UMADDL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("UMADDL", 4, Operands { v0, v1, v2, v3 })
    if isXr(v0) && isWr(v1) && isWr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 5, sa_wm, 0, sa_xa, sa_wn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UMADDL")
}

// UMAX instruction have 5 forms:
//
//   * UMAX  <Wd>, <Wn>, #<uimm>
//   * UMAX  <Wd>, <Wn>, <Wm>
//   * UMAX  <Xd>, <Xn>, #<uimm>
//   * UMAX  <Xd>, <Xn>, <Xm>
//   * UMAX  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) UMAX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMAX", 3, Operands { v0, v1, v2 })
    // UMAX  <Wd>, <Wn>, #<uimm>
    if isWr(v0) && isWr(v1) && isFpImm8(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_uimm := asFpImm8(v2)
        return p.setins(minmax_imm(0, 0, 0, 1, sa_uimm, sa_wn, sa_wd))
    }
    // UMAX  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 25, sa_wn, sa_wd))
    }
    // UMAX  <Xd>, <Xn>, #<uimm>
    if isXr(v0) && isXr(v1) && isFpImm8(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_uimm := asFpImm8(v2)
        return p.setins(minmax_imm(1, 0, 0, 1, sa_uimm, sa_xn, sa_xd))
    }
    // UMAX  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 25, sa_xn, sa_xd))
    }
    // UMAX  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 12, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UMAX")
}

// UMAXP instruction have one single form:
//
//   * UMAXP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) UMAXP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMAXP", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 20, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UMAXP")
}

// UMAXV instruction have one single form:
//
//   * UMAXV  <V><d>, <Vn>.<T>
//
func (self *Program) UMAXV(v0, v1 interface{}) *Instruction {
    p := self.alloc("UMAXV", 2, Operands { v0, v1 })
    if isAdvSIMD(v0) && isVr(v1) && isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec4S) {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for UMAXV")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        if maskp(sa_t, 1, 2) != sa_v {
            panic("aarch64: invalid combination of operands for UMAXV")
        }
        return p.setins(asimdall(sa_t & 1, 1, maskp(sa_t, 1, 2), 10, sa_vn, sa_d))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UMAXV")
}

// UMIN instruction have 5 forms:
//
//   * UMIN  <Wd>, <Wn>, #<uimm>
//   * UMIN  <Wd>, <Wn>, <Wm>
//   * UMIN  <Xd>, <Xn>, #<uimm>
//   * UMIN  <Xd>, <Xn>, <Xm>
//   * UMIN  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) UMIN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMIN", 3, Operands { v0, v1, v2 })
    // UMIN  <Wd>, <Wn>, #<uimm>
    if isWr(v0) && isWr(v1) && isFpImm8(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_uimm := asFpImm8(v2)
        return p.setins(minmax_imm(0, 0, 0, 3, sa_uimm, sa_wn, sa_wd))
    }
    // UMIN  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 27, sa_wn, sa_wd))
    }
    // UMIN  <Xd>, <Xn>, #<uimm>
    if isXr(v0) && isXr(v1) && isFpImm8(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_uimm := asFpImm8(v2)
        return p.setins(minmax_imm(1, 0, 0, 3, sa_uimm, sa_xn, sa_xd))
    }
    // UMIN  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 27, sa_xn, sa_xd))
    }
    // UMIN  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 13, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UMIN")
}

// UMINP instruction have one single form:
//
//   * UMINP  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) UMINP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMINP", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 21, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UMINP")
}

// UMINV instruction have one single form:
//
//   * UMINV  <V><d>, <Vn>.<T>
//
func (self *Program) UMINV(v0, v1 interface{}) *Instruction {
    p := self.alloc("UMINV", 2, Operands { v0, v1 })
    if isAdvSIMD(v0) && isVr(v1) && isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec4S) {
        var sa_t uint32
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            default: panic("aarch64: invalid scalar operand size for UMINV")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        if maskp(sa_t, 1, 2) != sa_v {
            panic("aarch64: invalid combination of operands for UMINV")
        }
        return p.setins(asimdall(sa_t & 1, 1, maskp(sa_t, 1, 2), 26, sa_vn, sa_d))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UMINV")
}

// UMLAL instruction have 2 forms:
//
//   * UMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * UMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) UMLAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMLAL", 3, Operands { v0, v1, v2 })
    // UMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UMLAL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UMLAL")
        }
        return p.setins(asimddiff(0, 1, sa_ta, sa_vm, 8, sa_vn, sa_vd))
    }
    // UMLAL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for UMLAL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for UMLAL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UMLAL")
        }
        return p.setins(asimdelem(
            0,
            1,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            2,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UMLAL")
}

// UMLAL2 instruction have 2 forms:
//
//   * UMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * UMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) UMLAL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMLAL2", 3, Operands { v0, v1, v2 })
    // UMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UMLAL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UMLAL2")
        }
        return p.setins(asimddiff(1, 1, sa_ta, sa_vm, 8, sa_vn, sa_vd))
    }
    // UMLAL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for UMLAL2")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for UMLAL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UMLAL2")
        }
        return p.setins(asimdelem(
            1,
            1,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            2,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UMLAL2")
}

// UMLSL instruction have 2 forms:
//
//   * UMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * UMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) UMLSL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMLSL", 3, Operands { v0, v1, v2 })
    // UMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UMLSL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UMLSL")
        }
        return p.setins(asimddiff(0, 1, sa_ta, sa_vm, 10, sa_vn, sa_vd))
    }
    // UMLSL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for UMLSL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for UMLSL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UMLSL")
        }
        return p.setins(asimdelem(
            0,
            1,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            6,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UMLSL")
}

// UMLSL2 instruction have 2 forms:
//
//   * UMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * UMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) UMLSL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMLSL2", 3, Operands { v0, v1, v2 })
    // UMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UMLSL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UMLSL2")
        }
        return p.setins(asimddiff(1, 1, sa_ta, sa_vm, 10, sa_vn, sa_vd))
    }
    // UMLSL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for UMLSL2")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for UMLSL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UMLSL2")
        }
        return p.setins(asimdelem(
            1,
            1,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            6,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UMLSL2")
}

// UMMLA instruction have one single form:
//
//   * UMMLA  <Vd>.4S, <Vn>.16B, <Vm>.16B
//
func (self *Program) UMMLA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMMLA", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec16B && isVr(v2) && vfmt(v2) == Vec16B {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame2(1, 1, 2, sa_vm, 4, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UMMLA")
}

// UMOV instruction have 2 forms:
//
//   * UMOV  <Wd>, <Vn>.<Ts>[<index>]
//   * UMOV  <Xd>, <Vn>.<Ts>[<index>]
//
func (self *Program) UMOV(v0, v1 interface{}) *Instruction {
    p := self.alloc("UMOV", 2, Operands { v0, v1 })
    // UMOV  <Wd>, <Vn>.<Ts>[<index>]
    if isWr(v0) && isVri(v1) {
        var sa_ts uint32
        var sa_ts__bit_mask uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(VidxRegister).ID())
        switch vmoder(v1) {
            case ModeB: sa_ts = 0b00001
            case ModeH: sa_ts = 0b00010
            case ModeS: sa_ts = 0b00100
            default: panic("aarch64: unreachable")
        }
        switch vmoder(v1) {
            case ModeB: sa_ts__bit_mask = 0b00001
            case ModeH: sa_ts__bit_mask = 0b00011
            case ModeS: sa_ts__bit_mask = 0b00111
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v1))
        if sa_index != sa_ts & sa_ts__bit_mask {
            panic("aarch64: invalid combination of operands for UMOV")
        }
        return p.setins(asimdins(0, 0, sa_index, 7, sa_vn, sa_wd))
    }
    // UMOV  <Xd>, <Vn>.<Ts>[<index>]
    if isXr(v0) && isVri(v1) {
        var sa_ts_1 uint32
        var sa_ts_1__bit_mask uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(VidxRegister).ID())
        switch vmoder(v1) {
            case ModeD: sa_ts_1 = 0b01000
            default: panic("aarch64: unreachable")
        }
        switch vmoder(v1) {
            case ModeD: sa_ts_1__bit_mask = 0b01111
            default: panic("aarch64: unreachable")
        }
        sa_index_1 := uint32(vidxr(v1))
        if sa_index_1 != sa_ts_1 & sa_ts_1__bit_mask {
            panic("aarch64: invalid combination of operands for UMOV")
        }
        return p.setins(asimdins(1, 0, sa_index_1, 7, sa_vn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UMOV")
}

// UMSUBL instruction have one single form:
//
//   * UMSUBL  <Xd>, <Wn>, <Wm>, <Xa>
//
func (self *Program) UMSUBL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("UMSUBL", 4, Operands { v0, v1, v2, v3 })
    if isXr(v0) && isWr(v1) && isWr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 5, sa_wm, 1, sa_xa, sa_wn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UMSUBL")
}

// UMULH instruction have one single form:
//
//   * UMULH  <Xd>, <Xn>, <Xm>
//
func (self *Program) UMULH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMULH", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 6, sa_xm, 0, 31, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UMULH")
}

// UMULL instruction have 2 forms:
//
//   * UMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * UMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) UMULL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMULL", 3, Operands { v0, v1, v2 })
    // UMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UMULL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UMULL")
        }
        return p.setins(asimddiff(0, 1, sa_ta, sa_vm, 12, sa_vn, sa_vd))
    }
    // UMULL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for UMULL")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for UMULL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UMULL")
        }
        return p.setins(asimdelem(
            0,
            1,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            10,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UMULL")
}

// UMULL2 instruction have 2 forms:
//
//   * UMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//   * UMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
//
func (self *Program) UMULL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMULL2", 3, Operands { v0, v1, v2 })
    // UMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UMULL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UMULL2")
        }
        return p.setins(asimddiff(1, 1, sa_ta, sa_vm, 12, sa_vn, sa_vd))
    }
    // UMULL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Ts>[<index>]
    if isVr(v0) && isVfmt(v0, Vec4S, Vec2D) && isVr(v1) && isVfmt(v1, Vec4H, Vec8H, Vec2S, Vec4S) && isVri(v2) {
        var sa_ta uint32
        var sa_tb uint32
        var sa_ts uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        switch vmoder(v2) {
            case ModeH: sa_ts = 0b01
            case ModeS: sa_ts = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_index := uint32(vidxr(v2))
        if maskp(sa_index, 3, 2) != sa_ta ||
           sa_ta != maskp(sa_tb, 1, 2) ||
           maskp(sa_tb, 1, 2) != sa_ts ||
           sa_ts != maskp(sa_vm, 5, 2) {
            panic("aarch64: invalid combination of operands for UMULL2")
        }
        if sa_index & 1 != maskp(sa_vm, 4, 1) {
            panic("aarch64: invalid combination of operands for UMULL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UMULL2")
        }
        return p.setins(asimdelem(
            1,
            1,
            maskp(sa_index, 3, 2),
            maskp(sa_index, 2, 1),
            sa_index & 1,
            mask(sa_vm, 4),
            10,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UMULL2")
}

// UQADD instruction have 2 forms:
//
//   * UQADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * UQADD  <V><d>, <V><n>, <V><m>
//
func (self *Program) UQADD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UQADD", 3, Operands { v0, v1, v2 })
    // UQADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 1, sa_vn, sa_vd))
    }
    // UQADD  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for UQADD")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(1, sa_v, sa_m, 1, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UQADD")
}

// UQRSHL instruction have 2 forms:
//
//   * UQRSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * UQRSHL  <V><d>, <V><n>, <V><m>
//
func (self *Program) UQRSHL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UQRSHL", 3, Operands { v0, v1, v2 })
    // UQRSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 11, sa_vn, sa_vd))
    }
    // UQRSHL  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for UQRSHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(1, sa_v, sa_m, 11, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UQRSHL")
}

// UQRSHRN instruction have 2 forms:
//
//   * UQRSHRN  <Vb><d>, <Va><n>, #<shift>
//   * UQRSHRN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) UQRSHRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UQRSHRN", 3, Operands { v0, v1, v2 })
    // UQRSHRN  <Vb><d>, <Va><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) {
        var sa_shift_1 uint32
        var sa_va uint32
        var sa_va__bit_mask uint32
        var sa_vb uint32
        var sa_vb__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_vb = 0b0001
            case HRegister: sa_vb = 0b0010
            case SRegister: sa_vb = 0b0100
            default: panic("aarch64: invalid scalar operand size for UQRSHRN")
        }
        switch v0.(type) {
            case BRegister: sa_vb__bit_mask = 0b1111
            case HRegister: sa_vb__bit_mask = 0b1110
            case SRegister: sa_vb__bit_mask = 0b1100
            default: panic("aarch64: invalid scalar operand size for UQRSHRN")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_va = 0b0001
            case SRegister: sa_va = 0b0010
            case DRegister: sa_va = 0b0100
            default: panic("aarch64: invalid scalar operand size for UQRSHRN")
        }
        switch v1.(type) {
            case HRegister: sa_va__bit_mask = 0b1111
            case SRegister: sa_va__bit_mask = 0b1110
            case DRegister: sa_va__bit_mask = 0b1100
            default: panic("aarch64: invalid scalar operand size for UQRSHRN")
        }
        switch sa_vb {
            case 0b0001: sa_shift_1 = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift_1 = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift_1 = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for UQRSHRN")
        }
        if maskp(sa_shift_1, 3, 4) != sa_va & sa_va__bit_mask || sa_va & sa_va__bit_mask != sa_vb & sa_vb__bit_mask {
            panic("aarch64: invalid combination of operands for UQRSHRN")
        }
        return p.setins(asisdshf(1, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 19, sa_n, sa_d))
    }
    // UQRSHRN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for UQRSHRN")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for UQRSHRN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UQRSHRN")
        }
        return p.setins(asimdshf(0, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 19, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UQRSHRN")
}

// UQRSHRN2 instruction have one single form:
//
//   * UQRSHRN2  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) UQRSHRN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UQRSHRN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for UQRSHRN2")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for UQRSHRN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UQRSHRN2")
        }
        return p.setins(asimdshf(1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 19, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UQRSHRN2")
}

// UQSHL instruction have 4 forms:
//
//   * UQSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * UQSHL  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * UQSHL  <V><d>, <V><n>, <V><m>
//   * UQSHL  <V><d>, <V><n>, #<shift>
//
func (self *Program) UQSHL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UQSHL", 3, Operands { v0, v1, v2 })
    // UQSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 9, sa_vn, sa_vd))
    }
    // UQSHL  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift = uint32(asLit(v2)) + 32
            case 0b1000: sa_shift = uint32(asLit(v2)) + 64
            default: panic("aarch64: invalid operand 'sa_shift' for UQSHL")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for UQSHL")
        }
        return p.setins(asimdshf(sa_t & 1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 14, sa_vn, sa_vd))
    }
    // UQSHL  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for UQSHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(1, sa_v, sa_m, 9, sa_n, sa_d))
    }
    // UQSHL  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b0001
            case HRegister: sa_v = 0b0010
            case SRegister: sa_v = 0b0100
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for UQSHL")
        }
        switch v0.(type) {
            case BRegister: sa_v__bit_mask = 0b1111
            case HRegister: sa_v__bit_mask = 0b1110
            case SRegister: sa_v__bit_mask = 0b1100
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for UQSHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b0001: sa_shift_1 = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift_1 = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift_1 = uint32(asLit(v2)) + 32
            case 0b1000: sa_shift_1 = uint32(asLit(v2)) + 64
            default: panic("aarch64: invalid operand 'sa_shift_1' for UQSHL")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for UQSHL")
        }
        return p.setins(asisdshf(1, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 14, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UQSHL")
}

// UQSHRN instruction have 2 forms:
//
//   * UQSHRN  <Vb><d>, <Va><n>, #<shift>
//   * UQSHRN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) UQSHRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UQSHRN", 3, Operands { v0, v1, v2 })
    // UQSHRN  <Vb><d>, <Va><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) {
        var sa_shift_1 uint32
        var sa_va uint32
        var sa_va__bit_mask uint32
        var sa_vb uint32
        var sa_vb__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_vb = 0b0001
            case HRegister: sa_vb = 0b0010
            case SRegister: sa_vb = 0b0100
            default: panic("aarch64: invalid scalar operand size for UQSHRN")
        }
        switch v0.(type) {
            case BRegister: sa_vb__bit_mask = 0b1111
            case HRegister: sa_vb__bit_mask = 0b1110
            case SRegister: sa_vb__bit_mask = 0b1100
            default: panic("aarch64: invalid scalar operand size for UQSHRN")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_va = 0b0001
            case SRegister: sa_va = 0b0010
            case DRegister: sa_va = 0b0100
            default: panic("aarch64: invalid scalar operand size for UQSHRN")
        }
        switch v1.(type) {
            case HRegister: sa_va__bit_mask = 0b1111
            case SRegister: sa_va__bit_mask = 0b1110
            case DRegister: sa_va__bit_mask = 0b1100
            default: panic("aarch64: invalid scalar operand size for UQSHRN")
        }
        switch sa_vb {
            case 0b0001: sa_shift_1 = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift_1 = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift_1 = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for UQSHRN")
        }
        if maskp(sa_shift_1, 3, 4) != sa_va & sa_va__bit_mask || sa_va & sa_va__bit_mask != sa_vb & sa_vb__bit_mask {
            panic("aarch64: invalid combination of operands for UQSHRN")
        }
        return p.setins(asisdshf(1, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 18, sa_n, sa_d))
    }
    // UQSHRN  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for UQSHRN")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for UQSHRN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UQSHRN")
        }
        return p.setins(asimdshf(0, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 18, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UQSHRN")
}

// UQSHRN2 instruction have one single form:
//
//   * UQSHRN2  <Vd>.<Tb>, <Vn>.<Ta>, #<shift>
//
func (self *Program) UQSHRN2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UQSHRN2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        switch maskp(sa_tb, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for UQSHRN2")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for UQSHRN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UQSHRN2")
        }
        return p.setins(asimdshf(1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 18, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UQSHRN2")
}

// UQSUB instruction have 2 forms:
//
//   * UQSUB  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * UQSUB  <V><d>, <V><n>, <V><m>
//
func (self *Program) UQSUB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UQSUB", 3, Operands { v0, v1, v2 })
    // UQSUB  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 5, sa_vn, sa_vd))
    }
    // UQSUB  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for UQSUB")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(1, sa_v, sa_m, 5, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UQSUB")
}

// UQXTN instruction have 2 forms:
//
//   * UQXTN  <Vb><d>, <Va><n>
//   * UQXTN  <Vd>.<Tb>, <Vn>.<Ta>
//
func (self *Program) UQXTN(v0, v1 interface{}) *Instruction {
    p := self.alloc("UQXTN", 2, Operands { v0, v1 })
    // UQXTN  <Vb><d>, <Va><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) {
        var sa_va uint32
        var sa_vb uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_vb = 0b00
            case HRegister: sa_vb = 0b01
            case SRegister: sa_vb = 0b10
            default: panic("aarch64: invalid scalar operand size for UQXTN")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch v1.(type) {
            case HRegister: sa_va = 0b00
            case SRegister: sa_va = 0b01
            case DRegister: sa_va = 0b10
            default: panic("aarch64: invalid scalar operand size for UQXTN")
        }
        if sa_va != sa_vb {
            panic("aarch64: invalid combination of operands for UQXTN")
        }
        return p.setins(asisdmisc(1, sa_va, 20, sa_n, sa_d))
    }
    // UQXTN  <Vd>.<Tb>, <Vn>.<Ta>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UQXTN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for UQXTN")
        }
        return p.setins(asimdmisc(0, 1, sa_ta, 20, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UQXTN")
}

// UQXTN2 instruction have one single form:
//
//   * UQXTN2  <Vd>.<Tb>, <Vn>.<Ta>
//
func (self *Program) UQXTN2(v0, v1 interface{}) *Instruction {
    p := self.alloc("UQXTN2", 2, Operands { v0, v1 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for UQXTN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for UQXTN2")
        }
        return p.setins(asimdmisc(1, 1, sa_ta, 20, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UQXTN2")
}

// URECPE instruction have one single form:
//
//   * URECPE  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) URECPE(v0, v1 interface{}) *Instruction {
    p := self.alloc("URECPE", 2, Operands { v0, v1 })
    if isVr(v0) && isVfmt(v0, Vec2S, Vec4S) && isVr(v1) && isVfmt(v1, Vec2S, Vec4S) && vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 0, size, 28, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for URECPE")
}

// URHADD instruction have one single form:
//
//   * URHADD  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) URHADD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("URHADD", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for URHADD")
}

// URSHL instruction have 2 forms:
//
//   * URSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * URSHL  <V><d>, <V><n>, <V><m>
//
func (self *Program) URSHL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("URSHL", 3, Operands { v0, v1, v2 })
    // URSHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 10, sa_vn, sa_vd))
    }
    // URSHL  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for URSHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(1, sa_v, sa_m, 10, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for URSHL")
}

// URSHR instruction have 2 forms:
//
//   * URSHR  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * URSHR  <V><d>, <V><n>, #<shift>
//
func (self *Program) URSHR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("URSHR", 3, Operands { v0, v1, v2 })
    // URSHR  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            case 0b1000: sa_shift = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for URSHR")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for URSHR")
        }
        return p.setins(asimdshf(sa_t & 1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 4, sa_vn, sa_vd))
    }
    // URSHR  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for URSHR")
        }
        switch v0.(type) {
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for URSHR")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for URSHR")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for URSHR")
        }
        return p.setins(asisdshf(1, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 4, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for URSHR")
}

// URSQRTE instruction have one single form:
//
//   * URSQRTE  <Vd>.<T>, <Vn>.<T>
//
func (self *Program) URSQRTE(v0, v1 interface{}) *Instruction {
    p := self.alloc("URSQRTE", 2, Operands { v0, v1 })
    if isVr(v0) && isVfmt(v0, Vec2S, Vec4S) && isVr(v1) && isVfmt(v1, Vec2S, Vec4S) && vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_t = 0b00
            case Vec4S: sa_t = 0b01
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        size := uint32(0b10)
        size |= maskp(sa_t, 1, 1)
        return p.setins(asimdmisc(sa_t & 1, 1, size, 28, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for URSQRTE")
}

// URSRA instruction have 2 forms:
//
//   * URSRA  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * URSRA  <V><d>, <V><n>, #<shift>
//
func (self *Program) URSRA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("URSRA", 3, Operands { v0, v1, v2 })
    // URSRA  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            case 0b1000: sa_shift = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for URSRA")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for URSRA")
        }
        return p.setins(asimdshf(sa_t & 1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 6, sa_vn, sa_vd))
    }
    // URSRA  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for URSRA")
        }
        switch v0.(type) {
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for URSRA")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for URSRA")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for URSRA")
        }
        return p.setins(asisdshf(1, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 6, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for URSRA")
}

// USDOT instruction have 2 forms:
//
//   * USDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.4B[<index>]
//   * USDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) USDOT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("USDOT", 3, Operands { v0, v1, v2 })
    // USDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.4B[<index>]
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVri(v2) &&
       vmoder(v2) == Mode4B {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b0
            case Vec16B: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(VidxRegister).ID())
        sa_index := uint32(vidxr(v2))
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for USDOT")
        }
        return p.setins(asimdelem(
            sa_ta,
            0,
            2,
            sa_index & 1,
            maskp(sa_vm, 4, 1),
            mask(sa_vm, 4),
            15,
            maskp(sa_index, 1, 1),
            sa_vn,
            sa_vd,
        ))
    }
    // USDOT  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
    if isVr(v0) &&
       isVfmt(v0, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec2S: sa_ta = 0b0
            case Vec4S: sa_ta = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b0
            case Vec16B: sa_tb = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != sa_tb {
            panic("aarch64: invalid combination of operands for USDOT")
        }
        return p.setins(asimdsame2(sa_ta, 0, 2, sa_vm, 3, sa_vn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for USDOT")
}

// USHL instruction have 2 forms:
//
//   * USHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//   * USHL  <V><d>, <V><n>, <V><m>
//
func (self *Program) USHL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("USHL", 3, Operands { v0, v1, v2 })
    // USHL  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame(sa_t & 1, 1, maskp(sa_t, 1, 2), sa_vm, 8, sa_vn, sa_vd))
    }
    // USHL  <V><d>, <V><n>, <V><m>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isAdvSIMD(v2) && isSameType(v0, v1) && isSameType(v1, v2) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for USHL")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        return p.setins(asisdsame(1, sa_v, sa_m, 8, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for USHL")
}

// USHLL instruction have one single form:
//
//   * USHLL  <Vd>.<Ta>, <Vn>.<Tb>, #<shift>
//
func (self *Program) USHLL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("USHLL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        switch sa_ta {
            case 0b0001: sa_shift = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift = uint32(asLit(v2)) + 32
            default: panic("aarch64: invalid operand 'sa_shift' for USHLL")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for USHLL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for USHLL")
        }
        return p.setins(asimdshf(0, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 20, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for USHLL")
}

// USHLL2 instruction have one single form:
//
//   * USHLL2  <Vd>.<Ta>, <Vn>.<Tb>, #<shift>
//
func (self *Program) USHLL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("USHLL2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isFpBits(v2) {
        var sa_shift uint32
        var sa_ta uint32
        var sa_ta__bit_mask uint32
        var sa_tb uint32
        var sa_tb__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b0001
            case Vec4S: sa_ta = 0b0010
            case Vec2D: sa_ta = 0b0100
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8H: sa_ta__bit_mask = 0b1111
            case Vec4S: sa_ta__bit_mask = 0b1110
            case Vec2D: sa_ta__bit_mask = 0b1100
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b00010
            case Vec16B: sa_tb = 0b00011
            case Vec4H: sa_tb = 0b00100
            case Vec8H: sa_tb = 0b00101
            case Vec2S: sa_tb = 0b01000
            case Vec4S: sa_tb = 0b01001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v1) {
            case Vec8B: sa_tb__bit_mask = 0b11111
            case Vec16B: sa_tb__bit_mask = 0b11111
            case Vec4H: sa_tb__bit_mask = 0b11101
            case Vec8H: sa_tb__bit_mask = 0b11101
            case Vec2S: sa_tb__bit_mask = 0b11001
            case Vec4S: sa_tb__bit_mask = 0b11001
            default: panic("aarch64: unreachable")
        }
        switch sa_ta {
            case 0b0001: sa_shift = uint32(asLit(v2)) + 8
            case 0b0010: sa_shift = uint32(asLit(v2)) + 16
            case 0b0100: sa_shift = uint32(asLit(v2)) + 32
            default: panic("aarch64: invalid operand 'sa_shift' for USHLL2")
        }
        if maskp(sa_shift, 3, 4) != sa_ta & sa_ta__bit_mask ||
           sa_ta & sa_ta__bit_mask != maskp(sa_tb, 1, 4) & maskp(sa_tb__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for USHLL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for USHLL2")
        }
        return p.setins(asimdshf(1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 20, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for USHLL2")
}

// USHR instruction have 2 forms:
//
//   * USHR  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * USHR  <V><d>, <V><n>, #<shift>
//
func (self *Program) USHR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("USHR", 3, Operands { v0, v1, v2 })
    // USHR  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            case 0b1000: sa_shift = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for USHR")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for USHR")
        }
        return p.setins(asimdshf(sa_t & 1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 0, sa_vn, sa_vd))
    }
    // USHR  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for USHR")
        }
        switch v0.(type) {
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for USHR")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for USHR")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for USHR")
        }
        return p.setins(asisdshf(1, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 0, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for USHR")
}

// USMMLA instruction have one single form:
//
//   * USMMLA  <Vd>.4S, <Vn>.16B, <Vm>.16B
//
func (self *Program) USMMLA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("USMMLA", 3, Operands { v0, v1, v2 })
    if isVr(v0) && vfmt(v0) == Vec4S && isVr(v1) && vfmt(v1) == Vec16B && isVr(v2) && vfmt(v2) == Vec16B {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdsame2(1, 0, 2, sa_vm, 5, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for USMMLA")
}

// USQADD instruction have 2 forms:
//
//   * USQADD  <Vd>.<T>, <Vn>.<T>
//   * USQADD  <V><d>, <V><n>
//
func (self *Program) USQADD(v0, v1 interface{}) *Instruction {
    p := self.alloc("USQADD", 2, Operands { v0, v1 })
    // USQADD  <Vd>.<T>, <Vn>.<T>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        return p.setins(asimdmisc(sa_t & 1, 1, maskp(sa_t, 1, 2), 3, sa_vn, sa_vd))
    }
    // USQADD  <V><d>, <V><n>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isSameType(v0, v1) {
        var sa_v uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case BRegister: sa_v = 0b00
            case HRegister: sa_v = 0b01
            case SRegister: sa_v = 0b10
            case DRegister: sa_v = 0b11
            default: panic("aarch64: invalid scalar operand size for USQADD")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        return p.setins(asisdmisc(1, sa_v, 3, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for USQADD")
}

// USRA instruction have 2 forms:
//
//   * USRA  <Vd>.<T>, <Vn>.<T>, #<shift>
//   * USRA  <V><d>, <V><n>, #<shift>
//
func (self *Program) USRA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("USRA", 3, Operands { v0, v1, v2 })
    // USRA  <Vd>.<T>, <Vn>.<T>, #<shift>
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isFpBits(v2) &&
       vfmt(v0) == vfmt(v1) {
        var sa_shift uint32
        var sa_t uint32
        var sa_t__bit_mask uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b00010
            case Vec16B: sa_t = 0b00011
            case Vec4H: sa_t = 0b00100
            case Vec8H: sa_t = 0b00101
            case Vec2S: sa_t = 0b01000
            case Vec4S: sa_t = 0b01001
            case Vec2D: sa_t = 0b10001
            default: panic("aarch64: unreachable")
        }
        switch vfmt(v0) {
            case Vec8B: sa_t__bit_mask = 0b11111
            case Vec16B: sa_t__bit_mask = 0b11111
            case Vec4H: sa_t__bit_mask = 0b11101
            case Vec8H: sa_t__bit_mask = 0b11101
            case Vec2S: sa_t__bit_mask = 0b11001
            case Vec4S: sa_t__bit_mask = 0b11001
            case Vec2D: sa_t__bit_mask = 0b10001
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch maskp(sa_t, 1, 4) {
            case 0b0001: sa_shift = 16 - uint32(asLit(v2))
            case 0b0010: sa_shift = 32 - uint32(asLit(v2))
            case 0b0100: sa_shift = 64 - uint32(asLit(v2))
            case 0b1000: sa_shift = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift' for USRA")
        }
        if maskp(sa_shift, 3, 4) != maskp(sa_t, 1, 4) & maskp(sa_t__bit_mask, 1, 4) {
            panic("aarch64: invalid combination of operands for USRA")
        }
        return p.setins(asimdshf(sa_t & 1, 1, maskp(sa_shift, 3, 4), mask(sa_shift, 3), 2, sa_vn, sa_vd))
    }
    // USRA  <V><d>, <V><n>, #<shift>
    if isAdvSIMD(v0) && isAdvSIMD(v1) && isFpBits(v2) && isSameType(v0, v1) {
        var sa_shift_1 uint32
        var sa_v uint32
        var sa_v__bit_mask uint32
        sa_d := uint32(v0.(asm.Register).ID())
        switch v0.(type) {
            case DRegister: sa_v = 0b1000
            default: panic("aarch64: invalid scalar operand size for USRA")
        }
        switch v0.(type) {
            case DRegister: sa_v__bit_mask = 0b1000
            default: panic("aarch64: invalid scalar operand size for USRA")
        }
        sa_n := uint32(v1.(asm.Register).ID())
        switch sa_v {
            case 0b1000: sa_shift_1 = 128 - uint32(asLit(v2))
            default: panic("aarch64: invalid operand 'sa_shift_1' for USRA")
        }
        if maskp(sa_shift_1, 3, 4) != sa_v & sa_v__bit_mask {
            panic("aarch64: invalid combination of operands for USRA")
        }
        return p.setins(asisdshf(1, maskp(sa_shift_1, 3, 4), mask(sa_shift_1, 3), 2, sa_n, sa_d))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for USRA")
}

// USUBL instruction have one single form:
//
//   * USUBL  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) USUBL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("USUBL", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for USUBL")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for USUBL")
        }
        return p.setins(asimddiff(0, 1, sa_ta, sa_vm, 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for USUBL")
}

// USUBL2 instruction have one single form:
//
//   * USUBL2  <Vd>.<Ta>, <Vn>.<Tb>, <Vm>.<Tb>
//
func (self *Program) USUBL2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("USUBL2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v1) == vfmt(v2) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vm := uint32(v2.(asm.Register).ID())
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for USUBL2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for USUBL2")
        }
        return p.setins(asimddiff(1, 1, sa_ta, sa_vm, 2, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for USUBL2")
}

// USUBW instruction have one single form:
//
//   * USUBW  <Vd>.<Ta>, <Vn>.<Ta>, <Vm>.<Tb>
//
func (self *Program) USUBW(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("USUBW", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        switch vfmt(v2) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for USUBW")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for USUBW")
        }
        return p.setins(asimddiff(0, 1, sa_ta, sa_vm, 3, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for USUBW")
}

// USUBW2 instruction have one single form:
//
//   * USUBW2  <Vd>.<Ta>, <Vn>.<Ta>, <Vm>.<Tb>
//
func (self *Program) USUBW2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("USUBW2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8H, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       vfmt(v0) == vfmt(v1) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        switch vfmt(v2) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for USUBW2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for USUBW2")
        }
        return p.setins(asimddiff(1, 1, sa_ta, sa_vm, 3, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for USUBW2")
}

// UZP1 instruction have one single form:
//
//   * UZP1  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) UZP1(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UZP1", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdperm(sa_t & 1, maskp(sa_t, 1, 2), sa_vm, 1, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UZP1")
}

// UZP2 instruction have one single form:
//
//   * UZP2  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) UZP2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UZP2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdperm(sa_t & 1, maskp(sa_t, 1, 2), sa_vm, 5, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UZP2")
}

// WFE instruction have one single form:
//
//   * WFE
//
func (self *Program) WFE() *Instruction {
    p := self.alloc("WFE", 0, Operands {})
    return p.setins(hints(0, 2))
}

// WFET instruction have one single form:
//
//   * WFET  <Xt>
//
func (self *Program) WFET(v0 interface{}) *Instruction {
    p := self.alloc("WFET", 1, Operands { v0 })
    if isXr(v0) {
        sa_xt := uint32(v0.(asm.Register).ID())
        Rt := uint32(0b00000)
        Rt |= sa_xt
        return p.setins(systeminstrswithreg(0, 0, Rt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for WFET")
}

// WFI instruction have one single form:
//
//   * WFI
//
func (self *Program) WFI() *Instruction {
    p := self.alloc("WFI", 0, Operands {})
    return p.setins(hints(0, 3))
}

// WFIT instruction have one single form:
//
//   * WFIT  <Xt>
//
func (self *Program) WFIT(v0 interface{}) *Instruction {
    p := self.alloc("WFIT", 1, Operands { v0 })
    if isXr(v0) {
        sa_xt := uint32(v0.(asm.Register).ID())
        Rt := uint32(0b00000)
        Rt |= sa_xt
        return p.setins(systeminstrswithreg(0, 1, Rt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for WFIT")
}

// XAFLAG instruction have one single form:
//
//   * XAFLAG
//
func (self *Program) XAFLAG() *Instruction {
    p := self.alloc("XAFLAG", 0, Operands {})
    return p.setins(pstate(0, 0, 1, 31))
}

// XAR instruction have one single form:
//
//   * XAR  <Vd>.2D, <Vn>.2D, <Vm>.2D, #<imm6>
//
func (self *Program) XAR(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("XAR", 4, Operands { v0, v1, v2, v3 })
    if isVr(v0) && vfmt(v0) == Vec2D && isVr(v1) && vfmt(v1) == Vec2D && isVr(v2) && vfmt(v2) == Vec2D && isUimm6(v3) {
        sa_vd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        sa_imm6 := asUimm6(v3)
        return p.setins(crypto3_imm6(sa_vm, sa_imm6, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for XAR")
}

// XPACD instruction have one single form:
//
//   * XPACD  <Xd>
//
func (self *Program) XPACD(v0 interface{}) *Instruction {
    p := self.alloc("XPACD", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 17, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for XPACD")
}

// XPACI instruction have one single form:
//
//   * XPACI  <Xd>
//
func (self *Program) XPACI(v0 interface{}) *Instruction {
    p := self.alloc("XPACI", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 16, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for XPACI")
}

// XPACLRI instruction have one single form:
//
//   * XPACLRI
//
func (self *Program) XPACLRI() *Instruction {
    p := self.alloc("XPACLRI", 0, Operands {})
    return p.setins(hints(0, 7))
}

// XTN instruction have one single form:
//
//   * XTN  <Vd>.<Tb>, <Vn>.<Ta>
//
func (self *Program) XTN(v0, v1 interface{}) *Instruction {
    p := self.alloc("XTN", 2, Operands { v0, v1 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for XTN")
        }
        if sa_tb & 1 != 0 {
            panic("aarch64: invalid combination of operands for XTN")
        }
        return p.setins(asimdmisc(0, 0, sa_ta, 18, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for XTN")
}

// XTN2 instruction have one single form:
//
//   * XTN2  <Vd>.<Tb>, <Vn>.<Ta>
//
func (self *Program) XTN2(v0, v1 interface{}) *Instruction {
    p := self.alloc("XTN2", 2, Operands { v0, v1 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S) &&
       isVr(v1) &&
       isVfmt(v1, Vec8H, Vec4S, Vec2D) {
        var sa_ta uint32
        var sa_tb uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_tb = 0b000
            case Vec16B: sa_tb = 0b001
            case Vec4H: sa_tb = 0b010
            case Vec8H: sa_tb = 0b011
            case Vec2S: sa_tb = 0b100
            case Vec4S: sa_tb = 0b101
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        switch vfmt(v1) {
            case Vec8H: sa_ta = 0b00
            case Vec4S: sa_ta = 0b01
            case Vec2D: sa_ta = 0b10
            default: panic("aarch64: unreachable")
        }
        if sa_ta != maskp(sa_tb, 1, 2) {
            panic("aarch64: invalid combination of operands for XTN2")
        }
        if sa_tb & 1 != 1 {
            panic("aarch64: invalid combination of operands for XTN2")
        }
        return p.setins(asimdmisc(1, 0, sa_ta, 18, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for XTN2")
}

// YIELD instruction have one single form:
//
//   * YIELD
//
func (self *Program) YIELD() *Instruction {
    p := self.alloc("YIELD", 0, Operands {})
    return p.setins(hints(0, 1))
}

// ZIP1 instruction have one single form:
//
//   * ZIP1  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) ZIP1(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ZIP1", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdperm(sa_t & 1, maskp(sa_t, 1, 2), sa_vm, 3, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ZIP1")
}

// ZIP2 instruction have one single form:
//
//   * ZIP2  <Vd>.<T>, <Vn>.<T>, <Vm>.<T>
//
func (self *Program) ZIP2(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ZIP2", 3, Operands { v0, v1, v2 })
    if isVr(v0) &&
       isVfmt(v0, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v1) &&
       isVfmt(v1, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       isVr(v2) &&
       isVfmt(v2, Vec8B, Vec16B, Vec4H, Vec8H, Vec2S, Vec4S, Vec2D) &&
       vfmt(v0) == vfmt(v1) &&
       vfmt(v1) == vfmt(v2) {
        var sa_t uint32
        sa_vd := uint32(v0.(asm.Register).ID())
        switch vfmt(v0) {
            case Vec8B: sa_t = 0b000
            case Vec16B: sa_t = 0b001
            case Vec4H: sa_t = 0b010
            case Vec8H: sa_t = 0b011
            case Vec2S: sa_t = 0b100
            case Vec4S: sa_t = 0b101
            case Vec2D: sa_t = 0b111
            default: panic("aarch64: unreachable")
        }
        sa_vn := uint32(v1.(asm.Register).ID())
        sa_vm := uint32(v2.(asm.Register).ID())
        return p.setins(asimdperm(sa_t & 1, maskp(sa_t, 1, 2), sa_vm, 7, sa_vn, sa_vd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ZIP2")
}
