// Code generated by "mkasm_aarch64.py", DO NOT EDIT.

package aarch64

import (
    `github.com/chenzhuoyu/iasm/asm`
)

const (
    _N_args = 6
)

// ABS instruction have 2 forms:
//
//   * ABS  <Wd>, <Wn>
//   * ABS  <Xd>, <Xn>
//
func (self *Program) ABS(v0, v1 interface{}) *Instruction {
    p := self.alloc("ABS", 2, Operands { v0, v1 })
    // ABS  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 8, sa_wn, sa_wd))
    }
    // ABS  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 8, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ABS")
}

// ADC instruction have 2 forms:
//
//   * ADC  <Wd>, <Wn>, <Wm>
//   * ADC  <Xd>, <Xn>, <Xm>
//
func (self *Program) ADC(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ADC", 3, Operands { v0, v1, v2 })
    // ADC  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(0, 0, 0, sa_wm, sa_wn, sa_wd))
    }
    // ADC  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(1, 0, 0, sa_xm, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADC")
}

// ADCS instruction have 2 forms:
//
//   * ADCS  <Wd>, <Wn>, <Wm>
//   * ADCS  <Xd>, <Xn>, <Xm>
//
func (self *Program) ADCS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ADCS", 3, Operands { v0, v1, v2 })
    // ADCS  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(0, 0, 1, sa_wm, sa_wn, sa_wd))
    }
    // ADCS  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(1, 0, 1, sa_xm, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADCS")
}

// ADD instruction have 6 forms:
//
//   * ADD  <Wd|WSP>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
//   * ADD  <Wd|WSP>, <Wn|WSP>, #<imm>{, <shift>}
//   * ADD  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ADD  <Xd|SP>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
//   * ADD  <Xd|SP>, <Xn|SP>, #<imm>{, <shift>}
//   * ADD  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) ADD(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ADD", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ADD", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ADD takes 3 or 4 operands")
    }
    // ADD  <Wd|WSP>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) &&
       isWrOrWSP(v0) &&
       isWrOrWSP(v1) &&
       isWr(v2) &&
       (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_extend = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_ext(0, 0, 0, 0, sa_wm, sa_extend, sa_amount, sa_wn_wsp, sa_wd_wsp))
    }
    // ADD  <Wd|WSP>, <Wn|WSP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isWrOrWSP(v0) &&
       isWrOrWSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(0, 0, 0, sa_shift, sa_imm, sa_wn_wsp, sa_wd_wsp))
    }
    // ADD  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(0, 0, 0, sa_shift, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ADD  <Xd|SP>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) &&
       isXrOrSP(v0) &&
       isXrOrSP(v1) &&
       isWrOrXr(v2) &&
       (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend_1 uint32
        var sa_r [4]uint32
        var sa_r__bit_mask [4]uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        switch true {
            case isWr(v2): sa_r = [4]uint32{0b000, 0b010, 0b100, 0b110}
            case isXr(v2): sa_r = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        switch true {
            case isWr(v2): sa_r__bit_mask = [4]uint32{0b110, 0b111, 0b110, 0b111}
            case isXr(v2): sa_r__bit_mask = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        if len(vv) == 1 {
            sa_extend_1 = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        if !matchany(sa_extend_1, &sa_r__bit_mask[0], &sa_r[0], 4) {
            panic("aarch64: invalid combination of operands for ADD")
        }
        return p.setins(addsub_ext(1, 0, 0, 0, sa_m, sa_extend_1, sa_amount, sa_xn_sp, sa_xd_sp))
    }
    // ADD  <Xd|SP>, <Xn|SP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isXrOrSP(v0) &&
       isXrOrSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(1, 0, 0, sa_shift, sa_imm, sa_xn_sp, sa_xd_sp))
    }
    // ADD  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(1, 0, 0, sa_shift, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADD")
}

// ADDG instruction have one single form:
//
//   * ADDG  <Xd|SP>, <Xn|SP>, #<uimm6>, #<uimm4>
//
func (self *Program) ADDG(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("ADDG", 4, Operands { v0, v1, v2, v3 })
    if isXrOrSP(v0) && isXrOrSP(v1) && isUimm6(v2) && isUimm4(v3) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_uimm6 := asUimm6(v2)
        sa_uimm4 := asUimm4(v3)
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd_sp
        return p.setins(addsub_immtags(1, 0, 0, sa_uimm6, 0, sa_uimm4, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ADDG")
}

// ADDS instruction have 6 forms:
//
//   * ADDS  <Wd>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
//   * ADDS  <Wd>, <Wn|WSP>, #<imm>{, <shift>}
//   * ADDS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ADDS  <Xd>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
//   * ADDS  <Xd>, <Xn|SP>, #<imm>{, <shift>}
//   * ADDS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) ADDS(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ADDS", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ADDS", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ADDS takes 3 or 4 operands")
    }
    // ADDS  <Wd>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWrOrWSP(v1) && isWr(v2) && (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_extend = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_ext(0, 0, 1, 0, sa_wm, sa_extend, sa_amount, sa_wn_wsp, sa_wd))
    }
    // ADDS  <Wd>, <Wn|WSP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isWr(v0) &&
       isWrOrWSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(0, 0, 1, sa_shift, sa_imm, sa_wn_wsp, sa_wd))
    }
    // ADDS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(0, 0, 1, sa_shift, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ADDS  <Xd>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXrOrSP(v1) && isWrOrXr(v2) && (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend_1 uint32
        var sa_r [4]uint32
        var sa_r__bit_mask [4]uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        switch true {
            case isWr(v2): sa_r = [4]uint32{0b000, 0b010, 0b100, 0b110}
            case isXr(v2): sa_r = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        switch true {
            case isWr(v2): sa_r__bit_mask = [4]uint32{0b110, 0b111, 0b110, 0b111}
            case isXr(v2): sa_r__bit_mask = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        if len(vv) == 1 {
            sa_extend_1 = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        if !matchany(sa_extend_1, &sa_r__bit_mask[0], &sa_r[0], 4) {
            panic("aarch64: invalid combination of operands for ADDS")
        }
        return p.setins(addsub_ext(1, 0, 1, 0, sa_m, sa_extend_1, sa_amount, sa_xn_sp, sa_xd))
    }
    // ADDS  <Xd>, <Xn|SP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isXr(v0) &&
       isXrOrSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(1, 0, 1, sa_shift, sa_imm, sa_xn_sp, sa_xd))
    }
    // ADDS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(1, 0, 1, sa_shift, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ADDS")
}

// ADR instruction have one single form:
//
//   * ADR  <Xd>, <label>
//
func (self *Program) ADR(v0, v1 interface{}) *Instruction {
    p := self.alloc("ADR", 2, Operands { v0, v1 })
    if isXr(v0) && isLabel(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 {
            delta := uint32(sa_label.RelativeTo(pc))
            return pcreladdr(0, delta & 0x3, (delta >> 2) & 0x7ffff, sa_xd)
        })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ADR")
}

// ADRP instruction have one single form:
//
//   * ADRP  <Xd>, <label>
//
func (self *Program) ADRP(v0, v1 interface{}) *Instruction {
    p := self.alloc("ADRP", 2, Operands { v0, v1 })
    if isXr(v0) && isLabel(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 {
            delta := uint32(sa_label.RelativeTo(pc))
            return pcreladdr(1, delta & 0x3, (delta >> 2) & 0x7ffff, sa_xd)
        })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ADRP")
}

// AND instruction have 4 forms:
//
//   * AND  <Wd|WSP>, <Wn>, #<imm>
//   * AND  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * AND  <Xd|SP>, <Xn>, #<imm>
//   * AND  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) AND(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("AND", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("AND", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction AND takes 3 or 4 operands")
    }
    // AND  <Wd|WSP>, <Wn>, #<imm>
    if isWrOrWSP(v0) && isWr(v1) && isMask32(v2) {
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_imm := asMaskOp(v2)
        return p.setins(log_imm(0, 0, 0, (sa_imm >> 6) & 0x3f, sa_imm & 0x3f, sa_wn, sa_wd_wsp))
    }
    // AND  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 0, sa_shift, 0, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // AND  <Xd|SP>, <Xn>, #<imm>
    if isXrOrSP(v0) && isXr(v1) && isMask64(v2) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_imm_1 := asMaskOp(v2)
        return p.setins(log_imm(1, 0, (sa_imm_1 >> 12) & 0x1, (sa_imm_1 >> 6) & 0x3f, sa_imm_1 & 0x3f, sa_xn, sa_xd_sp))
    }
    // AND  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 0, sa_shift, 0, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for AND")
}

// ANDS instruction have 4 forms:
//
//   * ANDS  <Wd>, <Wn>, #<imm>
//   * ANDS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ANDS  <Xd>, <Xn>, #<imm>
//   * ANDS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) ANDS(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ANDS", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ANDS", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ANDS takes 3 or 4 operands")
    }
    // ANDS  <Wd>, <Wn>, #<imm>
    if isWr(v0) && isWr(v1) && isMask32(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_imm := asMaskOp(v2)
        return p.setins(log_imm(0, 3, 0, (sa_imm >> 6) & 0x3f, sa_imm & 0x3f, sa_wn, sa_wd))
    }
    // ANDS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 3, sa_shift, 0, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ANDS  <Xd>, <Xn>, #<imm>
    if isXr(v0) && isXr(v1) && isMask64(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_imm_1 := asMaskOp(v2)
        return p.setins(log_imm(1, 3, (sa_imm_1 >> 12) & 0x1, (sa_imm_1 >> 6) & 0x3f, sa_imm_1 & 0x3f, sa_xn, sa_xd))
    }
    // ANDS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 3, sa_shift, 0, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ANDS")
}

// ASRV instruction have 2 forms:
//
//   * ASRV  <Wd>, <Wn>, <Wm>
//   * ASRV  <Xd>, <Xn>, <Xm>
//
func (self *Program) ASRV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ASRV", 3, Operands { v0, v1, v2 })
    // ASRV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 10, sa_wn, sa_wd))
    }
    // ASRV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 10, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ASRV")
}

// AUTDA instruction have one single form:
//
//   * AUTDA  <Xd>, <Xn|SP>
//
func (self *Program) AUTDA(v0, v1 interface{}) *Instruction {
    p := self.alloc("AUTDA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 6, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTDA")
}

// AUTDB instruction have one single form:
//
//   * AUTDB  <Xd>, <Xn|SP>
//
func (self *Program) AUTDB(v0, v1 interface{}) *Instruction {
    p := self.alloc("AUTDB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 7, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTDB")
}

// AUTDZA instruction have one single form:
//
//   * AUTDZA  <Xd>
//
func (self *Program) AUTDZA(v0 interface{}) *Instruction {
    p := self.alloc("AUTDZA", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 14, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTDZA")
}

// AUTDZB instruction have one single form:
//
//   * AUTDZB  <Xd>
//
func (self *Program) AUTDZB(v0 interface{}) *Instruction {
    p := self.alloc("AUTDZB", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 15, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTDZB")
}

// AUTIA instruction have one single form:
//
//   * AUTIA  <Xd>, <Xn|SP>
//
func (self *Program) AUTIA(v0, v1 interface{}) *Instruction {
    p := self.alloc("AUTIA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 4, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTIA")
}

// AUTIA1716 instruction have one single form:
//
//   * AUTIA1716
//
func (self *Program) AUTIA1716() *Instruction {
    p := self.alloc("AUTIA1716", 0, Operands {})
    return p.setins(hints(1, 4))
}

// AUTIASP instruction have one single form:
//
//   * AUTIASP
//
func (self *Program) AUTIASP() *Instruction {
    p := self.alloc("AUTIASP", 0, Operands {})
    return p.setins(hints(3, 5))
}

// AUTIAZ instruction have one single form:
//
//   * AUTIAZ
//
func (self *Program) AUTIAZ() *Instruction {
    p := self.alloc("AUTIAZ", 0, Operands {})
    return p.setins(hints(3, 4))
}

// AUTIB instruction have one single form:
//
//   * AUTIB  <Xd>, <Xn|SP>
//
func (self *Program) AUTIB(v0, v1 interface{}) *Instruction {
    p := self.alloc("AUTIB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 5, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTIB")
}

// AUTIB1716 instruction have one single form:
//
//   * AUTIB1716
//
func (self *Program) AUTIB1716() *Instruction {
    p := self.alloc("AUTIB1716", 0, Operands {})
    return p.setins(hints(1, 6))
}

// AUTIBSP instruction have one single form:
//
//   * AUTIBSP
//
func (self *Program) AUTIBSP() *Instruction {
    p := self.alloc("AUTIBSP", 0, Operands {})
    return p.setins(hints(3, 7))
}

// AUTIBZ instruction have one single form:
//
//   * AUTIBZ
//
func (self *Program) AUTIBZ() *Instruction {
    p := self.alloc("AUTIBZ", 0, Operands {})
    return p.setins(hints(3, 6))
}

// AUTIZA instruction have one single form:
//
//   * AUTIZA  <Xd>
//
func (self *Program) AUTIZA(v0 interface{}) *Instruction {
    p := self.alloc("AUTIZA", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 12, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTIZA")
}

// AUTIZB instruction have one single form:
//
//   * AUTIZB  <Xd>
//
func (self *Program) AUTIZB(v0 interface{}) *Instruction {
    p := self.alloc("AUTIZB", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 13, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for AUTIZB")
}

// AXFLAG instruction have one single form:
//
//   * AXFLAG
//
func (self *Program) AXFLAG() *Instruction {
    p := self.alloc("AXFLAG", 0, Operands {})
    return p.setins(pstate(0, 0, 2, 31))
}

// BEQ instruction have one single form:
//
//   * B.eq  <label>
//
func (self *Program) BEQ(v0 interface{}) *Instruction {
    p := self.alloc("BEQ", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 0) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BEQ")
}

// BNE instruction have one single form:
//
//   * B.ne  <label>
//
func (self *Program) BNE(v0 interface{}) *Instruction {
    p := self.alloc("BNE", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 1) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BNE")
}

// BCS instruction have one single form:
//
//   * B.cs  <label>
//
func (self *Program) BCS(v0 interface{}) *Instruction {
    p := self.alloc("BCS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 2) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCS")
}

// BHS instruction have one single form:
//
//   * B.hs  <label>
//
func (self *Program) BHS(v0 interface{}) *Instruction {
    p := self.alloc("BHS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 2) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BHS")
}

// BCC instruction have one single form:
//
//   * B.cc  <label>
//
func (self *Program) BCC(v0 interface{}) *Instruction {
    p := self.alloc("BCC", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 3) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCC")
}

// BLO instruction have one single form:
//
//   * B.lo  <label>
//
func (self *Program) BLO(v0 interface{}) *Instruction {
    p := self.alloc("BLO", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 3) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLO")
}

// BMI instruction have one single form:
//
//   * B.mi  <label>
//
func (self *Program) BMI(v0 interface{}) *Instruction {
    p := self.alloc("BMI", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 4) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BMI")
}

// BPL instruction have one single form:
//
//   * B.pl  <label>
//
func (self *Program) BPL(v0 interface{}) *Instruction {
    p := self.alloc("BPL", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 5) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BPL")
}

// BVS instruction have one single form:
//
//   * B.vs  <label>
//
func (self *Program) BVS(v0 interface{}) *Instruction {
    p := self.alloc("BVS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 6) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BVS")
}

// BVC instruction have one single form:
//
//   * B.vc  <label>
//
func (self *Program) BVC(v0 interface{}) *Instruction {
    p := self.alloc("BVC", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 7) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BVC")
}

// BHI instruction have one single form:
//
//   * B.hi  <label>
//
func (self *Program) BHI(v0 interface{}) *Instruction {
    p := self.alloc("BHI", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 8) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BHI")
}

// BLS instruction have one single form:
//
//   * B.ls  <label>
//
func (self *Program) BLS(v0 interface{}) *Instruction {
    p := self.alloc("BLS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 9) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLS")
}

// BGE instruction have one single form:
//
//   * B.ge  <label>
//
func (self *Program) BGE(v0 interface{}) *Instruction {
    p := self.alloc("BGE", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 10) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BGE")
}

// BLT instruction have one single form:
//
//   * B.lt  <label>
//
func (self *Program) BLT(v0 interface{}) *Instruction {
    p := self.alloc("BLT", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 11) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLT")
}

// BGT instruction have one single form:
//
//   * B.gt  <label>
//
func (self *Program) BGT(v0 interface{}) *Instruction {
    p := self.alloc("BGT", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 12) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BGT")
}

// BLE instruction have one single form:
//
//   * B.le  <label>
//
func (self *Program) BLE(v0 interface{}) *Instruction {
    p := self.alloc("BLE", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 13) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLE")
}

// BAL instruction have one single form:
//
//   * B.al  <label>
//
func (self *Program) BAL(v0 interface{}) *Instruction {
    p := self.alloc("BAL", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 0, 14) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BAL")
}

// BCEQ instruction have one single form:
//
//   * BC.eq  <label>
//
func (self *Program) BCEQ(v0 interface{}) *Instruction {
    p := self.alloc("BCEQ", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 0) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCEQ")
}

// BCNE instruction have one single form:
//
//   * BC.ne  <label>
//
func (self *Program) BCNE(v0 interface{}) *Instruction {
    p := self.alloc("BCNE", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 1) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCNE")
}

// BCCS instruction have one single form:
//
//   * BC.cs  <label>
//
func (self *Program) BCCS(v0 interface{}) *Instruction {
    p := self.alloc("BCCS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 2) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCCS")
}

// BCHS instruction have one single form:
//
//   * BC.hs  <label>
//
func (self *Program) BCHS(v0 interface{}) *Instruction {
    p := self.alloc("BCHS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 2) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCHS")
}

// BCCC instruction have one single form:
//
//   * BC.cc  <label>
//
func (self *Program) BCCC(v0 interface{}) *Instruction {
    p := self.alloc("BCCC", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 3) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCCC")
}

// BCLO instruction have one single form:
//
//   * BC.lo  <label>
//
func (self *Program) BCLO(v0 interface{}) *Instruction {
    p := self.alloc("BCLO", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 3) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCLO")
}

// BCMI instruction have one single form:
//
//   * BC.mi  <label>
//
func (self *Program) BCMI(v0 interface{}) *Instruction {
    p := self.alloc("BCMI", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 4) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCMI")
}

// BCPL instruction have one single form:
//
//   * BC.pl  <label>
//
func (self *Program) BCPL(v0 interface{}) *Instruction {
    p := self.alloc("BCPL", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 5) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCPL")
}

// BCVS instruction have one single form:
//
//   * BC.vs  <label>
//
func (self *Program) BCVS(v0 interface{}) *Instruction {
    p := self.alloc("BCVS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 6) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCVS")
}

// BCVC instruction have one single form:
//
//   * BC.vc  <label>
//
func (self *Program) BCVC(v0 interface{}) *Instruction {
    p := self.alloc("BCVC", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 7) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCVC")
}

// BCHI instruction have one single form:
//
//   * BC.hi  <label>
//
func (self *Program) BCHI(v0 interface{}) *Instruction {
    p := self.alloc("BCHI", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 8) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCHI")
}

// BCLS instruction have one single form:
//
//   * BC.ls  <label>
//
func (self *Program) BCLS(v0 interface{}) *Instruction {
    p := self.alloc("BCLS", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 9) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCLS")
}

// BCGE instruction have one single form:
//
//   * BC.ge  <label>
//
func (self *Program) BCGE(v0 interface{}) *Instruction {
    p := self.alloc("BCGE", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 10) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCGE")
}

// BCLT instruction have one single form:
//
//   * BC.lt  <label>
//
func (self *Program) BCLT(v0 interface{}) *Instruction {
    p := self.alloc("BCLT", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 11) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCLT")
}

// BCGT instruction have one single form:
//
//   * BC.gt  <label>
//
func (self *Program) BCGT(v0 interface{}) *Instruction {
    p := self.alloc("BCGT", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 12) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCGT")
}

// BCLE instruction have one single form:
//
//   * BC.le  <label>
//
func (self *Program) BCLE(v0 interface{}) *Instruction {
    p := self.alloc("BCLE", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 13) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCLE")
}

// BCAL instruction have one single form:
//
//   * BC.al  <label>
//
func (self *Program) BCAL(v0 interface{}) *Instruction {
    p := self.alloc("BCAL", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return condbranch(0, uint32(sa_label.RelativeTo(pc)), 1, 14) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BCAL")
}

// BFCVT instruction have one single form:
//
//   * BFCVT  <Hd>, <Sn>
//
func (self *Program) BFCVT(v0, v1 interface{}) *Instruction {
    p := self.alloc("BFCVT", 2, Operands { v0, v1 })
    if isHr(v0) && isSr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 6, sa_sn, sa_hd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BFCVT")
}

// BFM instruction have 2 forms:
//
//   * BFM  <Wd>, <Wn>, #<immr>, #<imms>
//   * BFM  <Xd>, <Xn>, #<immr>, #<imms>
//
func (self *Program) BFM(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("BFM", 4, Operands { v0, v1, v2, v3 })
    // BFM  <Wd>, <Wn>, #<immr>, #<imms>
    if isWr(v0) && isWr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_immr := asUimm6(v2)
        sa_imms := asUimm6(v3)
        return p.setins(bitfield(0, 1, 0, sa_immr, sa_imms, sa_wn, sa_wd))
    }
    // BFM  <Xd>, <Xn>, #<immr>, #<imms>
    if isXr(v0) && isXr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_immr_1 := asUimm6(v2)
        sa_imms_1 := asUimm6(v3)
        return p.setins(bitfield(1, 1, 1, sa_immr_1, sa_imms_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for BFM")
}

// BIC instruction have 2 forms:
//
//   * BIC  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * BIC  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) BIC(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("BIC", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("BIC", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction BIC takes 3 or 4 operands")
    }
    // BIC  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 0, sa_shift, 1, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // BIC  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 0, sa_shift, 1, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for BIC")
}

// BICS instruction have 2 forms:
//
//   * BICS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * BICS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) BICS(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("BICS", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("BICS", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction BICS takes 3 or 4 operands")
    }
    // BICS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 3, sa_shift, 1, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // BICS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 3, sa_shift, 1, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for BICS")
}

// BL instruction have one single form:
//
//   * BL  <label>
//
func (self *Program) BL(v0 interface{}) *Instruction {
    p := self.alloc("BL", 1, Operands { v0 })
    if isLabel(v0) {
        sa_label := v0.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return branch_imm(1, uint32(sa_label.RelativeTo(pc))) })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BL")
}

// BLR instruction have one single form:
//
//   * BLR  <Xn>
//
func (self *Program) BLR(v0 interface{}) *Instruction {
    p := self.alloc("BLR", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(1, 31, 0, sa_xn, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLR")
}

// BLRAA instruction have one single form:
//
//   * BLRAA  <Xn>, <Xm|SP>
//
func (self *Program) BLRAA(v0, v1 interface{}) *Instruction {
    p := self.alloc("BLRAA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm_sp := uint32(v1.(asm.Register).ID())
        op4 := uint32(0b00000)
        op4 |= sa_xm_sp
        return p.setins(branch_reg(9, 31, 2, sa_xn, op4))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLRAA")
}

// BLRAAZ instruction have one single form:
//
//   * BLRAAZ  <Xn>
//
func (self *Program) BLRAAZ(v0 interface{}) *Instruction {
    p := self.alloc("BLRAAZ", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(1, 31, 2, sa_xn, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLRAAZ")
}

// BLRAB instruction have one single form:
//
//   * BLRAB  <Xn>, <Xm|SP>
//
func (self *Program) BLRAB(v0, v1 interface{}) *Instruction {
    p := self.alloc("BLRAB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm_sp := uint32(v1.(asm.Register).ID())
        op4 := uint32(0b00000)
        op4 |= sa_xm_sp
        return p.setins(branch_reg(9, 31, 3, sa_xn, op4))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLRAB")
}

// BLRABZ instruction have one single form:
//
//   * BLRABZ  <Xn>
//
func (self *Program) BLRABZ(v0 interface{}) *Instruction {
    p := self.alloc("BLRABZ", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(1, 31, 3, sa_xn, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BLRABZ")
}

// BR instruction have one single form:
//
//   * BR  <Xn>
//
func (self *Program) BR(v0 interface{}) *Instruction {
    p := self.alloc("BR", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(0, 31, 0, sa_xn, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BR")
}

// BRAA instruction have one single form:
//
//   * BRAA  <Xn>, <Xm|SP>
//
func (self *Program) BRAA(v0, v1 interface{}) *Instruction {
    p := self.alloc("BRAA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm_sp := uint32(v1.(asm.Register).ID())
        op4 := uint32(0b00000)
        op4 |= sa_xm_sp
        return p.setins(branch_reg(8, 31, 2, sa_xn, op4))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRAA")
}

// BRAAZ instruction have one single form:
//
//   * BRAAZ  <Xn>
//
func (self *Program) BRAAZ(v0 interface{}) *Instruction {
    p := self.alloc("BRAAZ", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(0, 31, 2, sa_xn, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRAAZ")
}

// BRAB instruction have one single form:
//
//   * BRAB  <Xn>, <Xm|SP>
//
func (self *Program) BRAB(v0, v1 interface{}) *Instruction {
    p := self.alloc("BRAB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm_sp := uint32(v1.(asm.Register).ID())
        op4 := uint32(0b00000)
        op4 |= sa_xm_sp
        return p.setins(branch_reg(8, 31, 3, sa_xn, op4))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRAB")
}

// BRABZ instruction have one single form:
//
//   * BRABZ  <Xn>
//
func (self *Program) BRABZ(v0 interface{}) *Instruction {
    p := self.alloc("BRABZ", 1, Operands { v0 })
    if isXr(v0) {
        sa_xn := uint32(v0.(asm.Register).ID())
        return p.setins(branch_reg(0, 31, 3, sa_xn, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRABZ")
}

// BRK instruction have one single form:
//
//   * BRK  #<imm>
//
func (self *Program) BRK(v0 interface{}) *Instruction {
    p := self.alloc("BRK", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(1, sa_imm, 0, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BRK")
}

// BTI instruction have one single form:
//
//   * BTI  {<targets>}
//
func (self *Program) BTI(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("BTI", 0, Operands {})
        case 1  : p = self.alloc("BTI", 1, Operands { vv[0] })
        default : panic("instruction BTI takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isTargets(vv[0])) {
        sa_targets := _BrOmitted
        if len(vv) == 1 {
            sa_targets = vv[0].(BranchTarget)
        }
        op2 := uint32(0b000)
        switch sa_targets {
            case _BrOmitted: op2 |= 0b00 << 1
            case BrC: op2 |= 0b01 << 1
            case BrJ: op2 |= 0b10 << 1
            case BrJC: op2 |= 0b11 << 1
            default: panic("aarch64: invalid combination of operands for BTI")
        }
        return p.setins(hints(4, op2))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for BTI")
}

// CAS instruction have 2 forms:
//
//   * CAS  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * CAS  <Xs>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) CAS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CAS", 3, Operands { v0, v1, v2 })
    // CAS  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(2, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    // CAS  <Xs>, <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(3, 0, sa_xs, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CAS")
}

// CASA instruction have 2 forms:
//
//   * CASA  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * CASA  <Xs>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASA", 3, Operands { v0, v1, v2 })
    // CASA  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(2, 1, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    // CASA  <Xs>, <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(3, 1, sa_xs, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASA")
}

// CASAB instruction have one single form:
//
//   * CASAB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(0, 1, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASAB")
}

// CASAH instruction have one single form:
//
//   * CASAH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(1, 1, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASAH")
}

// CASAL instruction have 2 forms:
//
//   * CASAL  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * CASAL  <Xs>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASAL", 3, Operands { v0, v1, v2 })
    // CASAL  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(2, 1, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    // CASAL  <Xs>, <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(3, 1, sa_xs, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASAL")
}

// CASALB instruction have one single form:
//
//   * CASALB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(0, 1, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASALB")
}

// CASALH instruction have one single form:
//
//   * CASALH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(1, 1, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASALH")
}

// CASB instruction have one single form:
//
//   * CASB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(0, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASB")
}

// CASH instruction have one single form:
//
//   * CASH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(1, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASH")
}

// CASL instruction have 2 forms:
//
//   * CASL  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * CASL  <Xs>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASL", 3, Operands { v0, v1, v2 })
    // CASL  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(2, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    // CASL  <Xs>, <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(3, 0, sa_xs, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASL")
}

// CASLB instruction have one single form:
//
//   * CASLB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(0, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASLB")
}

// CASLH instruction have one single form:
//
//   * CASLH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) CASLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CASLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(comswap(1, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CASLH")
}

// CASP instruction have 2 forms:
//
//   * CASP  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
//   * CASP  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
//
func (self *Program) CASP(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("CASP", 5, Operands { v0, v1, v2, v3, v4 })
    // CASP  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isNextReg(v1, v0, 1) &&
       isWr(v2) &&
       isWr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(0, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    // CASP  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(1, 0, sa_xs, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASP")
}

// CASPA instruction have 2 forms:
//
//   * CASPA  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
//   * CASPA  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
//
func (self *Program) CASPA(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("CASPA", 5, Operands { v0, v1, v2, v3, v4 })
    // CASPA  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isNextReg(v1, v0, 1) &&
       isWr(v2) &&
       isWr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(0, 1, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    // CASPA  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(1, 1, sa_xs, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASPA")
}

// CASPAL instruction have 2 forms:
//
//   * CASPAL  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
//   * CASPAL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
//
func (self *Program) CASPAL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("CASPAL", 5, Operands { v0, v1, v2, v3, v4 })
    // CASPAL  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isNextReg(v1, v0, 1) &&
       isWr(v2) &&
       isWr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(0, 1, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    // CASPAL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(1, 1, sa_xs, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASPAL")
}

// CASPL instruction have 2 forms:
//
//   * CASPL  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
//   * CASPL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
//
func (self *Program) CASPL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("CASPL", 5, Operands { v0, v1, v2, v3, v4 })
    // CASPL  <Ws>, <W(s+1)>, <Wt>, <W(t+1)>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isNextReg(v1, v0, 1) &&
       isWr(v2) &&
       isWr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(0, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    // CASPL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       (moffs(v4) == 0 || moffs(v4) == 0) &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(comswappr(1, 0, sa_xs, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CASPL")
}

// CBNZ instruction have 2 forms:
//
//   * CBNZ  <Wt>, <label>
//   * CBNZ  <Xt>, <label>
//
func (self *Program) CBNZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("CBNZ", 2, Operands { v0, v1 })
    // CBNZ  <Wt>, <label>
    if isWr(v0) && isLabel(v1) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return compbranch(0, 1, uint32(sa_label.RelativeTo(pc)), sa_wt) })
    }
    // CBNZ  <Xt>, <label>
    if isXr(v0) && isLabel(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return compbranch(1, 1, uint32(sa_label.RelativeTo(pc)), sa_xt) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CBNZ")
}

// CBZ instruction have 2 forms:
//
//   * CBZ  <Wt>, <label>
//   * CBZ  <Xt>, <label>
//
func (self *Program) CBZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("CBZ", 2, Operands { v0, v1 })
    // CBZ  <Wt>, <label>
    if isWr(v0) && isLabel(v1) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return compbranch(0, 0, uint32(sa_label.RelativeTo(pc)), sa_wt) })
    }
    // CBZ  <Xt>, <label>
    if isXr(v0) && isLabel(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return compbranch(1, 0, uint32(sa_label.RelativeTo(pc)), sa_xt) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CBZ")
}

// CCMN instruction have 4 forms:
//
//   * CCMN  <Wn>, #<imm>, #<nzcv>, <cond>
//   * CCMN  <Wn>, <Wm>, #<nzcv>, <cond>
//   * CCMN  <Xn>, #<imm>, #<nzcv>, <cond>
//   * CCMN  <Xn>, <Xm>, #<nzcv>, <cond>
//
func (self *Program) CCMN(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CCMN", 4, Operands { v0, v1, v2, v3 })
    // CCMN  <Wn>, #<imm>, #<nzcv>, <cond>
    if isWr(v0) && isUimm5(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_wn := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm5(v1)
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_imm(0, 0, 1, sa_imm, sa_cond, 0, sa_wn, 0, sa_nzcv))
    }
    // CCMN  <Wn>, <Wm>, #<nzcv>, <cond>
    if isWr(v0) && isWr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_wn := uint32(v0.(asm.Register).ID())
        sa_wm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_reg(0, 0, 1, sa_wm, sa_cond, 0, sa_wn, 0, sa_nzcv))
    }
    // CCMN  <Xn>, #<imm>, #<nzcv>, <cond>
    if isXr(v0) && isUimm5(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm5(v1)
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_imm(1, 0, 1, sa_imm, sa_cond, 0, sa_xn, 0, sa_nzcv))
    }
    // CCMN  <Xn>, <Xm>, #<nzcv>, <cond>
    if isXr(v0) && isXr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_reg(1, 0, 1, sa_xm, sa_cond, 0, sa_xn, 0, sa_nzcv))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CCMN")
}

// CCMP instruction have 4 forms:
//
//   * CCMP  <Wn>, #<imm>, #<nzcv>, <cond>
//   * CCMP  <Wn>, <Wm>, #<nzcv>, <cond>
//   * CCMP  <Xn>, #<imm>, #<nzcv>, <cond>
//   * CCMP  <Xn>, <Xm>, #<nzcv>, <cond>
//
func (self *Program) CCMP(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CCMP", 4, Operands { v0, v1, v2, v3 })
    // CCMP  <Wn>, #<imm>, #<nzcv>, <cond>
    if isWr(v0) && isUimm5(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_wn := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm5(v1)
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_imm(0, 1, 1, sa_imm, sa_cond, 0, sa_wn, 0, sa_nzcv))
    }
    // CCMP  <Wn>, <Wm>, #<nzcv>, <cond>
    if isWr(v0) && isWr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_wn := uint32(v0.(asm.Register).ID())
        sa_wm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_reg(0, 1, 1, sa_wm, sa_cond, 0, sa_wn, 0, sa_nzcv))
    }
    // CCMP  <Xn>, #<imm>, #<nzcv>, <cond>
    if isXr(v0) && isUimm5(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm5(v1)
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_imm(1, 1, 1, sa_imm, sa_cond, 0, sa_xn, 0, sa_nzcv))
    }
    // CCMP  <Xn>, <Xm>, #<nzcv>, <cond>
    if isXr(v0) && isXr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_xm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condcmp_reg(1, 1, 1, sa_xm, sa_cond, 0, sa_xn, 0, sa_nzcv))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CCMP")
}

// CFINV instruction have one single form:
//
//   * CFINV
//
func (self *Program) CFINV() *Instruction {
    p := self.alloc("CFINV", 0, Operands {})
    return p.setins(pstate(0, 0, 0, 31))
}

// CHKFEAT instruction have one single form:
//
//   * CHKFEAT
//
func (self *Program) CHKFEAT() *Instruction {
    p := self.alloc("CHKFEAT", 0, Operands {})
    return p.setins(hints(5, 0))
}

// CLRBHB instruction have one single form:
//
//   * CLRBHB
//
func (self *Program) CLRBHB() *Instruction {
    p := self.alloc("CLRBHB", 0, Operands {})
    return p.setins(hints(2, 6))
}

// CLREX instruction have one single form:
//
//   * CLREX  {#<imm>}
//
func (self *Program) CLREX(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("CLREX", 0, Operands {})
        case 1  : p = self.alloc("CLREX", 1, Operands { vv[0] })
        default : panic("instruction CLREX takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isUimm4(vv[0])) {
        var sa_imm uint32
        if len(vv) == 1 {
            sa_imm = asUimm4(vv[0])
        }
        return p.setins(barriers(sa_imm, 2, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CLREX")
}

// CLS instruction have 2 forms:
//
//   * CLS  <Wd>, <Wn>
//   * CLS  <Xd>, <Xn>
//
func (self *Program) CLS(v0, v1 interface{}) *Instruction {
    p := self.alloc("CLS", 2, Operands { v0, v1 })
    // CLS  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 5, sa_wn, sa_wd))
    }
    // CLS  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 5, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CLS")
}

// CLZ instruction have 2 forms:
//
//   * CLZ  <Wd>, <Wn>
//   * CLZ  <Xd>, <Xn>
//
func (self *Program) CLZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("CLZ", 2, Operands { v0, v1 })
    // CLZ  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 4, sa_wn, sa_wd))
    }
    // CLZ  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 4, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CLZ")
}

// CNT instruction have 2 forms:
//
//   * CNT  <Wd>, <Wn>
//   * CNT  <Xd>, <Xn>
//
func (self *Program) CNT(v0, v1 interface{}) *Instruction {
    p := self.alloc("CNT", 2, Operands { v0, v1 })
    // CNT  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 7, sa_wn, sa_wd))
    }
    // CNT  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 7, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CNT")
}

// CPYE instruction have one single form:
//
//   * CPYE  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYE", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 0, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYE")
}

// CPYEN instruction have one single form:
//
//   * CPYEN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 12, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEN")
}

// CPYERN instruction have one single form:
//
//   * CPYERN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 8, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERN")
}

// CPYERT instruction have one single form:
//
//   * CPYERT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 2, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERT")
}

// CPYERTN instruction have one single form:
//
//   * CPYERTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 14, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERTN")
}

// CPYERTRN instruction have one single form:
//
//   * CPYERTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 10, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERTRN")
}

// CPYERTWN instruction have one single form:
//
//   * CPYERTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYERTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYERTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 6, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYERTWN")
}

// CPYET instruction have one single form:
//
//   * CPYET  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYET", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 3, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYET")
}

// CPYETN instruction have one single form:
//
//   * CPYETN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYETN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYETN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 15, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYETN")
}

// CPYETRN instruction have one single form:
//
//   * CPYETRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYETRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYETRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 11, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYETRN")
}

// CPYETWN instruction have one single form:
//
//   * CPYETWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYETWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYETWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 7, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYETWN")
}

// CPYEWN instruction have one single form:
//
//   * CPYEWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 4, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWN")
}

// CPYEWT instruction have one single form:
//
//   * CPYEWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 1, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWT")
}

// CPYEWTN instruction have one single form:
//
//   * CPYEWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 13, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWTN")
}

// CPYEWTRN instruction have one single form:
//
//   * CPYEWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 9, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWTRN")
}

// CPYEWTWN instruction have one single form:
//
//   * CPYEWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYEWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYEWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 2, sa_xs_1, 5, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYEWTWN")
}

// CPYFE instruction have one single form:
//
//   * CPYFE  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFE", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 0, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFE")
}

// CPYFEN instruction have one single form:
//
//   * CPYFEN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 12, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEN")
}

// CPYFERN instruction have one single form:
//
//   * CPYFERN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 8, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERN")
}

// CPYFERT instruction have one single form:
//
//   * CPYFERT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 2, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERT")
}

// CPYFERTN instruction have one single form:
//
//   * CPYFERTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 14, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERTN")
}

// CPYFERTRN instruction have one single form:
//
//   * CPYFERTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 10, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERTRN")
}

// CPYFERTWN instruction have one single form:
//
//   * CPYFERTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFERTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFERTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 6, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFERTWN")
}

// CPYFET instruction have one single form:
//
//   * CPYFET  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFET", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 3, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFET")
}

// CPYFETN instruction have one single form:
//
//   * CPYFETN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFETN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFETN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 15, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFETN")
}

// CPYFETRN instruction have one single form:
//
//   * CPYFETRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFETRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFETRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 11, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFETRN")
}

// CPYFETWN instruction have one single form:
//
//   * CPYFETWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFETWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFETWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 7, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFETWN")
}

// CPYFEWN instruction have one single form:
//
//   * CPYFEWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 4, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWN")
}

// CPYFEWT instruction have one single form:
//
//   * CPYFEWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 1, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWT")
}

// CPYFEWTN instruction have one single form:
//
//   * CPYFEWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 13, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWTN")
}

// CPYFEWTRN instruction have one single form:
//
//   * CPYFEWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 9, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWTRN")
}

// CPYFEWTWN instruction have one single form:
//
//   * CPYFEWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFEWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFEWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_2 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 2, sa_xs_1, 5, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFEWTWN")
}

// CPYFM instruction have one single form:
//
//   * CPYFM  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFM", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 0, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFM")
}

// CPYFMN instruction have one single form:
//
//   * CPYFMN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 12, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMN")
}

// CPYFMRN instruction have one single form:
//
//   * CPYFMRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 8, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRN")
}

// CPYFMRT instruction have one single form:
//
//   * CPYFMRT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 2, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRT")
}

// CPYFMRTN instruction have one single form:
//
//   * CPYFMRTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 14, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRTN")
}

// CPYFMRTRN instruction have one single form:
//
//   * CPYFMRTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 10, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRTRN")
}

// CPYFMRTWN instruction have one single form:
//
//   * CPYFMRTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMRTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMRTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 6, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMRTWN")
}

// CPYFMT instruction have one single form:
//
//   * CPYFMT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 3, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMT")
}

// CPYFMTN instruction have one single form:
//
//   * CPYFMTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 15, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMTN")
}

// CPYFMTRN instruction have one single form:
//
//   * CPYFMTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 11, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMTRN")
}

// CPYFMTWN instruction have one single form:
//
//   * CPYFMTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 7, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMTWN")
}

// CPYFMWN instruction have one single form:
//
//   * CPYFMWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 4, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWN")
}

// CPYFMWT instruction have one single form:
//
//   * CPYFMWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 1, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWT")
}

// CPYFMWTN instruction have one single form:
//
//   * CPYFMWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 13, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWTN")
}

// CPYFMWTRN instruction have one single form:
//
//   * CPYFMWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 9, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWTRN")
}

// CPYFMWTWN instruction have one single form:
//
//   * CPYFMWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFMWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFMWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 1, sa_xs_1, 5, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFMWTWN")
}

// CPYFP instruction have one single form:
//
//   * CPYFP  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFP", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 0, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFP")
}

// CPYFPN instruction have one single form:
//
//   * CPYFPN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 12, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPN")
}

// CPYFPRN instruction have one single form:
//
//   * CPYFPRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 8, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRN")
}

// CPYFPRT instruction have one single form:
//
//   * CPYFPRT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 2, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRT")
}

// CPYFPRTN instruction have one single form:
//
//   * CPYFPRTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 14, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRTN")
}

// CPYFPRTRN instruction have one single form:
//
//   * CPYFPRTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 10, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRTRN")
}

// CPYFPRTWN instruction have one single form:
//
//   * CPYFPRTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPRTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPRTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 6, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPRTWN")
}

// CPYFPT instruction have one single form:
//
//   * CPYFPT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 3, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPT")
}

// CPYFPTN instruction have one single form:
//
//   * CPYFPTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 15, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPTN")
}

// CPYFPTRN instruction have one single form:
//
//   * CPYFPTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 11, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPTRN")
}

// CPYFPTWN instruction have one single form:
//
//   * CPYFPTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 7, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPTWN")
}

// CPYFPWN instruction have one single form:
//
//   * CPYFPWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 4, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWN")
}

// CPYFPWT instruction have one single form:
//
//   * CPYFPWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 1, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWT")
}

// CPYFPWTN instruction have one single form:
//
//   * CPYFPWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 13, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWTN")
}

// CPYFPWTRN instruction have one single form:
//
//   * CPYFPWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 9, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWTRN")
}

// CPYFPWTWN instruction have one single form:
//
//   * CPYFPWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYFPWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYFPWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 0, sa_xs, 5, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYFPWTWN")
}

// CPYM instruction have one single form:
//
//   * CPYM  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYM", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 0, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYM")
}

// CPYMN instruction have one single form:
//
//   * CPYMN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 12, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMN")
}

// CPYMRN instruction have one single form:
//
//   * CPYMRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 8, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRN")
}

// CPYMRT instruction have one single form:
//
//   * CPYMRT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 2, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRT")
}

// CPYMRTN instruction have one single form:
//
//   * CPYMRTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 14, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRTN")
}

// CPYMRTRN instruction have one single form:
//
//   * CPYMRTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 10, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRTRN")
}

// CPYMRTWN instruction have one single form:
//
//   * CPYMRTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMRTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMRTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 6, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMRTWN")
}

// CPYMT instruction have one single form:
//
//   * CPYMT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 3, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMT")
}

// CPYMTN instruction have one single form:
//
//   * CPYMTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 15, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMTN")
}

// CPYMTRN instruction have one single form:
//
//   * CPYMTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 11, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMTRN")
}

// CPYMTWN instruction have one single form:
//
//   * CPYMTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 7, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMTWN")
}

// CPYMWN instruction have one single form:
//
//   * CPYMWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 4, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWN")
}

// CPYMWT instruction have one single form:
//
//   * CPYMWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 1, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWT")
}

// CPYMWTN instruction have one single form:
//
//   * CPYMWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 13, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWTN")
}

// CPYMWTRN instruction have one single form:
//
//   * CPYMWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 9, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWTRN")
}

// CPYMWTWN instruction have one single form:
//
//   * CPYMWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYMWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYMWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xs_1 := uint32(mbase(v1).ID())
        sa_xn_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 1, sa_xs_1, 5, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYMWTWN")
}

// CPYP instruction have one single form:
//
//   * CPYP  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYP", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 0, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYP")
}

// CPYPN instruction have one single form:
//
//   * CPYPN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 12, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPN")
}

// CPYPRN instruction have one single form:
//
//   * CPYPRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 8, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRN")
}

// CPYPRT instruction have one single form:
//
//   * CPYPRT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 2, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRT")
}

// CPYPRTN instruction have one single form:
//
//   * CPYPRTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 14, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRTN")
}

// CPYPRTRN instruction have one single form:
//
//   * CPYPRTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 10, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRTRN")
}

// CPYPRTWN instruction have one single form:
//
//   * CPYPRTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPRTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPRTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 6, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPRTWN")
}

// CPYPT instruction have one single form:
//
//   * CPYPT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 3, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPT")
}

// CPYPTN instruction have one single form:
//
//   * CPYPTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 15, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPTN")
}

// CPYPTRN instruction have one single form:
//
//   * CPYPTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 11, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPTRN")
}

// CPYPTWN instruction have one single form:
//
//   * CPYPTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 7, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPTWN")
}

// CPYPWN instruction have one single form:
//
//   * CPYPWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 4, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWN")
}

// CPYPWT instruction have one single form:
//
//   * CPYPWT  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 1, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWT")
}

// CPYPWTN instruction have one single form:
//
//   * CPYPWTN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 13, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWTN")
}

// CPYPWTRN instruction have one single form:
//
//   * CPYPWTRN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWTRN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWTRN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 9, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWTRN")
}

// CPYPWTWN instruction have one single form:
//
//   * CPYPWTWN  [<Xd>]!, [<Xs>]!, <Xn>!
//
func (self *Program) CPYPWTWN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CPYPWTWN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isMem(v1) &&
       isXr(mbase(v1)) &&
       midx(v1) == nil &&
       moffs(v1) == 0 &&
       mext(v1) == PreIndex &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xs := uint32(mbase(v1).ID())
        sa_xn := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 0, sa_xs, 5, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CPYPWTWN")
}

// CRC32B instruction have one single form:
//
//   * CRC32B  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32B(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32B", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 16, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32B")
}

// CRC32CB instruction have one single form:
//
//   * CRC32CB  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32CB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32CB", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 20, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32CB")
}

// CRC32CH instruction have one single form:
//
//   * CRC32CH  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32CH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32CH", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 21, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32CH")
}

// CRC32CW instruction have one single form:
//
//   * CRC32CW  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32CW(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32CW", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 22, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32CW")
}

// CRC32CX instruction have one single form:
//
//   * CRC32CX  <Wd>, <Wn>, <Xm>
//
func (self *Program) CRC32CX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32CX", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isXr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 23, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32CX")
}

// CRC32H instruction have one single form:
//
//   * CRC32H  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32H(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32H", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 17, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32H")
}

// CRC32W instruction have one single form:
//
//   * CRC32W  <Wd>, <Wn>, <Wm>
//
func (self *Program) CRC32W(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32W", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 18, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32W")
}

// CRC32X instruction have one single form:
//
//   * CRC32X  <Wd>, <Wn>, <Xm>
//
func (self *Program) CRC32X(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("CRC32X", 3, Operands { v0, v1, v2 })
    if isWr(v0) && isWr(v1) && isXr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 19, sa_wn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for CRC32X")
}

// CSDB instruction have one single form:
//
//   * CSDB
//
func (self *Program) CSDB() *Instruction {
    p := self.alloc("CSDB", 0, Operands {})
    return p.setins(hints(2, 4))
}

// CSEL instruction have 2 forms:
//
//   * CSEL  <Wd>, <Wn>, <Wm>, <cond>
//   * CSEL  <Xd>, <Xn>, <Xm>, <cond>
//
func (self *Program) CSEL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CSEL", 4, Operands { v0, v1, v2, v3 })
    // CSEL  <Wd>, <Wn>, <Wm>, <cond>
    if isWr(v0) && isWr(v1) && isWr(v2) && isBrCond(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(0, 0, 0, sa_wm, sa_cond, 0, sa_wn, sa_wd))
    }
    // CSEL  <Xd>, <Xn>, <Xm>, <cond>
    if isXr(v0) && isXr(v1) && isXr(v2) && isBrCond(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(1, 0, 0, sa_xm, sa_cond, 0, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CSEL")
}

// CSINC instruction have 2 forms:
//
//   * CSINC  <Wd>, <Wn>, <Wm>, <cond>
//   * CSINC  <Xd>, <Xn>, <Xm>, <cond>
//
func (self *Program) CSINC(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CSINC", 4, Operands { v0, v1, v2, v3 })
    // CSINC  <Wd>, <Wn>, <Wm>, <cond>
    if isWr(v0) && isWr(v1) && isWr(v2) && isBrCond(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(0, 0, 0, sa_wm, sa_cond, 1, sa_wn, sa_wd))
    }
    // CSINC  <Xd>, <Xn>, <Xm>, <cond>
    if isXr(v0) && isXr(v1) && isXr(v2) && isBrCond(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(1, 0, 0, sa_xm, sa_cond, 1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CSINC")
}

// CSINV instruction have 2 forms:
//
//   * CSINV  <Wd>, <Wn>, <Wm>, <cond>
//   * CSINV  <Xd>, <Xn>, <Xm>, <cond>
//
func (self *Program) CSINV(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CSINV", 4, Operands { v0, v1, v2, v3 })
    // CSINV  <Wd>, <Wn>, <Wm>, <cond>
    if isWr(v0) && isWr(v1) && isWr(v2) && isBrCond(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(0, 1, 0, sa_wm, sa_cond, 0, sa_wn, sa_wd))
    }
    // CSINV  <Xd>, <Xn>, <Xm>, <cond>
    if isXr(v0) && isXr(v1) && isXr(v2) && isBrCond(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(1, 1, 0, sa_xm, sa_cond, 0, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CSINV")
}

// CSNEG instruction have 2 forms:
//
//   * CSNEG  <Wd>, <Wn>, <Wm>, <cond>
//   * CSNEG  <Xd>, <Xn>, <Xm>, <cond>
//
func (self *Program) CSNEG(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("CSNEG", 4, Operands { v0, v1, v2, v3 })
    // CSNEG  <Wd>, <Wn>, <Wm>, <cond>
    if isWr(v0) && isWr(v1) && isWr(v2) && isBrCond(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(0, 1, 0, sa_wm, sa_cond, 1, sa_wn, sa_wd))
    }
    // CSNEG  <Xd>, <Xn>, <Xm>, <cond>
    if isXr(v0) && isXr(v1) && isXr(v2) && isBrCond(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(condsel(1, 1, 0, sa_xm, sa_cond, 1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CSNEG")
}

// CTZ instruction have 2 forms:
//
//   * CTZ  <Wd>, <Wn>
//   * CTZ  <Xd>, <Xn>
//
func (self *Program) CTZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("CTZ", 2, Operands { v0, v1 })
    // CTZ  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 6, sa_wn, sa_wd))
    }
    // CTZ  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 6, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for CTZ")
}

// DCPS1 instruction have one single form:
//
//   * DCPS1  {#<imm>}
//
func (self *Program) DCPS1(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("DCPS1", 0, Operands {})
        case 1  : p = self.alloc("DCPS1", 1, Operands { vv[0] })
        default : panic("instruction DCPS1 takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isUimm16(vv[0])) {
        var sa_imm uint32
        if len(vv) == 1 {
            sa_imm = asUimm16(vv[0])
        }
        return p.setins(exception(5, sa_imm, 0, 1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for DCPS1")
}

// DCPS2 instruction have one single form:
//
//   * DCPS2  {#<imm>}
//
func (self *Program) DCPS2(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("DCPS2", 0, Operands {})
        case 1  : p = self.alloc("DCPS2", 1, Operands { vv[0] })
        default : panic("instruction DCPS2 takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isUimm16(vv[0])) {
        var sa_imm uint32
        if len(vv) == 1 {
            sa_imm = asUimm16(vv[0])
        }
        return p.setins(exception(5, sa_imm, 0, 2))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for DCPS2")
}

// DCPS3 instruction have one single form:
//
//   * DCPS3  {#<imm>}
//
func (self *Program) DCPS3(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("DCPS3", 0, Operands {})
        case 1  : p = self.alloc("DCPS3", 1, Operands { vv[0] })
        default : panic("instruction DCPS3 takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isUimm16(vv[0])) {
        var sa_imm uint32
        if len(vv) == 1 {
            sa_imm = asUimm16(vv[0])
        }
        return p.setins(exception(5, sa_imm, 0, 3))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for DCPS3")
}

// DGH instruction have one single form:
//
//   * DGH
//
func (self *Program) DGH() *Instruction {
    p := self.alloc("DGH", 0, Operands {})
    return p.setins(hints(0, 6))
}

// DMB instruction have one single form:
//
//   * DMB  <option>|#<imm>
//
func (self *Program) DMB(v0 interface{}) *Instruction {
    p := self.alloc("DMB", 1, Operands { v0 })
    if isOption(v0) {
        sa_option := v0.(BarrierOption)
        sa_imm := uint32(sa_option)
        return p.setins(barriers(sa_imm, 5, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for DMB")
}

// DRPS instruction have one single form:
//
//   * DRPS
//
func (self *Program) DRPS() *Instruction {
    p := self.alloc("DRPS", 0, Operands {})
    return p.setins(branch_reg(5, 31, 0, 31, 0))
}

// DSB instruction have 2 forms:
//
//   * DSB  <option>|#<imm>
//   * DSB  <option>nXS
//
func (self *Program) DSB(v0 interface{}) *Instruction {
    p := self.alloc("DSB", 1, Operands { v0 })
    // DSB  <option>|#<imm>
    if isOption(v0) {
        sa_option := v0.(BarrierOption)
        sa_imm := uint32(sa_option)
        return p.setins(barriers(sa_imm, 4, 31))
    }
    // DSB  <option>nXS
    if isOptionNXS(v0) {
        sa_option_1 := v0.(BarrierOption).nxs()
        CRm := uint32(0b0010)
        CRm |= sa_option_1 << 2
        return p.setins(barriers(CRm, 1, 31))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for DSB")
}

// EON instruction have 2 forms:
//
//   * EON  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * EON  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) EON(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("EON", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("EON", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction EON takes 3 or 4 operands")
    }
    // EON  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 2, sa_shift, 1, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // EON  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 2, sa_shift, 1, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for EON")
}

// EOR instruction have 4 forms:
//
//   * EOR  <Wd|WSP>, <Wn>, #<imm>
//   * EOR  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * EOR  <Xd|SP>, <Xn>, #<imm>
//   * EOR  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) EOR(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("EOR", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("EOR", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction EOR takes 3 or 4 operands")
    }
    // EOR  <Wd|WSP>, <Wn>, #<imm>
    if isWrOrWSP(v0) && isWr(v1) && isMask32(v2) {
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_imm := asMaskOp(v2)
        return p.setins(log_imm(0, 2, 0, (sa_imm >> 6) & 0x3f, sa_imm & 0x3f, sa_wn, sa_wd_wsp))
    }
    // EOR  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 2, sa_shift, 0, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // EOR  <Xd|SP>, <Xn>, #<imm>
    if isXrOrSP(v0) && isXr(v1) && isMask64(v2) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_imm_1 := asMaskOp(v2)
        return p.setins(log_imm(1, 2, (sa_imm_1 >> 12) & 0x1, (sa_imm_1 >> 6) & 0x3f, sa_imm_1 & 0x3f, sa_xn, sa_xd_sp))
    }
    // EOR  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 2, sa_shift, 0, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for EOR")
}

// ERET instruction have one single form:
//
//   * ERET
//
func (self *Program) ERET() *Instruction {
    p := self.alloc("ERET", 0, Operands {})
    return p.setins(branch_reg(4, 31, 0, 31, 0))
}

// ERETAA instruction have one single form:
//
//   * ERETAA
//
func (self *Program) ERETAA() *Instruction {
    p := self.alloc("ERETAA", 0, Operands {})
    return p.setins(branch_reg(4, 31, 2, 31, 31))
}

// ERETAB instruction have one single form:
//
//   * ERETAB
//
func (self *Program) ERETAB() *Instruction {
    p := self.alloc("ERETAB", 0, Operands {})
    return p.setins(branch_reg(4, 31, 3, 31, 31))
}

// ESB instruction have one single form:
//
//   * ESB
//
func (self *Program) ESB() *Instruction {
    p := self.alloc("ESB", 0, Operands {})
    return p.setins(hints(2, 0))
}

// EXTR instruction have 2 forms:
//
//   * EXTR  <Wd>, <Wn>, <Wm>, #<lsb>
//   * EXTR  <Xd>, <Xn>, <Xm>, #<lsb>
//
func (self *Program) EXTR(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("EXTR", 4, Operands { v0, v1, v2, v3 })
    // EXTR  <Wd>, <Wn>, <Wm>, #<lsb>
    if isWr(v0) && isWr(v1) && isWr(v2) && isUimm6(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_lsb := asUimm6(v3)
        return p.setins(extract(0, 0, 0, 0, sa_wm, sa_lsb, sa_wn, sa_wd))
    }
    // EXTR  <Xd>, <Xn>, <Xm>, #<lsb>
    if isXr(v0) && isXr(v1) && isXr(v2) && isUimm6(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_lsb_1 := asUimm6(v3)
        return p.setins(extract(1, 0, 1, 0, sa_xm, sa_lsb_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for EXTR")
}

// FABS instruction have 3 forms:
//
//   * FABS  <Dd>, <Dn>
//   * FABS  <Hd>, <Hn>
//   * FABS  <Sd>, <Sn>
//
func (self *Program) FABS(v0, v1 interface{}) *Instruction {
    p := self.alloc("FABS", 2, Operands { v0, v1 })
    // FABS  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 1, sa_dn, sa_dd))
    }
    // FABS  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 1, sa_hn, sa_hd))
    }
    // FABS  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 1, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FABS")
}

// FADD instruction have 3 forms:
//
//   * FADD  <Dd>, <Dn>, <Dm>
//   * FADD  <Hd>, <Hn>, <Hm>
//   * FADD  <Sd>, <Sn>, <Sm>
//
func (self *Program) FADD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FADD", 3, Operands { v0, v1, v2 })
    // FADD  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 2, sa_dn, sa_dd))
    }
    // FADD  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 2, sa_hn, sa_hd))
    }
    // FADD  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 2, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FADD")
}

// FCCMP instruction have 3 forms:
//
//   * FCCMP  <Dn>, <Dm>, #<nzcv>, <cond>
//   * FCCMP  <Hn>, <Hm>, #<nzcv>, <cond>
//   * FCCMP  <Sn>, <Sm>, #<nzcv>, <cond>
//
func (self *Program) FCCMP(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FCCMP", 4, Operands { v0, v1, v2, v3 })
    // FCCMP  <Dn>, <Dm>, #<nzcv>, <cond>
    if isDr(v0) && isDr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_dn := uint32(v0.(asm.Register).ID())
        sa_dm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatccmp(0, 0, 1, sa_dm, sa_cond, sa_dn, 0, sa_nzcv))
    }
    // FCCMP  <Hn>, <Hm>, #<nzcv>, <cond>
    if isHr(v0) && isHr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_hn := uint32(v0.(asm.Register).ID())
        sa_hm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatccmp(0, 0, 3, sa_hm, sa_cond, sa_hn, 0, sa_nzcv))
    }
    // FCCMP  <Sn>, <Sm>, #<nzcv>, <cond>
    if isSr(v0) && isSr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_sn := uint32(v0.(asm.Register).ID())
        sa_sm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatccmp(0, 0, 0, sa_sm, sa_cond, sa_sn, 0, sa_nzcv))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCCMP")
}

// FCCMPE instruction have 3 forms:
//
//   * FCCMPE  <Dn>, <Dm>, #<nzcv>, <cond>
//   * FCCMPE  <Hn>, <Hm>, #<nzcv>, <cond>
//   * FCCMPE  <Sn>, <Sm>, #<nzcv>, <cond>
//
func (self *Program) FCCMPE(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FCCMPE", 4, Operands { v0, v1, v2, v3 })
    // FCCMPE  <Dn>, <Dm>, #<nzcv>, <cond>
    if isDr(v0) && isDr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_dn := uint32(v0.(asm.Register).ID())
        sa_dm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatccmp(0, 0, 1, sa_dm, sa_cond, sa_dn, 1, sa_nzcv))
    }
    // FCCMPE  <Hn>, <Hm>, #<nzcv>, <cond>
    if isHr(v0) && isHr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_hn := uint32(v0.(asm.Register).ID())
        sa_hm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatccmp(0, 0, 3, sa_hm, sa_cond, sa_hn, 1, sa_nzcv))
    }
    // FCCMPE  <Sn>, <Sm>, #<nzcv>, <cond>
    if isSr(v0) && isSr(v1) && isUimm4(v2) && isBrCond(v3) {
        sa_sn := uint32(v0.(asm.Register).ID())
        sa_sm := uint32(v1.(asm.Register).ID())
        sa_nzcv := asUimm4(v2)
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatccmp(0, 0, 0, sa_sm, sa_cond, sa_sn, 1, sa_nzcv))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCCMPE")
}

// FCMP instruction have 6 forms:
//
//   * FCMP  <Dn>, #0.0
//   * FCMP  <Dn>, <Dm>
//   * FCMP  <Hn>, #0.0
//   * FCMP  <Hn>, <Hm>
//   * FCMP  <Sn>, #0.0
//   * FCMP  <Sn>, <Sm>
//
func (self *Program) FCMP(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCMP", 2, Operands { v0, v1 })
    // FCMP  <Dn>, #0.0
    if isDr(v0) && isFloatLit(v1, 0.0) {
        sa_dn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 1, 0, 0, sa_dn, 8))
    }
    // FCMP  <Dn>, <Dm>
    if isDr(v0) && isDr(v1) {
        sa_dn_1 := uint32(v0.(asm.Register).ID())
        sa_dm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 1, sa_dm, 0, sa_dn_1, 0))
    }
    // FCMP  <Hn>, #0.0
    if isHr(v0) && isFloatLit(v1, 0.0) {
        sa_hn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 3, 0, 0, sa_hn, 8))
    }
    // FCMP  <Hn>, <Hm>
    if isHr(v0) && isHr(v1) {
        sa_hn_1 := uint32(v0.(asm.Register).ID())
        sa_hm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 3, sa_hm, 0, sa_hn_1, 0))
    }
    // FCMP  <Sn>, #0.0
    if isSr(v0) && isFloatLit(v1, 0.0) {
        sa_sn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 0, 0, 0, sa_sn, 8))
    }
    // FCMP  <Sn>, <Sm>
    if isSr(v0) && isSr(v1) {
        sa_sn_1 := uint32(v0.(asm.Register).ID())
        sa_sm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 0, sa_sm, 0, sa_sn_1, 0))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCMP")
}

// FCMPE instruction have 6 forms:
//
//   * FCMPE  <Dn>, #0.0
//   * FCMPE  <Dn>, <Dm>
//   * FCMPE  <Hn>, #0.0
//   * FCMPE  <Hn>, <Hm>
//   * FCMPE  <Sn>, #0.0
//   * FCMPE  <Sn>, <Sm>
//
func (self *Program) FCMPE(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCMPE", 2, Operands { v0, v1 })
    // FCMPE  <Dn>, #0.0
    if isDr(v0) && isFloatLit(v1, 0.0) {
        sa_dn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 1, 0, 0, sa_dn, 24))
    }
    // FCMPE  <Dn>, <Dm>
    if isDr(v0) && isDr(v1) {
        sa_dn_1 := uint32(v0.(asm.Register).ID())
        sa_dm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 1, sa_dm, 0, sa_dn_1, 16))
    }
    // FCMPE  <Hn>, #0.0
    if isHr(v0) && isFloatLit(v1, 0.0) {
        sa_hn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 3, 0, 0, sa_hn, 24))
    }
    // FCMPE  <Hn>, <Hm>
    if isHr(v0) && isHr(v1) {
        sa_hn_1 := uint32(v0.(asm.Register).ID())
        sa_hm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 3, sa_hm, 0, sa_hn_1, 16))
    }
    // FCMPE  <Sn>, #0.0
    if isSr(v0) && isFloatLit(v1, 0.0) {
        sa_sn := uint32(v0.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 0, 0, 0, sa_sn, 24))
    }
    // FCMPE  <Sn>, <Sm>
    if isSr(v0) && isSr(v1) {
        sa_sn_1 := uint32(v0.(asm.Register).ID())
        sa_sm := uint32(v1.(asm.Register).ID())
        return p.setins(floatcmp(0, 0, 0, sa_sm, 0, sa_sn_1, 16))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCMPE")
}

// FCSEL instruction have 3 forms:
//
//   * FCSEL  <Dd>, <Dn>, <Dm>, <cond>
//   * FCSEL  <Hd>, <Hn>, <Hm>, <cond>
//   * FCSEL  <Sd>, <Sn>, <Sm>, <cond>
//
func (self *Program) FCSEL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FCSEL", 4, Operands { v0, v1, v2, v3 })
    // FCSEL  <Dd>, <Dn>, <Dm>, <cond>
    if isDr(v0) && isDr(v1) && isDr(v2) && isBrCond(v3) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatsel(0, 0, 1, sa_dm, sa_cond, sa_dn, sa_dd))
    }
    // FCSEL  <Hd>, <Hn>, <Hm>, <cond>
    if isHr(v0) && isHr(v1) && isHr(v2) && isBrCond(v3) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatsel(0, 0, 3, sa_hm, sa_cond, sa_hn, sa_hd))
    }
    // FCSEL  <Sd>, <Sn>, <Sm>, <cond>
    if isSr(v0) && isSr(v1) && isSr(v2) && isBrCond(v3) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        sa_cond := uint32(v3.(BranchCondition))
        return p.setins(floatsel(0, 0, 0, sa_sm, sa_cond, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCSEL")
}

// FCVT instruction have 6 forms:
//
//   * FCVT  <Dd>, <Hn>
//   * FCVT  <Dd>, <Sn>
//   * FCVT  <Hd>, <Dn>
//   * FCVT  <Hd>, <Sn>
//   * FCVT  <Sd>, <Dn>
//   * FCVT  <Sd>, <Hn>
//
func (self *Program) FCVT(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVT", 2, Operands { v0, v1 })
    // FCVT  <Dd>, <Hn>
    if isDr(v0) && isHr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 5, sa_hn, sa_dd))
    }
    // FCVT  <Dd>, <Sn>
    if isDr(v0) && isSr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 5, sa_sn, sa_dd))
    }
    // FCVT  <Hd>, <Dn>
    if isHr(v0) && isDr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 7, sa_dn, sa_hd))
    }
    // FCVT  <Hd>, <Sn>
    if isHr(v0) && isSr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 7, sa_sn, sa_hd))
    }
    // FCVT  <Sd>, <Dn>
    if isSr(v0) && isDr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 4, sa_dn, sa_sd))
    }
    // FCVT  <Sd>, <Hn>
    if isSr(v0) && isHr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 4, sa_hn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVT")
}

// FCVTAS instruction have 6 forms:
//
//   * FCVTAS  <Wd>, <Dn>
//   * FCVTAS  <Wd>, <Hn>
//   * FCVTAS  <Wd>, <Sn>
//   * FCVTAS  <Xd>, <Dn>
//   * FCVTAS  <Xd>, <Hn>
//   * FCVTAS  <Xd>, <Sn>
//
func (self *Program) FCVTAS(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTAS", 2, Operands { v0, v1 })
    // FCVTAS  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 4, sa_dn, sa_wd))
    }
    // FCVTAS  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 4, sa_hn, sa_wd))
    }
    // FCVTAS  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 4, sa_sn, sa_wd))
    }
    // FCVTAS  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 4, sa_dn, sa_xd))
    }
    // FCVTAS  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 4, sa_hn, sa_xd))
    }
    // FCVTAS  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 4, sa_sn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTAS")
}

// FCVTAU instruction have 6 forms:
//
//   * FCVTAU  <Wd>, <Dn>
//   * FCVTAU  <Wd>, <Hn>
//   * FCVTAU  <Wd>, <Sn>
//   * FCVTAU  <Xd>, <Dn>
//   * FCVTAU  <Xd>, <Hn>
//   * FCVTAU  <Xd>, <Sn>
//
func (self *Program) FCVTAU(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTAU", 2, Operands { v0, v1 })
    // FCVTAU  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 5, sa_dn, sa_wd))
    }
    // FCVTAU  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 5, sa_hn, sa_wd))
    }
    // FCVTAU  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 5, sa_sn, sa_wd))
    }
    // FCVTAU  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 5, sa_dn, sa_xd))
    }
    // FCVTAU  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 5, sa_hn, sa_xd))
    }
    // FCVTAU  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 5, sa_sn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTAU")
}

// FCVTMS instruction have 6 forms:
//
//   * FCVTMS  <Wd>, <Dn>
//   * FCVTMS  <Wd>, <Hn>
//   * FCVTMS  <Wd>, <Sn>
//   * FCVTMS  <Xd>, <Dn>
//   * FCVTMS  <Xd>, <Hn>
//   * FCVTMS  <Xd>, <Sn>
//
func (self *Program) FCVTMS(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTMS", 2, Operands { v0, v1 })
    // FCVTMS  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 2, 0, sa_dn, sa_wd))
    }
    // FCVTMS  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 2, 0, sa_hn, sa_wd))
    }
    // FCVTMS  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 2, 0, sa_sn, sa_wd))
    }
    // FCVTMS  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 2, 0, sa_dn, sa_xd))
    }
    // FCVTMS  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 2, 0, sa_hn, sa_xd))
    }
    // FCVTMS  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 2, 0, sa_sn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTMS")
}

// FCVTMU instruction have 6 forms:
//
//   * FCVTMU  <Wd>, <Dn>
//   * FCVTMU  <Wd>, <Hn>
//   * FCVTMU  <Wd>, <Sn>
//   * FCVTMU  <Xd>, <Dn>
//   * FCVTMU  <Xd>, <Hn>
//   * FCVTMU  <Xd>, <Sn>
//
func (self *Program) FCVTMU(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTMU", 2, Operands { v0, v1 })
    // FCVTMU  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 2, 1, sa_dn, sa_wd))
    }
    // FCVTMU  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 2, 1, sa_hn, sa_wd))
    }
    // FCVTMU  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 2, 1, sa_sn, sa_wd))
    }
    // FCVTMU  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 2, 1, sa_dn, sa_xd))
    }
    // FCVTMU  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 2, 1, sa_hn, sa_xd))
    }
    // FCVTMU  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 2, 1, sa_sn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTMU")
}

// FCVTNS instruction have 6 forms:
//
//   * FCVTNS  <Wd>, <Dn>
//   * FCVTNS  <Wd>, <Hn>
//   * FCVTNS  <Wd>, <Sn>
//   * FCVTNS  <Xd>, <Dn>
//   * FCVTNS  <Xd>, <Hn>
//   * FCVTNS  <Xd>, <Sn>
//
func (self *Program) FCVTNS(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTNS", 2, Operands { v0, v1 })
    // FCVTNS  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 0, sa_dn, sa_wd))
    }
    // FCVTNS  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 0, sa_hn, sa_wd))
    }
    // FCVTNS  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 0, sa_sn, sa_wd))
    }
    // FCVTNS  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 0, sa_dn, sa_xd))
    }
    // FCVTNS  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 0, sa_hn, sa_xd))
    }
    // FCVTNS  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 0, sa_sn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTNS")
}

// FCVTNU instruction have 6 forms:
//
//   * FCVTNU  <Wd>, <Dn>
//   * FCVTNU  <Wd>, <Hn>
//   * FCVTNU  <Wd>, <Sn>
//   * FCVTNU  <Xd>, <Dn>
//   * FCVTNU  <Xd>, <Hn>
//   * FCVTNU  <Xd>, <Sn>
//
func (self *Program) FCVTNU(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTNU", 2, Operands { v0, v1 })
    // FCVTNU  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 1, sa_dn, sa_wd))
    }
    // FCVTNU  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 1, sa_hn, sa_wd))
    }
    // FCVTNU  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 1, sa_sn, sa_wd))
    }
    // FCVTNU  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 1, sa_dn, sa_xd))
    }
    // FCVTNU  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 1, sa_hn, sa_xd))
    }
    // FCVTNU  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 1, sa_sn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTNU")
}

// FCVTPS instruction have 6 forms:
//
//   * FCVTPS  <Wd>, <Dn>
//   * FCVTPS  <Wd>, <Hn>
//   * FCVTPS  <Wd>, <Sn>
//   * FCVTPS  <Xd>, <Dn>
//   * FCVTPS  <Xd>, <Hn>
//   * FCVTPS  <Xd>, <Sn>
//
func (self *Program) FCVTPS(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTPS", 2, Operands { v0, v1 })
    // FCVTPS  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 1, 0, sa_dn, sa_wd))
    }
    // FCVTPS  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 1, 0, sa_hn, sa_wd))
    }
    // FCVTPS  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 1, 0, sa_sn, sa_wd))
    }
    // FCVTPS  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 1, 0, sa_dn, sa_xd))
    }
    // FCVTPS  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 1, 0, sa_hn, sa_xd))
    }
    // FCVTPS  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 1, 0, sa_sn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTPS")
}

// FCVTPU instruction have 6 forms:
//
//   * FCVTPU  <Wd>, <Dn>
//   * FCVTPU  <Wd>, <Hn>
//   * FCVTPU  <Wd>, <Sn>
//   * FCVTPU  <Xd>, <Dn>
//   * FCVTPU  <Xd>, <Hn>
//   * FCVTPU  <Xd>, <Sn>
//
func (self *Program) FCVTPU(v0, v1 interface{}) *Instruction {
    p := self.alloc("FCVTPU", 2, Operands { v0, v1 })
    // FCVTPU  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 1, 1, sa_dn, sa_wd))
    }
    // FCVTPU  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 1, 1, sa_hn, sa_wd))
    }
    // FCVTPU  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 1, 1, sa_sn, sa_wd))
    }
    // FCVTPU  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 1, 1, sa_dn, sa_xd))
    }
    // FCVTPU  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 1, 1, sa_hn, sa_xd))
    }
    // FCVTPU  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 1, 1, sa_sn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTPU")
}

// FCVTZS instruction have 12 forms:
//
//   * FCVTZS  <Wd>, <Dn>, #<fbits>
//   * FCVTZS  <Wd>, <Dn>
//   * FCVTZS  <Wd>, <Hn>, #<fbits>
//   * FCVTZS  <Wd>, <Hn>
//   * FCVTZS  <Wd>, <Sn>, #<fbits>
//   * FCVTZS  <Wd>, <Sn>
//   * FCVTZS  <Xd>, <Dn>, #<fbits>
//   * FCVTZS  <Xd>, <Dn>
//   * FCVTZS  <Xd>, <Hn>, #<fbits>
//   * FCVTZS  <Xd>, <Hn>
//   * FCVTZS  <Xd>, <Sn>, #<fbits>
//   * FCVTZS  <Xd>, <Sn>
//
func (self *Program) FCVTZS(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("FCVTZS", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("FCVTZS", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction FCVTZS takes 2 or 3 operands")
    }
    // FCVTZS  <Wd>, <Dn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isDr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 1, 3, 0, sa_fbits, sa_dn, sa_wd))
    }
    // FCVTZS  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 3, 0, sa_dn, sa_wd))
    }
    // FCVTZS  <Wd>, <Hn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isHr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 3, 3, 0, sa_fbits, sa_hn, sa_wd))
    }
    // FCVTZS  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 3, 0, sa_hn, sa_wd))
    }
    // FCVTZS  <Wd>, <Sn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isSr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 0, 3, 0, sa_fbits, sa_sn, sa_wd))
    }
    // FCVTZS  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 3, 0, sa_sn, sa_wd))
    }
    // FCVTZS  <Xd>, <Dn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isDr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 1, 3, 0, sa_fbits_1, sa_dn, sa_xd))
    }
    // FCVTZS  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 3, 0, sa_dn, sa_xd))
    }
    // FCVTZS  <Xd>, <Hn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isHr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 3, 3, 0, sa_fbits_1, sa_hn, sa_xd))
    }
    // FCVTZS  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 3, 0, sa_hn, sa_xd))
    }
    // FCVTZS  <Xd>, <Sn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isSr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 0, 3, 0, sa_fbits_1, sa_sn, sa_xd))
    }
    // FCVTZS  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 3, 0, sa_sn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTZS")
}

// FCVTZU instruction have 12 forms:
//
//   * FCVTZU  <Wd>, <Dn>, #<fbits>
//   * FCVTZU  <Wd>, <Dn>
//   * FCVTZU  <Wd>, <Hn>, #<fbits>
//   * FCVTZU  <Wd>, <Hn>
//   * FCVTZU  <Wd>, <Sn>, #<fbits>
//   * FCVTZU  <Wd>, <Sn>
//   * FCVTZU  <Xd>, <Dn>, #<fbits>
//   * FCVTZU  <Xd>, <Dn>
//   * FCVTZU  <Xd>, <Hn>, #<fbits>
//   * FCVTZU  <Xd>, <Hn>
//   * FCVTZU  <Xd>, <Sn>, #<fbits>
//   * FCVTZU  <Xd>, <Sn>
//
func (self *Program) FCVTZU(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("FCVTZU", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("FCVTZU", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction FCVTZU takes 2 or 3 operands")
    }
    // FCVTZU  <Wd>, <Dn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isDr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 1, 3, 1, sa_fbits, sa_dn, sa_wd))
    }
    // FCVTZU  <Wd>, <Dn>
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 3, 1, sa_dn, sa_wd))
    }
    // FCVTZU  <Wd>, <Hn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isHr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 3, 3, 1, sa_fbits, sa_hn, sa_wd))
    }
    // FCVTZU  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 3, 1, sa_hn, sa_wd))
    }
    // FCVTZU  <Wd>, <Sn>, #<fbits>
    if len(vv) == 1 && isWr(v0) && isSr(v1) && isFpBits(vv[0]) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 0, 3, 1, sa_fbits, sa_sn, sa_wd))
    }
    // FCVTZU  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 3, 1, sa_sn, sa_wd))
    }
    // FCVTZU  <Xd>, <Dn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isDr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 1, 3, 1, sa_fbits_1, sa_dn, sa_xd))
    }
    // FCVTZU  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 3, 1, sa_dn, sa_xd))
    }
    // FCVTZU  <Xd>, <Hn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isHr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 3, 3, 1, sa_fbits_1, sa_hn, sa_xd))
    }
    // FCVTZU  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 3, 1, sa_hn, sa_xd))
    }
    // FCVTZU  <Xd>, <Sn>, #<fbits>
    if len(vv) == 1 && isXr(v0) && isSr(v1) && isFpBits(vv[0]) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 0, 3, 1, sa_fbits_1, sa_sn, sa_xd))
    }
    // FCVTZU  <Xd>, <Sn>
    if isXr(v0) && isSr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 3, 1, sa_sn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FCVTZU")
}

// FDIV instruction have 3 forms:
//
//   * FDIV  <Dd>, <Dn>, <Dm>
//   * FDIV  <Hd>, <Hn>, <Hm>
//   * FDIV  <Sd>, <Sn>, <Sm>
//
func (self *Program) FDIV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FDIV", 3, Operands { v0, v1, v2 })
    // FDIV  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 1, sa_dn, sa_dd))
    }
    // FDIV  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 1, sa_hn, sa_hd))
    }
    // FDIV  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 1, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FDIV")
}

// FJCVTZS instruction have one single form:
//
//   * FJCVTZS  <Wd>, <Dn>
//
func (self *Program) FJCVTZS(v0, v1 interface{}) *Instruction {
    p := self.alloc("FJCVTZS", 2, Operands { v0, v1 })
    if isWr(v0) && isDr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 3, 6, sa_dn, sa_wd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for FJCVTZS")
}

// FMADD instruction have 3 forms:
//
//   * FMADD  <Dd>, <Dn>, <Dm>, <Da>
//   * FMADD  <Hd>, <Hn>, <Hm>, <Ha>
//   * FMADD  <Sd>, <Sn>, <Sm>, <Sa>
//
func (self *Program) FMADD(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FMADD", 4, Operands { v0, v1, v2, v3 })
    // FMADD  <Dd>, <Dn>, <Dm>, <Da>
    if isDr(v0) && isDr(v1) && isDr(v2) && isDr(v3) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        sa_da := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 1, 0, sa_dm, 0, sa_da, sa_dn, sa_dd))
    }
    // FMADD  <Hd>, <Hn>, <Hm>, <Ha>
    if isHr(v0) && isHr(v1) && isHr(v2) && isHr(v3) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        sa_ha := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 3, 0, sa_hm, 0, sa_ha, sa_hn, sa_hd))
    }
    // FMADD  <Sd>, <Sn>, <Sm>, <Sa>
    if isSr(v0) && isSr(v1) && isSr(v2) && isSr(v3) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        sa_sa := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 0, 0, sa_sm, 0, sa_sa, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMADD")
}

// FMAX instruction have 3 forms:
//
//   * FMAX  <Dd>, <Dn>, <Dm>
//   * FMAX  <Hd>, <Hn>, <Hm>
//   * FMAX  <Sd>, <Sn>, <Sm>
//
func (self *Program) FMAX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMAX", 3, Operands { v0, v1, v2 })
    // FMAX  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 4, sa_dn, sa_dd))
    }
    // FMAX  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 4, sa_hn, sa_hd))
    }
    // FMAX  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 4, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMAX")
}

// FMAXNM instruction have 3 forms:
//
//   * FMAXNM  <Dd>, <Dn>, <Dm>
//   * FMAXNM  <Hd>, <Hn>, <Hm>
//   * FMAXNM  <Sd>, <Sn>, <Sm>
//
func (self *Program) FMAXNM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMAXNM", 3, Operands { v0, v1, v2 })
    // FMAXNM  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 6, sa_dn, sa_dd))
    }
    // FMAXNM  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 6, sa_hn, sa_hd))
    }
    // FMAXNM  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 6, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMAXNM")
}

// FMIN instruction have 3 forms:
//
//   * FMIN  <Dd>, <Dn>, <Dm>
//   * FMIN  <Hd>, <Hn>, <Hm>
//   * FMIN  <Sd>, <Sn>, <Sm>
//
func (self *Program) FMIN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMIN", 3, Operands { v0, v1, v2 })
    // FMIN  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 5, sa_dn, sa_dd))
    }
    // FMIN  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 5, sa_hn, sa_hd))
    }
    // FMIN  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 5, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMIN")
}

// FMINNM instruction have 3 forms:
//
//   * FMINNM  <Dd>, <Dn>, <Dm>
//   * FMINNM  <Hd>, <Hn>, <Hm>
//   * FMINNM  <Sd>, <Sn>, <Sm>
//
func (self *Program) FMINNM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMINNM", 3, Operands { v0, v1, v2 })
    // FMINNM  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 7, sa_dn, sa_dd))
    }
    // FMINNM  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 7, sa_hn, sa_hd))
    }
    // FMINNM  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 7, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMINNM")
}

// FMOV instruction have 16 forms:
//
//   * FMOV  <Wd>, <Hn>
//   * FMOV  <Wd>, <Sn>
//   * FMOV  <Xd>, <Dn>
//   * FMOV  <Xd>, <Hn>
//   * FMOV  <Xd>, <Vn>.D[1]
//   * FMOV  <Dd>, <Xn>
//   * FMOV  <Dd>, <Dn>
//   * FMOV  <Dd>, #<imm>
//   * FMOV  <Hd>, <Wn>
//   * FMOV  <Hd>, <Xn>
//   * FMOV  <Hd>, <Hn>
//   * FMOV  <Hd>, #<imm>
//   * FMOV  <Sd>, <Wn>
//   * FMOV  <Sd>, <Sn>
//   * FMOV  <Sd>, #<imm>
//   * FMOV  <Vd>.D[1], <Xn>
//
func (self *Program) FMOV(v0, v1 interface{}) *Instruction {
    p := self.alloc("FMOV", 2, Operands { v0, v1 })
    // FMOV  <Wd>, <Hn>
    if isWr(v0) && isHr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 6, sa_hn, sa_wd))
    }
    // FMOV  <Wd>, <Sn>
    if isWr(v0) && isSr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 6, sa_sn, sa_wd))
    }
    // FMOV  <Xd>, <Dn>
    if isXr(v0) && isDr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 6, sa_dn, sa_xd))
    }
    // FMOV  <Xd>, <Hn>
    if isXr(v0) && isHr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 6, sa_hn, sa_xd))
    }
    // FMOV  <Xd>, <Vn>.D[1]
    if isXr(v0) && isVri(v1) && vstrr(v1) == VecD && vidxr(v1) == 1 {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_vn := uint32(v1.(_Indexed128r).ID())
        return p.setins(float2int(1, 0, 2, 1, 6, sa_vn, sa_xd))
    }
    // FMOV  <Dd>, <Xn>
    if isDr(v0) && isXr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 7, sa_xn, sa_dd))
    }
    // FMOV  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 0, sa_dn, sa_dd))
    }
    // FMOV  <Dd>, #<imm>
    if isDr(v0) && isFpImm8(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_imm := asFpImm8(v1)
        return p.setins(floatimm(0, 0, 1, sa_imm, 0, sa_dd))
    }
    // FMOV  <Hd>, <Wn>
    if isHr(v0) && isWr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 7, sa_wn, sa_hd))
    }
    // FMOV  <Hd>, <Xn>
    if isHr(v0) && isXr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 7, sa_xn, sa_hd))
    }
    // FMOV  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 0, sa_hn, sa_hd))
    }
    // FMOV  <Hd>, #<imm>
    if isHr(v0) && isFpImm8(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_imm := asFpImm8(v1)
        return p.setins(floatimm(0, 0, 3, sa_imm, 0, sa_hd))
    }
    // FMOV  <Sd>, <Wn>
    if isSr(v0) && isWr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 7, sa_wn, sa_sd))
    }
    // FMOV  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 0, sa_sn, sa_sd))
    }
    // FMOV  <Sd>, #<imm>
    if isSr(v0) && isFpImm8(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_imm := asFpImm8(v1)
        return p.setins(floatimm(0, 0, 0, sa_imm, 0, sa_sd))
    }
    // FMOV  <Vd>.D[1], <Xn>
    if isVri(v0) && vstrr(v0) == VecD && vidxr(v0) == 1 && isXr(v1) {
        sa_vd := uint32(v0.(_Indexed128r).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 2, 1, 7, sa_xn, sa_vd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMOV")
}

// FMSUB instruction have 3 forms:
//
//   * FMSUB  <Dd>, <Dn>, <Dm>, <Da>
//   * FMSUB  <Hd>, <Hn>, <Hm>, <Ha>
//   * FMSUB  <Sd>, <Sn>, <Sm>, <Sa>
//
func (self *Program) FMSUB(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FMSUB", 4, Operands { v0, v1, v2, v3 })
    // FMSUB  <Dd>, <Dn>, <Dm>, <Da>
    if isDr(v0) && isDr(v1) && isDr(v2) && isDr(v3) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        sa_da := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 1, 0, sa_dm, 1, sa_da, sa_dn, sa_dd))
    }
    // FMSUB  <Hd>, <Hn>, <Hm>, <Ha>
    if isHr(v0) && isHr(v1) && isHr(v2) && isHr(v3) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        sa_ha := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 3, 0, sa_hm, 1, sa_ha, sa_hn, sa_hd))
    }
    // FMSUB  <Sd>, <Sn>, <Sm>, <Sa>
    if isSr(v0) && isSr(v1) && isSr(v2) && isSr(v3) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        sa_sa := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 0, 0, sa_sm, 1, sa_sa, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMSUB")
}

// FMUL instruction have 3 forms:
//
//   * FMUL  <Dd>, <Dn>, <Dm>
//   * FMUL  <Hd>, <Hn>, <Hm>
//   * FMUL  <Sd>, <Sn>, <Sm>
//
func (self *Program) FMUL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FMUL", 3, Operands { v0, v1, v2 })
    // FMUL  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 0, sa_dn, sa_dd))
    }
    // FMUL  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 0, sa_hn, sa_hd))
    }
    // FMUL  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 0, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FMUL")
}

// FNEG instruction have 3 forms:
//
//   * FNEG  <Dd>, <Dn>
//   * FNEG  <Hd>, <Hn>
//   * FNEG  <Sd>, <Sn>
//
func (self *Program) FNEG(v0, v1 interface{}) *Instruction {
    p := self.alloc("FNEG", 2, Operands { v0, v1 })
    // FNEG  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 2, sa_dn, sa_dd))
    }
    // FNEG  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 2, sa_hn, sa_hd))
    }
    // FNEG  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 2, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FNEG")
}

// FNMADD instruction have 3 forms:
//
//   * FNMADD  <Dd>, <Dn>, <Dm>, <Da>
//   * FNMADD  <Hd>, <Hn>, <Hm>, <Ha>
//   * FNMADD  <Sd>, <Sn>, <Sm>, <Sa>
//
func (self *Program) FNMADD(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FNMADD", 4, Operands { v0, v1, v2, v3 })
    // FNMADD  <Dd>, <Dn>, <Dm>, <Da>
    if isDr(v0) && isDr(v1) && isDr(v2) && isDr(v3) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        sa_da := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 1, 1, sa_dm, 0, sa_da, sa_dn, sa_dd))
    }
    // FNMADD  <Hd>, <Hn>, <Hm>, <Ha>
    if isHr(v0) && isHr(v1) && isHr(v2) && isHr(v3) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        sa_ha := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 3, 1, sa_hm, 0, sa_ha, sa_hn, sa_hd))
    }
    // FNMADD  <Sd>, <Sn>, <Sm>, <Sa>
    if isSr(v0) && isSr(v1) && isSr(v2) && isSr(v3) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        sa_sa := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 0, 1, sa_sm, 0, sa_sa, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FNMADD")
}

// FNMSUB instruction have 3 forms:
//
//   * FNMSUB  <Dd>, <Dn>, <Dm>, <Da>
//   * FNMSUB  <Hd>, <Hn>, <Hm>, <Ha>
//   * FNMSUB  <Sd>, <Sn>, <Sm>, <Sa>
//
func (self *Program) FNMSUB(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("FNMSUB", 4, Operands { v0, v1, v2, v3 })
    // FNMSUB  <Dd>, <Dn>, <Dm>, <Da>
    if isDr(v0) && isDr(v1) && isDr(v2) && isDr(v3) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        sa_da := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 1, 1, sa_dm, 1, sa_da, sa_dn, sa_dd))
    }
    // FNMSUB  <Hd>, <Hn>, <Hm>, <Ha>
    if isHr(v0) && isHr(v1) && isHr(v2) && isHr(v3) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        sa_ha := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 3, 1, sa_hm, 1, sa_ha, sa_hn, sa_hd))
    }
    // FNMSUB  <Sd>, <Sn>, <Sm>, <Sa>
    if isSr(v0) && isSr(v1) && isSr(v2) && isSr(v3) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        sa_sa := uint32(v3.(asm.Register).ID())
        return p.setins(floatdp3(0, 0, 0, 1, sa_sm, 1, sa_sa, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FNMSUB")
}

// FNMUL instruction have 3 forms:
//
//   * FNMUL  <Dd>, <Dn>, <Dm>
//   * FNMUL  <Hd>, <Hn>, <Hm>
//   * FNMUL  <Sd>, <Sn>, <Sm>
//
func (self *Program) FNMUL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FNMUL", 3, Operands { v0, v1, v2 })
    // FNMUL  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 8, sa_dn, sa_dd))
    }
    // FNMUL  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 8, sa_hn, sa_hd))
    }
    // FNMUL  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 8, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FNMUL")
}

// FRINT32X instruction have 2 forms:
//
//   * FRINT32X  <Dd>, <Dn>
//   * FRINT32X  <Sd>, <Sn>
//
func (self *Program) FRINT32X(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINT32X", 2, Operands { v0, v1 })
    // FRINT32X  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 17, sa_dn, sa_dd))
    }
    // FRINT32X  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 17, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINT32X")
}

// FRINT32Z instruction have 2 forms:
//
//   * FRINT32Z  <Dd>, <Dn>
//   * FRINT32Z  <Sd>, <Sn>
//
func (self *Program) FRINT32Z(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINT32Z", 2, Operands { v0, v1 })
    // FRINT32Z  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 16, sa_dn, sa_dd))
    }
    // FRINT32Z  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 16, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINT32Z")
}

// FRINT64X instruction have 2 forms:
//
//   * FRINT64X  <Dd>, <Dn>
//   * FRINT64X  <Sd>, <Sn>
//
func (self *Program) FRINT64X(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINT64X", 2, Operands { v0, v1 })
    // FRINT64X  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 19, sa_dn, sa_dd))
    }
    // FRINT64X  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 19, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINT64X")
}

// FRINT64Z instruction have 2 forms:
//
//   * FRINT64Z  <Dd>, <Dn>
//   * FRINT64Z  <Sd>, <Sn>
//
func (self *Program) FRINT64Z(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINT64Z", 2, Operands { v0, v1 })
    // FRINT64Z  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 18, sa_dn, sa_dd))
    }
    // FRINT64Z  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 18, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINT64Z")
}

// FRINTA instruction have 3 forms:
//
//   * FRINTA  <Dd>, <Dn>
//   * FRINTA  <Hd>, <Hn>
//   * FRINTA  <Sd>, <Sn>
//
func (self *Program) FRINTA(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTA", 2, Operands { v0, v1 })
    // FRINTA  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 12, sa_dn, sa_dd))
    }
    // FRINTA  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 12, sa_hn, sa_hd))
    }
    // FRINTA  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 12, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTA")
}

// FRINTI instruction have 3 forms:
//
//   * FRINTI  <Dd>, <Dn>
//   * FRINTI  <Hd>, <Hn>
//   * FRINTI  <Sd>, <Sn>
//
func (self *Program) FRINTI(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTI", 2, Operands { v0, v1 })
    // FRINTI  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 15, sa_dn, sa_dd))
    }
    // FRINTI  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 15, sa_hn, sa_hd))
    }
    // FRINTI  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 15, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTI")
}

// FRINTM instruction have 3 forms:
//
//   * FRINTM  <Dd>, <Dn>
//   * FRINTM  <Hd>, <Hn>
//   * FRINTM  <Sd>, <Sn>
//
func (self *Program) FRINTM(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTM", 2, Operands { v0, v1 })
    // FRINTM  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 10, sa_dn, sa_dd))
    }
    // FRINTM  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 10, sa_hn, sa_hd))
    }
    // FRINTM  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 10, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTM")
}

// FRINTN instruction have 3 forms:
//
//   * FRINTN  <Dd>, <Dn>
//   * FRINTN  <Hd>, <Hn>
//   * FRINTN  <Sd>, <Sn>
//
func (self *Program) FRINTN(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTN", 2, Operands { v0, v1 })
    // FRINTN  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 8, sa_dn, sa_dd))
    }
    // FRINTN  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 8, sa_hn, sa_hd))
    }
    // FRINTN  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 8, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTN")
}

// FRINTP instruction have 3 forms:
//
//   * FRINTP  <Dd>, <Dn>
//   * FRINTP  <Hd>, <Hn>
//   * FRINTP  <Sd>, <Sn>
//
func (self *Program) FRINTP(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTP", 2, Operands { v0, v1 })
    // FRINTP  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 9, sa_dn, sa_dd))
    }
    // FRINTP  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 9, sa_hn, sa_hd))
    }
    // FRINTP  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 9, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTP")
}

// FRINTX instruction have 3 forms:
//
//   * FRINTX  <Dd>, <Dn>
//   * FRINTX  <Hd>, <Hn>
//   * FRINTX  <Sd>, <Sn>
//
func (self *Program) FRINTX(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTX", 2, Operands { v0, v1 })
    // FRINTX  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 14, sa_dn, sa_dd))
    }
    // FRINTX  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 14, sa_hn, sa_hd))
    }
    // FRINTX  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 14, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTX")
}

// FRINTZ instruction have 3 forms:
//
//   * FRINTZ  <Dd>, <Dn>
//   * FRINTZ  <Hd>, <Hn>
//   * FRINTZ  <Sd>, <Sn>
//
func (self *Program) FRINTZ(v0, v1 interface{}) *Instruction {
    p := self.alloc("FRINTZ", 2, Operands { v0, v1 })
    // FRINTZ  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 11, sa_dn, sa_dd))
    }
    // FRINTZ  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 11, sa_hn, sa_hd))
    }
    // FRINTZ  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 11, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FRINTZ")
}

// FSQRT instruction have 3 forms:
//
//   * FSQRT  <Dd>, <Dn>
//   * FSQRT  <Hd>, <Hn>
//   * FSQRT  <Sd>, <Sn>
//
func (self *Program) FSQRT(v0, v1 interface{}) *Instruction {
    p := self.alloc("FSQRT", 2, Operands { v0, v1 })
    // FSQRT  <Dd>, <Dn>
    if isDr(v0) && isDr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 1, 3, sa_dn, sa_dd))
    }
    // FSQRT  <Hd>, <Hn>
    if isHr(v0) && isHr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 3, 3, sa_hn, sa_hd))
    }
    // FSQRT  <Sd>, <Sn>
    if isSr(v0) && isSr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        return p.setins(floatdp1(0, 0, 0, 3, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FSQRT")
}

// FSUB instruction have 3 forms:
//
//   * FSUB  <Dd>, <Dn>, <Dm>
//   * FSUB  <Hd>, <Hn>, <Hm>
//   * FSUB  <Sd>, <Sn>, <Sm>
//
func (self *Program) FSUB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("FSUB", 3, Operands { v0, v1, v2 })
    // FSUB  <Dd>, <Dn>, <Dm>
    if isDr(v0) && isDr(v1) && isDr(v2) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_dn := uint32(v1.(asm.Register).ID())
        sa_dm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 1, sa_dm, 3, sa_dn, sa_dd))
    }
    // FSUB  <Hd>, <Hn>, <Hm>
    if isHr(v0) && isHr(v1) && isHr(v2) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_hn := uint32(v1.(asm.Register).ID())
        sa_hm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 3, sa_hm, 3, sa_hn, sa_hd))
    }
    // FSUB  <Sd>, <Sn>, <Sm>
    if isSr(v0) && isSr(v1) && isSr(v2) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_sn := uint32(v1.(asm.Register).ID())
        sa_sm := uint32(v2.(asm.Register).ID())
        return p.setins(floatdp2(0, 0, 0, sa_sm, 3, sa_sn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for FSUB")
}

// GCSB instruction have one single form:
//
//   * GCSB DSYNC
//
func (self *Program) GCSB(v0 interface{}) *Instruction {
    p := self.alloc("GCSB", 1, Operands { v0 })
    if v0 == DSYNC {
        return p.setins(hints(2, 3))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for GCSB")
}

// GCSSTR instruction have one single form:
//
//   * GCSSTR  <Xt>, [<Xn|SP>]
//
func (self *Program) GCSSTR(v0, v1 interface{}) *Instruction {
    p := self.alloc("GCSSTR", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldst_gcs(0, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for GCSSTR")
}

// GCSSTTR instruction have one single form:
//
//   * GCSSTTR  <Xt>, [<Xn|SP>]
//
func (self *Program) GCSSTTR(v0, v1 interface{}) *Instruction {
    p := self.alloc("GCSSTTR", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldst_gcs(1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for GCSSTTR")
}

// GMI instruction have one single form:
//
//   * GMI  <Xd>, <Xn|SP>, <Xm>
//
func (self *Program) GMI(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("GMI", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXrOrSP(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        Rm := uint32(0b00000)
        Rm |= sa_xm
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd
        return p.setins(dp_2src(1, 0, Rm, 5, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for GMI")
}

// HINT instruction have one single form:
//
//   * HINT  #<imm>
//
func (self *Program) HINT(v0 interface{}) *Instruction {
    p := self.alloc("HINT", 1, Operands { v0 })
    if isUimm7(v0) {
        sa_imm := asUimm7(v0)
        return p.setins(hints((sa_imm >> 3) & 0xf, sa_imm & 0x7))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for HINT")
}

// HLT instruction have one single form:
//
//   * HLT  #<imm>
//
func (self *Program) HLT(v0 interface{}) *Instruction {
    p := self.alloc("HLT", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(2, sa_imm, 0, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for HLT")
}

// HVC instruction have one single form:
//
//   * HVC  #<imm>
//
func (self *Program) HVC(v0 interface{}) *Instruction {
    p := self.alloc("HVC", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(0, sa_imm, 0, 2))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for HVC")
}

// IRG instruction have one single form:
//
//   * IRG  <Xd|SP>, <Xn|SP>{, <Xm>}
//
func (self *Program) IRG(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("IRG", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("IRG", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction IRG takes 2 or 3 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && isXrOrSP(v0) && isXrOrSP(v1) && (len(vv) == 0 || isXr(vv[0])) {
        var sa_xm uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        if len(vv) == 1 {
            sa_xm = uint32(vv[0].(asm.Register).ID())
        }
        Rm := uint32(0b00000)
        Rm |= sa_xm
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd_sp
        return p.setins(dp_2src(1, 0, Rm, 4, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for IRG")
}

// ISB instruction have one single form:
//
//   * ISB  {<option>|#<imm>}
//
func (self *Program) ISB(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ISB", 0, Operands {})
        case 1  : p = self.alloc("ISB", 1, Operands { vv[0] })
        default : panic("instruction ISB takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isOption(vv[0])) {
        sa_option := SY
        if len(vv) == 1 {
            sa_option = vv[0].(BarrierOption)
        }
        sa_imm := uint32(sa_option)
        return p.setins(barriers(sa_imm, 6, 31))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ISB")
}

// LD64B instruction have one single form:
//
//   * LD64B  <Xt>, [<Xn|SP> {,#0}]
//
func (self *Program) LD64B(v0, v1 interface{}) *Instruction {
    p := self.alloc("LD64B", 2, Operands { v0, v1 })
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(memop(3, 0, 0, 0, 31, 1, 5, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LD64B")
}

// LDADD instruction have 2 forms:
//
//   * LDADD  <Ws>, <Wt>, [<Xn|SP>]
//   * LDADD  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDADD(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADD", 3, Operands { v0, v1, v2 })
    // LDADD  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    // LDADD  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDADD")
}

// LDADDA instruction have 2 forms:
//
//   * LDADDA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDADDA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDADDA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDA", 3, Operands { v0, v1, v2 })
    // LDADDA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    // LDADDA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDA")
}

// LDADDAB instruction have one single form:
//
//   * LDADDAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDAB")
}

// LDADDAH instruction have one single form:
//
//   * LDADDAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDAH")
}

// LDADDAL instruction have 2 forms:
//
//   * LDADDAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDADDAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDADDAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDAL", 3, Operands { v0, v1, v2 })
    // LDADDAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    // LDADDAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDAL")
}

// LDADDALB instruction have one single form:
//
//   * LDADDALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDALB")
}

// LDADDALH instruction have one single form:
//
//   * LDADDALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDALH")
}

// LDADDB instruction have one single form:
//
//   * LDADDB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDB")
}

// LDADDH instruction have one single form:
//
//   * LDADDH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDH")
}

// LDADDL instruction have 2 forms:
//
//   * LDADDL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDADDL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDADDL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDL", 3, Operands { v0, v1, v2 })
    // LDADDL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    // LDADDL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDL")
}

// LDADDLB instruction have one single form:
//
//   * LDADDLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDLB")
}

// LDADDLH instruction have one single form:
//
//   * LDADDLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDADDLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDADDLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDADDLH")
}

// LDAPR instruction have 4 forms:
//
//   * LDAPR  <Wt>, [<Xn|SP>], #4
//   * LDAPR  <Wt>, [<Xn|SP> {,#0}]
//   * LDAPR  <Xt>, [<Xn|SP>], #8
//   * LDAPR  <Xt>, [<Xn|SP> {,#0}]
//
func (self *Program) LDAPR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPR", 2, Operands { v0, v1 })
    // LDAPR  <Wt>, [<Xn|SP>], #4
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 4 && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldapstl_writeback(2, 1, sa_xn_sp, sa_wt))
    }
    // LDAPR  <Wt>, [<Xn|SP> {,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(memop(2, 0, 1, 0, 31, 1, 4, sa_xn_sp, sa_wt))
    }
    // LDAPR  <Xt>, [<Xn|SP>], #8
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 8 && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldapstl_writeback(3, 1, sa_xn_sp, sa_xt))
    }
    // LDAPR  <Xt>, [<Xn|SP> {,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(memop(3, 0, 1, 0, 31, 1, 4, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPR")
}

// LDAPRB instruction have one single form:
//
//   * LDAPRB  <Wt>, [<Xn|SP> {,#0}]
//
func (self *Program) LDAPRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPRB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(memop(0, 0, 1, 0, 31, 1, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPRB")
}

// LDAPRH instruction have one single form:
//
//   * LDAPRH  <Wt>, [<Xn|SP> {,#0}]
//
func (self *Program) LDAPRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPRH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(memop(1, 0, 1, 0, 31, 1, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPRH")
}

// LDAPUR instruction have 7 forms:
//
//   * LDAPUR  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDAPUR  <Xt>, [<Xn|SP>{, #<simm>}]
//   * LDAPUR  <Bt>, [<Xn|SP>{, #<simm>}]
//   * LDAPUR  <Dt>, [<Xn|SP>{, #<simm>}]
//   * LDAPUR  <Ht>, [<Xn|SP>{, #<simm>}]
//   * LDAPUR  <Qt>, [<Xn|SP>{, #<simm>}]
//   * LDAPUR  <St>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDAPUR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPUR", 2, Operands { v0, v1 })
    // LDAPUR  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(2, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDAPUR  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(3, 1, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDAPUR  <Bt>, [<Xn|SP>{, #<simm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(0, 1, sa_simm, sa_xn_sp, sa_bt))
    }
    // LDAPUR  <Dt>, [<Xn|SP>{, #<simm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(3, 1, sa_simm, sa_xn_sp, sa_dt))
    }
    // LDAPUR  <Ht>, [<Xn|SP>{, #<simm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(1, 1, sa_simm, sa_xn_sp, sa_ht))
    }
    // LDAPUR  <Qt>, [<Xn|SP>{, #<simm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(0, 3, sa_simm, sa_xn_sp, sa_qt))
    }
    // LDAPUR  <St>, [<Xn|SP>{, #<simm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(2, 1, sa_simm, sa_xn_sp, sa_st))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPUR")
}

// LDAPURB instruction have one single form:
//
//   * LDAPURB  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDAPURB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPURB", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPURB")
}

// LDAPURH instruction have one single form:
//
//   * LDAPURH  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDAPURH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPURH", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(1, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPURH")
}

// LDAPURSB instruction have 2 forms:
//
//   * LDAPURSB  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDAPURSB  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDAPURSB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPURSB", 2, Operands { v0, v1 })
    // LDAPURSB  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDAPURSB  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPURSB")
}

// LDAPURSH instruction have 2 forms:
//
//   * LDAPURSH  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDAPURSH  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDAPURSH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPURSH", 2, Operands { v0, v1 })
    // LDAPURSH  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(1, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDAPURSH  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(1, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPURSH")
}

// LDAPURSW instruction have one single form:
//
//   * LDAPURSW  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDAPURSW(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAPURSW", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(2, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAPURSW")
}

// LDAR instruction have 2 forms:
//
//   * LDAR  <Wt>, [<Xn|SP>{,#0}]
//   * LDAR  <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDAR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAR", 2, Operands { v0, v1 })
    // LDAR  <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(2, 1, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    // LDAR  <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(3, 1, 31, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAR")
}

// LDARB instruction have one single form:
//
//   * LDARB  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDARB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDARB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(0, 1, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDARB")
}

// LDARH instruction have one single form:
//
//   * LDARH  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDARH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDARH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(1, 1, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDARH")
}

// LDAXP instruction have 2 forms:
//
//   * LDAXP  <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
//   * LDAXP  <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
//
func (self *Program) LDAXP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDAXP", 3, Operands { v0, v1, v2 })
    // LDAXP  <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclp(0, 1, 31, 1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDAXP  <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclp(1, 1, 31, 1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAXP")
}

// LDAXR instruction have 2 forms:
//
//   * LDAXR  <Wt>, [<Xn|SP>{,#0}]
//   * LDAXR  <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDAXR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAXR", 2, Operands { v0, v1 })
    // LDAXR  <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(2, 1, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    // LDAXR  <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(3, 1, 31, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDAXR")
}

// LDAXRB instruction have one single form:
//
//   * LDAXRB  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDAXRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAXRB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(0, 1, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAXRB")
}

// LDAXRH instruction have one single form:
//
//   * LDAXRH  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDAXRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDAXRH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(1, 1, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDAXRH")
}

// LDCLR instruction have 2 forms:
//
//   * LDCLR  <Ws>, <Wt>, [<Xn|SP>]
//   * LDCLR  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDCLR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLR", 3, Operands { v0, v1, v2 })
    // LDCLR  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    // LDCLR  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 1, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLR")
}

// LDCLRA instruction have 2 forms:
//
//   * LDCLRA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDCLRA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDCLRA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRA", 3, Operands { v0, v1, v2 })
    // LDCLRA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    // LDCLRA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 1, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRA")
}

// LDCLRAB instruction have one single form:
//
//   * LDCLRAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRAB")
}

// LDCLRAH instruction have one single form:
//
//   * LDCLRAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRAH")
}

// LDCLRAL instruction have 2 forms:
//
//   * LDCLRAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDCLRAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDCLRAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRAL", 3, Operands { v0, v1, v2 })
    // LDCLRAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    // LDCLRAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 1, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRAL")
}

// LDCLRALB instruction have one single form:
//
//   * LDCLRALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRALB")
}

// LDCLRALH instruction have one single form:
//
//   * LDCLRALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRALH")
}

// LDCLRB instruction have one single form:
//
//   * LDCLRB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRB")
}

// LDCLRH instruction have one single form:
//
//   * LDCLRH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRH")
}

// LDCLRL instruction have 2 forms:
//
//   * LDCLRL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDCLRL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDCLRL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRL", 3, Operands { v0, v1, v2 })
    // LDCLRL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    // LDCLRL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 1, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRL")
}

// LDCLRLB instruction have one single form:
//
//   * LDCLRLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRLB")
}

// LDCLRLH instruction have one single form:
//
//   * LDCLRLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDCLRLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 1, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRLH")
}

// LDCLRP instruction have one single form:
//
//   * LDCLRP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDCLRP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 0, sa_xt2, 0, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRP")
}

// LDCLRPA instruction have one single form:
//
//   * LDCLRPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDCLRPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 0, sa_xt2, 0, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRPA")
}

// LDCLRPAL instruction have one single form:
//
//   * LDCLRPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDCLRPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 1, sa_xt2, 0, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRPAL")
}

// LDCLRPL instruction have one single form:
//
//   * LDCLRPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDCLRPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDCLRPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 1, sa_xt2, 0, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDCLRPL")
}

// LDEOR instruction have 2 forms:
//
//   * LDEOR  <Ws>, <Wt>, [<Xn|SP>]
//   * LDEOR  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDEOR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEOR", 3, Operands { v0, v1, v2 })
    // LDEOR  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    // LDEOR  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 2, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDEOR")
}

// LDEORA instruction have 2 forms:
//
//   * LDEORA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDEORA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDEORA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORA", 3, Operands { v0, v1, v2 })
    // LDEORA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    // LDEORA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 2, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORA")
}

// LDEORAB instruction have one single form:
//
//   * LDEORAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORAB")
}

// LDEORAH instruction have one single form:
//
//   * LDEORAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORAH")
}

// LDEORAL instruction have 2 forms:
//
//   * LDEORAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDEORAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDEORAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORAL", 3, Operands { v0, v1, v2 })
    // LDEORAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    // LDEORAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 2, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORAL")
}

// LDEORALB instruction have one single form:
//
//   * LDEORALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORALB")
}

// LDEORALH instruction have one single form:
//
//   * LDEORALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORALH")
}

// LDEORB instruction have one single form:
//
//   * LDEORB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORB")
}

// LDEORH instruction have one single form:
//
//   * LDEORH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORH")
}

// LDEORL instruction have 2 forms:
//
//   * LDEORL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDEORL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDEORL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORL", 3, Operands { v0, v1, v2 })
    // LDEORL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    // LDEORL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 2, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORL")
}

// LDEORLB instruction have one single form:
//
//   * LDEORLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORLB")
}

// LDEORLH instruction have one single form:
//
//   * LDEORLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDEORLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDEORLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 2, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDEORLH")
}

// LDG instruction have one single form:
//
//   * LDG  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDG(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDG", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt
        return p.setins(ldsttags(1, sa_simm, 0, Rn, Rt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDG")
}

// LDGM instruction have one single form:
//
//   * LDGM  <Xt>, [<Xn|SP>]
//
func (self *Program) LDGM(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDGM", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt
        return p.setins(ldsttags(3, 0, 0, Rn, Rt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDGM")
}

// LDIAPP instruction have 4 forms:
//
//   * LDIAPP  <Wt1>, <Wt2>, [<Xn|SP>], #8
//   * LDIAPP  <Wt1>, <Wt2>, [<Xn|SP>]
//   * LDIAPP  <Xt1>, <Xt2>, [<Xn|SP>], #16
//   * LDIAPP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDIAPP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDIAPP", 3, Operands { v0, v1, v2 })
    // LDIAPP  <Wt1>, <Wt2>, [<Xn|SP>], #8
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 8 &&
       mext(v2) == PostIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(2, 1, sa_wt2, 0, sa_xn_sp, sa_wt1))
    }
    // LDIAPP  <Wt1>, <Wt2>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(2, 1, sa_wt2, 1, sa_xn_sp, sa_wt1))
    }
    // LDIAPP  <Xt1>, <Xt2>, [<Xn|SP>], #16
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 16 &&
       mext(v2) == PostIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(3, 1, sa_xt2, 0, sa_xn_sp, sa_xt1))
    }
    // LDIAPP  <Xt1>, <Xt2>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(3, 1, sa_xt2, 1, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDIAPP")
}

// LDLAR instruction have 2 forms:
//
//   * LDLAR  <Wt>, [<Xn|SP>{,#0}]
//   * LDLAR  <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDLAR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDLAR", 2, Operands { v0, v1 })
    // LDLAR  <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(2, 1, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    // LDLAR  <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(3, 1, 31, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDLAR")
}

// LDLARB instruction have one single form:
//
//   * LDLARB  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDLARB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDLARB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(0, 1, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDLARB")
}

// LDLARH instruction have one single form:
//
//   * LDLARH  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDLARH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDLARH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(1, 1, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDLARH")
}

// LDNP instruction have 5 forms:
//
//   * LDNP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
//   * LDNP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * LDNP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
//   * LDNP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
//   * LDNP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
//
func (self *Program) LDNP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDNP", 3, Operands { v0, v1, v2 })
    // LDNP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(0, 0, 1, sa_imm, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDNP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(2, 0, 1, sa_imm_1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDNP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(1, 1, 1, sa_imm, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // LDNP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(2, 1, 1, sa_imm_1, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // LDNP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(0, 1, 1, sa_imm_2, sa_st2, sa_xn_sp, sa_st1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDNP")
}

// LDP instruction have 15 forms:
//
//   * LDP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <Wt1>, <Wt2>, [<Xn|SP>], #<imm>
//   * LDP  <Wt1>, <Wt2>, [<Xn|SP>, #<imm>]!
//   * LDP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
//   * LDP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
//   * LDP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <Dt1>, <Dt2>, [<Xn|SP>], #<imm>
//   * LDP  <Dt1>, <Dt2>, [<Xn|SP>, #<imm>]!
//   * LDP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <Qt1>, <Qt2>, [<Xn|SP>], #<imm>
//   * LDP  <Qt1>, <Qt2>, [<Xn|SP>, #<imm>]!
//   * LDP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
//   * LDP  <St1>, <St2>, [<Xn|SP>], #<imm>
//   * LDP  <St1>, <St2>, [<Xn|SP>, #<imm>]!
//
func (self *Program) LDP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDP", 3, Operands { v0, v1, v2 })
    // LDP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(0, 0, 1, sa_imm, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDP  <Wt1>, <Wt2>, [<Xn|SP>], #<imm>
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(0, 0, 1, sa_imm_1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDP  <Wt1>, <Wt2>, [<Xn|SP>, #<imm>]!
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(0, 0, 1, sa_imm_1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstpair_off(2, 0, 1, sa_imm_2, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_post(2, 0, 1, sa_imm_3, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(2, 0, 1, sa_imm_3, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(1, 1, 1, sa_imm, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // LDP  <Dt1>, <Dt2>, [<Xn|SP>], #<imm>
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(1, 1, 1, sa_imm_1, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // LDP  <Dt1>, <Dt2>, [<Xn|SP>, #<imm>]!
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(1, 1, 1, sa_imm_1, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // LDP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstpair_off(2, 1, 1, sa_imm_2, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // LDP  <Qt1>, <Qt2>, [<Xn|SP>], #<imm>
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_post(2, 1, 1, sa_imm_3, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // LDP  <Qt1>, <Qt2>, [<Xn|SP>, #<imm>]!
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(2, 1, 1, sa_imm_3, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // LDP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_4 := uint32(moffs(v2))
        return p.setins(ldstpair_off(0, 1, 1, sa_imm_4, sa_st2, sa_xn_sp, sa_st1))
    }
    // LDP  <St1>, <St2>, [<Xn|SP>], #<imm>
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_5 := uint32(moffs(v2))
        return p.setins(ldstpair_post(0, 1, 1, sa_imm_5, sa_st2, sa_xn_sp, sa_st1))
    }
    // LDP  <St1>, <St2>, [<Xn|SP>, #<imm>]!
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_5 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(0, 1, 1, sa_imm_5, sa_st2, sa_xn_sp, sa_st1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDP")
}

// LDPSW instruction have 3 forms:
//
//   * LDPSW  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * LDPSW  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
//   * LDPSW  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
//
func (self *Program) LDPSW(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDPSW", 3, Operands { v0, v1, v2 })
    // LDPSW  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(1, 0, 1, sa_imm, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDPSW  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(1, 0, 1, sa_imm_1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // LDPSW  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(1, 0, 1, sa_imm_1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDPSW")
}

// LDR instruction have 34 forms:
//
//   * LDR  <Wt>, [<Xn|SP>], #<simm>
//   * LDR  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Wt>, <label>
//   * LDR  <Xt>, [<Xn|SP>], #<simm>
//   * LDR  <Xt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Xt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Xt>, <label>
//   * LDR  <Bt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * LDR  <Bt>, [<Xn|SP>], #<simm>
//   * LDR  <Bt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Bt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Bt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * LDR  <Dt>, [<Xn|SP>], #<simm>
//   * LDR  <Dt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Dt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Dt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Dt>, <label>
//   * LDR  <Ht>, [<Xn|SP>], #<simm>
//   * LDR  <Ht>, [<Xn|SP>, #<simm>]!
//   * LDR  <Ht>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Ht>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Qt>, [<Xn|SP>], #<simm>
//   * LDR  <Qt>, [<Xn|SP>, #<simm>]!
//   * LDR  <Qt>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <Qt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <Qt>, <label>
//   * LDR  <St>, [<Xn|SP>], #<simm>
//   * LDR  <St>, [<Xn|SP>, #<simm>]!
//   * LDR  <St>, [<Xn|SP>{, #<pimm>}]
//   * LDR  <St>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDR  <St>, <label>
//
func (self *Program) LDR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDR", 2, Operands { v0, v1 })
    // LDR  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDR  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDR  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 0, 1, sa_pimm, sa_xn_sp, sa_wt))
    }
    // LDR  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 0, 1, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDR  <Wt>, <label>
    if isWr(v0) && isLabel(v1) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(0, 0, uint32(sa_label.RelativeTo(pc)), sa_wt) })
    }
    // LDR  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(3, 0, 1, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDR  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(3, 0, 1, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDR  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_1 := uint32(moffs(v1))
        return p.setins(ldst_pos(3, 0, 1, sa_pimm_1, sa_xn_sp, sa_xt))
    }
    // LDR  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_1 uint32
        var sa_extend uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount_1 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(3, 0, 1, sa_xm, sa_extend, sa_amount_1, sa_xn_sp, sa_xt))
    }
    // LDR  <Xt>, <label>
    if isXr(v0) && isLabel(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(1, 0, uint32(sa_label.RelativeTo(pc)), sa_xt) })
    }
    // LDR  <Bt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isBr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 1, 1, sa_xm, 3, sa_amount, sa_xn_sp, sa_bt))
    }
    // LDR  <Bt>, [<Xn|SP>], #<simm>
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 1, 1, sa_simm, sa_xn_sp, sa_bt))
    }
    // LDR  <Bt>, [<Xn|SP>, #<simm>]!
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 1, 1, sa_simm, sa_xn_sp, sa_bt))
    }
    // LDR  <Bt>, [<Xn|SP>{, #<pimm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 1, 1, sa_pimm, sa_xn_sp, sa_bt))
    }
    // LDR  <Bt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 1, 1, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_bt))
    }
    // LDR  <Dt>, [<Xn|SP>], #<simm>
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(3, 1, 1, sa_simm, sa_xn_sp, sa_dt))
    }
    // LDR  <Dt>, [<Xn|SP>, #<simm>]!
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(3, 1, 1, sa_simm, sa_xn_sp, sa_dt))
    }
    // LDR  <Dt>, [<Xn|SP>{, #<pimm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_1 := uint32(moffs(v1))
        return p.setins(ldst_pos(3, 1, 1, sa_pimm_1, sa_xn_sp, sa_dt))
    }
    // LDR  <Dt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isDr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_1 uint32
        var sa_extend_1 uint32
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_1 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(3, 1, 1, sa_xm, sa_extend_1, sa_amount_1, sa_xn_sp, sa_dt))
    }
    // LDR  <Dt>, <label>
    if isDr(v0) && isLabel(v1) {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(1, 1, uint32(sa_label.RelativeTo(pc)), sa_dt) })
    }
    // LDR  <Ht>, [<Xn|SP>], #<simm>
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 1, 1, sa_simm, sa_xn_sp, sa_ht))
    }
    // LDR  <Ht>, [<Xn|SP>, #<simm>]!
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 1, 1, sa_simm, sa_xn_sp, sa_ht))
    }
    // LDR  <Ht>, [<Xn|SP>{, #<pimm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_2 := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 1, 1, sa_pimm_2, sa_xn_sp, sa_ht))
    }
    // LDR  <Ht>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isHr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_2 uint32
        var sa_extend_1 uint32
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_2 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 1, 1, sa_xm, sa_extend_1, sa_amount_2, sa_xn_sp, sa_ht))
    }
    // LDR  <Qt>, [<Xn|SP>], #<simm>
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 1, 3, sa_simm, sa_xn_sp, sa_qt))
    }
    // LDR  <Qt>, [<Xn|SP>, #<simm>]!
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 1, 3, sa_simm, sa_xn_sp, sa_qt))
    }
    // LDR  <Qt>, [<Xn|SP>{, #<pimm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_3 := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 1, 3, sa_pimm_3, sa_xn_sp, sa_qt))
    }
    // LDR  <Qt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isQr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_3 uint32
        var sa_extend_1 uint32
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_3 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 1, 3, sa_xm, sa_extend_1, sa_amount_3, sa_xn_sp, sa_qt))
    }
    // LDR  <Qt>, <label>
    if isQr(v0) && isLabel(v1) {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(2, 1, uint32(sa_label.RelativeTo(pc)), sa_qt) })
    }
    // LDR  <St>, [<Xn|SP>], #<simm>
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 1, 1, sa_simm, sa_xn_sp, sa_st))
    }
    // LDR  <St>, [<Xn|SP>, #<simm>]!
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 1, 1, sa_simm, sa_xn_sp, sa_st))
    }
    // LDR  <St>, [<Xn|SP>{, #<pimm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_4 := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 1, 1, sa_pimm_4, sa_xn_sp, sa_st))
    }
    // LDR  <St>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isSr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_4 uint32
        var sa_extend_1 uint32
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_4 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 1, 1, sa_xm, sa_extend_1, sa_amount_4, sa_xn_sp, sa_st))
    }
    // LDR  <St>, <label>
    if isSr(v0) && isLabel(v1) {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(0, 1, uint32(sa_label.RelativeTo(pc)), sa_st) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDR")
}

// LDRAA instruction have 2 forms:
//
//   * LDRAA  <Xt>, [<Xn|SP>{, #<simm>}]!
//   * LDRAA  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDRAA(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRAA", 2, Operands { v0, v1 })
    // LDRAA  <Xt>, [<Xn|SP>{, #<simm>}]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_pac(3, 0, 0, (sa_simm >> 9) & 0x1, sa_simm & 0x1ff, 1, sa_xn_sp, sa_xt))
    }
    // LDRAA  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_pac(3, 0, 0, (sa_simm >> 9) & 0x1, sa_simm & 0x1ff, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRAA")
}

// LDRAB instruction have 2 forms:
//
//   * LDRAB  <Xt>, [<Xn|SP>{, #<simm>}]!
//   * LDRAB  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDRAB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRAB", 2, Operands { v0, v1 })
    // LDRAB  <Xt>, [<Xn|SP>{, #<simm>}]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_pac(3, 0, 1, (sa_simm >> 9) & 0x1, sa_simm & 0x1ff, 1, sa_xn_sp, sa_xt))
    }
    // LDRAB  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_pac(3, 0, 1, (sa_simm >> 9) & 0x1, sa_simm & 0x1ff, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRAB")
}

// LDRB instruction have 5 forms:
//
//   * LDRB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * LDRB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * LDRB  <Wt>, [<Xn|SP>], #<simm>
//   * LDRB  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDRB  <Wt>, [<Xn|SP>{, #<pimm>}]
//
func (self *Program) LDRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRB", 2, Operands { v0, v1 })
    // LDRB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 0, 1, sa_xm, 3, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 0, 1, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRB  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRB  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRB  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 0, 1, sa_pimm, sa_xn_sp, sa_wt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRB")
}

// LDRH instruction have 4 forms:
//
//   * LDRH  <Wt>, [<Xn|SP>], #<simm>
//   * LDRH  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDRH  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * LDRH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//
func (self *Program) LDRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRH", 2, Operands { v0, v1 })
    // LDRH  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRH  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRH  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 0, 1, sa_pimm, sa_xn_sp, sa_wt))
    }
    // LDRH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 0, 1, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRH")
}

// LDRSB instruction have 10 forms:
//
//   * LDRSB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * LDRSB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * LDRSB  <Wt>, [<Xn|SP>], #<simm>
//   * LDRSB  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDRSB  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * LDRSB  <Xt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * LDRSB  <Xt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * LDRSB  <Xt>, [<Xn|SP>], #<simm>
//   * LDRSB  <Xt>, [<Xn|SP>, #<simm>]!
//   * LDRSB  <Xt>, [<Xn|SP>{, #<pimm>}]
//
func (self *Program) LDRSB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRSB", 2, Operands { v0, v1 })
    // LDRSB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 0, 3, sa_xm, 3, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 0, 3, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 0, 3, sa_pimm, sa_xn_sp, sa_wt))
    }
    // LDRSB  <Xt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 0, 2, sa_xm, 3, sa_amount, sa_xn_sp, sa_xt))
    }
    // LDRSB  <Xt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 0, 2, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_xt))
    }
    // LDRSB  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSB  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSB  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 0, 2, sa_pimm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRSB")
}

// LDRSH instruction have 8 forms:
//
//   * LDRSH  <Wt>, [<Xn|SP>], #<simm>
//   * LDRSH  <Wt>, [<Xn|SP>, #<simm>]!
//   * LDRSH  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * LDRSH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDRSH  <Xt>, [<Xn|SP>], #<simm>
//   * LDRSH  <Xt>, [<Xn|SP>, #<simm>]!
//   * LDRSH  <Xt>, [<Xn|SP>{, #<pimm>}]
//   * LDRSH  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//
func (self *Program) LDRSH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRSH", 2, Operands { v0, v1 })
    // LDRSH  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRSH  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDRSH  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 0, 3, sa_pimm, sa_xn_sp, sa_wt))
    }
    // LDRSH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 0, 3, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // LDRSH  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSH  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSH  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 0, 2, sa_pimm, sa_xn_sp, sa_xt))
    }
    // LDRSH  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 0, 2, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRSH")
}

// LDRSW instruction have 5 forms:
//
//   * LDRSW  <Xt>, [<Xn|SP>], #<simm>
//   * LDRSW  <Xt>, [<Xn|SP>, #<simm>]!
//   * LDRSW  <Xt>, [<Xn|SP>{, #<pimm>}]
//   * LDRSW  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * LDRSW  <Xt>, <label>
//
func (self *Program) LDRSW(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDRSW", 2, Operands { v0, v1 })
    // LDRSW  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSW  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDRSW  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 0, 2, sa_pimm, sa_xn_sp, sa_xt))
    }
    // LDRSW  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 0, 2, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_xt))
    }
    // LDRSW  <Xt>, <label>
    if isXr(v0) && isLabel(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(2, 0, uint32(sa_label.RelativeTo(pc)), sa_xt) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDRSW")
}

// LDSET instruction have 2 forms:
//
//   * LDSET  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSET  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSET", 3, Operands { v0, v1, v2 })
    // LDSET  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    // LDSET  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 3, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSET")
}

// LDSETA instruction have 2 forms:
//
//   * LDSETA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSETA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSETA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETA", 3, Operands { v0, v1, v2 })
    // LDSETA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    // LDSETA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 3, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETA")
}

// LDSETAB instruction have one single form:
//
//   * LDSETAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETAB")
}

// LDSETAH instruction have one single form:
//
//   * LDSETAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETAH")
}

// LDSETAL instruction have 2 forms:
//
//   * LDSETAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSETAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSETAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETAL", 3, Operands { v0, v1, v2 })
    // LDSETAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    // LDSETAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 3, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETAL")
}

// LDSETALB instruction have one single form:
//
//   * LDSETALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETALB")
}

// LDSETALH instruction have one single form:
//
//   * LDSETALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETALH")
}

// LDSETB instruction have one single form:
//
//   * LDSETB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETB")
}

// LDSETH instruction have one single form:
//
//   * LDSETH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETH")
}

// LDSETL instruction have 2 forms:
//
//   * LDSETL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSETL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSETL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETL", 3, Operands { v0, v1, v2 })
    // LDSETL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    // LDSETL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 3, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETL")
}

// LDSETLB instruction have one single form:
//
//   * LDSETLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETLB")
}

// LDSETLH instruction have one single form:
//
//   * LDSETLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSETLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 3, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETLH")
}

// LDSETP instruction have one single form:
//
//   * LDSETP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDSETP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 0, sa_xt2, 0, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETP")
}

// LDSETPA instruction have one single form:
//
//   * LDSETPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDSETPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 0, sa_xt2, 0, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETPA")
}

// LDSETPAL instruction have one single form:
//
//   * LDSETPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDSETPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 1, sa_xt2, 0, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETPAL")
}

// LDSETPL instruction have one single form:
//
//   * LDSETPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) LDSETPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSETPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 1, sa_xt2, 0, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSETPL")
}

// LDSMAX instruction have 2 forms:
//
//   * LDSMAX  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMAX  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMAX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAX", 3, Operands { v0, v1, v2 })
    // LDSMAX  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    // LDSMAX  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 4, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAX")
}

// LDSMAXA instruction have 2 forms:
//
//   * LDSMAXA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMAXA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMAXA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXA", 3, Operands { v0, v1, v2 })
    // LDSMAXA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    // LDSMAXA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 4, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXA")
}

// LDSMAXAB instruction have one single form:
//
//   * LDSMAXAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXAB")
}

// LDSMAXAH instruction have one single form:
//
//   * LDSMAXAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXAH")
}

// LDSMAXAL instruction have 2 forms:
//
//   * LDSMAXAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMAXAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMAXAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXAL", 3, Operands { v0, v1, v2 })
    // LDSMAXAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    // LDSMAXAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 4, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXAL")
}

// LDSMAXALB instruction have one single form:
//
//   * LDSMAXALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXALB")
}

// LDSMAXALH instruction have one single form:
//
//   * LDSMAXALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXALH")
}

// LDSMAXB instruction have one single form:
//
//   * LDSMAXB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXB")
}

// LDSMAXH instruction have one single form:
//
//   * LDSMAXH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXH")
}

// LDSMAXL instruction have 2 forms:
//
//   * LDSMAXL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMAXL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMAXL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXL", 3, Operands { v0, v1, v2 })
    // LDSMAXL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    // LDSMAXL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 4, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXL")
}

// LDSMAXLB instruction have one single form:
//
//   * LDSMAXLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXLB")
}

// LDSMAXLH instruction have one single form:
//
//   * LDSMAXLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMAXLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMAXLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 4, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMAXLH")
}

// LDSMIN instruction have 2 forms:
//
//   * LDSMIN  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMIN  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMIN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMIN", 3, Operands { v0, v1, v2 })
    // LDSMIN  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    // LDSMIN  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 5, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMIN")
}

// LDSMINA instruction have 2 forms:
//
//   * LDSMINA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMINA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMINA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINA", 3, Operands { v0, v1, v2 })
    // LDSMINA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    // LDSMINA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 5, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINA")
}

// LDSMINAB instruction have one single form:
//
//   * LDSMINAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINAB")
}

// LDSMINAH instruction have one single form:
//
//   * LDSMINAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINAH")
}

// LDSMINAL instruction have 2 forms:
//
//   * LDSMINAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMINAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMINAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINAL", 3, Operands { v0, v1, v2 })
    // LDSMINAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    // LDSMINAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 5, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINAL")
}

// LDSMINALB instruction have one single form:
//
//   * LDSMINALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINALB")
}

// LDSMINALH instruction have one single form:
//
//   * LDSMINALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINALH")
}

// LDSMINB instruction have one single form:
//
//   * LDSMINB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINB")
}

// LDSMINH instruction have one single form:
//
//   * LDSMINH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINH")
}

// LDSMINL instruction have 2 forms:
//
//   * LDSMINL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDSMINL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDSMINL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINL", 3, Operands { v0, v1, v2 })
    // LDSMINL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    // LDSMINL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 5, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINL")
}

// LDSMINLB instruction have one single form:
//
//   * LDSMINLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINLB")
}

// LDSMINLH instruction have one single form:
//
//   * LDSMINLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDSMINLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDSMINLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 5, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDSMINLH")
}

// LDTR instruction have 2 forms:
//
//   * LDTR  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDTR  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDTR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDTR", 2, Operands { v0, v1 })
    // LDTR  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(2, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDTR  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(3, 0, 1, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDTR")
}

// LDTRB instruction have one single form:
//
//   * LDTRB  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDTRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDTRB", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(0, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDTRB")
}

// LDTRH instruction have one single form:
//
//   * LDTRH  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDTRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDTRH", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(1, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDTRH")
}

// LDTRSB instruction have 2 forms:
//
//   * LDTRSB  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDTRSB  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDTRSB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDTRSB", 2, Operands { v0, v1 })
    // LDTRSB  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(0, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDTRSB  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(0, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDTRSB")
}

// LDTRSH instruction have 2 forms:
//
//   * LDTRSH  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDTRSH  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDTRSH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDTRSH", 2, Operands { v0, v1 })
    // LDTRSH  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(1, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDTRSH  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(1, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDTRSH")
}

// LDTRSW instruction have one single form:
//
//   * LDTRSW  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDTRSW(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDTRSW", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(2, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDTRSW")
}

// LDUMAX instruction have 2 forms:
//
//   * LDUMAX  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMAX  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMAX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAX", 3, Operands { v0, v1, v2 })
    // LDUMAX  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    // LDUMAX  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 6, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAX")
}

// LDUMAXA instruction have 2 forms:
//
//   * LDUMAXA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMAXA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMAXA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXA", 3, Operands { v0, v1, v2 })
    // LDUMAXA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    // LDUMAXA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 6, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXA")
}

// LDUMAXAB instruction have one single form:
//
//   * LDUMAXAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXAB")
}

// LDUMAXAH instruction have one single form:
//
//   * LDUMAXAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXAH")
}

// LDUMAXAL instruction have 2 forms:
//
//   * LDUMAXAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMAXAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMAXAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXAL", 3, Operands { v0, v1, v2 })
    // LDUMAXAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    // LDUMAXAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 6, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXAL")
}

// LDUMAXALB instruction have one single form:
//
//   * LDUMAXALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXALB")
}

// LDUMAXALH instruction have one single form:
//
//   * LDUMAXALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXALH")
}

// LDUMAXB instruction have one single form:
//
//   * LDUMAXB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXB")
}

// LDUMAXH instruction have one single form:
//
//   * LDUMAXH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXH")
}

// LDUMAXL instruction have 2 forms:
//
//   * LDUMAXL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMAXL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMAXL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXL", 3, Operands { v0, v1, v2 })
    // LDUMAXL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    // LDUMAXL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 6, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXL")
}

// LDUMAXLB instruction have one single form:
//
//   * LDUMAXLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXLB")
}

// LDUMAXLH instruction have one single form:
//
//   * LDUMAXLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMAXLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMAXLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 6, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMAXLH")
}

// LDUMIN instruction have 2 forms:
//
//   * LDUMIN  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMIN  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMIN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMIN", 3, Operands { v0, v1, v2 })
    // LDUMIN  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    // LDUMIN  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 0, 7, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMIN")
}

// LDUMINA instruction have 2 forms:
//
//   * LDUMINA  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMINA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMINA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINA", 3, Operands { v0, v1, v2 })
    // LDUMINA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    // LDUMINA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 0, 7, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINA")
}

// LDUMINAB instruction have one single form:
//
//   * LDUMINAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINAB")
}

// LDUMINAH instruction have one single form:
//
//   * LDUMINAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINAH")
}

// LDUMINAL instruction have 2 forms:
//
//   * LDUMINAL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMINAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMINAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINAL", 3, Operands { v0, v1, v2 })
    // LDUMINAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    // LDUMINAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 0, 7, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINAL")
}

// LDUMINALB instruction have one single form:
//
//   * LDUMINALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINALB")
}

// LDUMINALH instruction have one single form:
//
//   * LDUMINALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINALH")
}

// LDUMINB instruction have one single form:
//
//   * LDUMINB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINB")
}

// LDUMINH instruction have one single form:
//
//   * LDUMINH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINH")
}

// LDUMINL instruction have 2 forms:
//
//   * LDUMINL  <Ws>, <Wt>, [<Xn|SP>]
//   * LDUMINL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) LDUMINL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINL", 3, Operands { v0, v1, v2 })
    // LDUMINL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    // LDUMINL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 0, 7, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINL")
}

// LDUMINLB instruction have one single form:
//
//   * LDUMINLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINLB")
}

// LDUMINLH instruction have one single form:
//
//   * LDUMINLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) LDUMINLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDUMINLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 0, 7, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDUMINLH")
}

// LDUR instruction have 7 forms:
//
//   * LDUR  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDUR  <Xt>, [<Xn|SP>{, #<simm>}]
//   * LDUR  <Bt>, [<Xn|SP>{, #<simm>}]
//   * LDUR  <Dt>, [<Xn|SP>{, #<simm>}]
//   * LDUR  <Ht>, [<Xn|SP>{, #<simm>}]
//   * LDUR  <Qt>, [<Xn|SP>{, #<simm>}]
//   * LDUR  <St>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDUR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDUR", 2, Operands { v0, v1 })
    // LDUR  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(2, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDUR  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(3, 0, 1, sa_simm, sa_xn_sp, sa_xt))
    }
    // LDUR  <Bt>, [<Xn|SP>{, #<simm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 1, 1, sa_simm, sa_xn_sp, sa_bt))
    }
    // LDUR  <Dt>, [<Xn|SP>{, #<simm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(3, 1, 1, sa_simm, sa_xn_sp, sa_dt))
    }
    // LDUR  <Ht>, [<Xn|SP>{, #<simm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(1, 1, 1, sa_simm, sa_xn_sp, sa_ht))
    }
    // LDUR  <Qt>, [<Xn|SP>{, #<simm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 1, 3, sa_simm, sa_xn_sp, sa_qt))
    }
    // LDUR  <St>, [<Xn|SP>{, #<simm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(2, 1, 1, sa_simm, sa_xn_sp, sa_st))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDUR")
}

// LDURB instruction have one single form:
//
//   * LDURB  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDURB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDURB", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDURB")
}

// LDURH instruction have one single form:
//
//   * LDURH  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDURH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDURH", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(1, 0, 1, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDURH")
}

// LDURSB instruction have 2 forms:
//
//   * LDURSB  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDURSB  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDURSB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDURSB", 2, Operands { v0, v1 })
    // LDURSB  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDURSB  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDURSB")
}

// LDURSH instruction have 2 forms:
//
//   * LDURSH  <Wt>, [<Xn|SP>{, #<simm>}]
//   * LDURSH  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDURSH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDURSH", 2, Operands { v0, v1 })
    // LDURSH  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(1, 0, 3, sa_simm, sa_xn_sp, sa_wt))
    }
    // LDURSH  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(1, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDURSH")
}

// LDURSW instruction have one single form:
//
//   * LDURSW  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) LDURSW(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDURSW", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(2, 0, 2, sa_simm, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDURSW")
}

// LDXP instruction have 2 forms:
//
//   * LDXP  <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
//   * LDXP  <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
//
func (self *Program) LDXP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LDXP", 3, Operands { v0, v1, v2 })
    // LDXP  <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclp(0, 1, 31, 0, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // LDXP  <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclp(1, 1, 31, 0, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDXP")
}

// LDXR instruction have 2 forms:
//
//   * LDXR  <Wt>, [<Xn|SP>{,#0}]
//   * LDXR  <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDXR(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDXR", 2, Operands { v0, v1 })
    // LDXR  <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(2, 1, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    // LDXR  <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(3, 1, 31, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LDXR")
}

// LDXRB instruction have one single form:
//
//   * LDXRB  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDXRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDXRB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(0, 1, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDXRB")
}

// LDXRH instruction have one single form:
//
//   * LDXRH  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) LDXRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("LDXRH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstexclr(1, 1, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for LDXRH")
}

// LSLV instruction have 2 forms:
//
//   * LSLV  <Wd>, <Wn>, <Wm>
//   * LSLV  <Xd>, <Xn>, <Xm>
//
func (self *Program) LSLV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LSLV", 3, Operands { v0, v1, v2 })
    // LSLV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 8, sa_wn, sa_wd))
    }
    // LSLV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 8, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LSLV")
}

// LSRV instruction have 2 forms:
//
//   * LSRV  <Wd>, <Wn>, <Wm>
//   * LSRV  <Xd>, <Xn>, <Xm>
//
func (self *Program) LSRV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("LSRV", 3, Operands { v0, v1, v2 })
    // LSRV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 9, sa_wn, sa_wd))
    }
    // LSRV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 9, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for LSRV")
}

// MADD instruction have 2 forms:
//
//   * MADD  <Wd>, <Wn>, <Wm>, <Wa>
//   * MADD  <Xd>, <Xn>, <Xm>, <Xa>
//
func (self *Program) MADD(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("MADD", 4, Operands { v0, v1, v2, v3 })
    // MADD  <Wd>, <Wn>, <Wm>, <Wa>
    if isWr(v0) && isWr(v1) && isWr(v2) && isWr(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_wa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(0, 0, 0, sa_wm, 0, sa_wa, sa_wn, sa_wd))
    }
    // MADD  <Xd>, <Xn>, <Xm>, <Xa>
    if isXr(v0) && isXr(v1) && isXr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 0, sa_xm, 0, sa_xa, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MADD")
}

// MOVK instruction have 2 forms:
//
//   * MOVK  <Wd>, #<imm>{, LSL #<shift>}
//   * MOVK  <Xd>, #<imm>{, LSL #<shift>}
//
func (self *Program) MOVK(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("MOVK", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("MOVK", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction MOVK takes 2 or 3 operands")
    }
    // MOVK  <Wd>, #<imm>{, LSL #<shift>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isUimm16(v1) && (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm16(v1)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(movewide(0, 3, sa_shift, sa_imm, sa_wd))
    }
    // MOVK  <Xd>, #<imm>{, LSL #<shift>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isUimm16(v1) && (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_shift_1 uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm16(v1)
        if len(vv) == 1 {
            sa_shift_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(movewide(1, 3, sa_shift_1, sa_imm, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MOVK")
}

// MOVN instruction have 2 forms:
//
//   * MOVN  <Wd>, #<imm>{, LSL #<shift>}
//   * MOVN  <Xd>, #<imm>{, LSL #<shift>}
//
func (self *Program) MOVN(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("MOVN", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("MOVN", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction MOVN takes 2 or 3 operands")
    }
    // MOVN  <Wd>, #<imm>{, LSL #<shift>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isUimm16(v1) && (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm16(v1)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(movewide(0, 0, sa_shift, sa_imm, sa_wd))
    }
    // MOVN  <Xd>, #<imm>{, LSL #<shift>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isUimm16(v1) && (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_shift_1 uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm16(v1)
        if len(vv) == 1 {
            sa_shift_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(movewide(1, 0, sa_shift_1, sa_imm, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MOVN")
}

// MOVZ instruction have 2 forms:
//
//   * MOVZ  <Wd>, #<imm>{, LSL #<shift>}
//   * MOVZ  <Xd>, #<imm>{, LSL #<shift>}
//
func (self *Program) MOVZ(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("MOVZ", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("MOVZ", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction MOVZ takes 2 or 3 operands")
    }
    // MOVZ  <Wd>, #<imm>{, LSL #<shift>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isUimm16(v1) && (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm16(v1)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(movewide(0, 2, sa_shift, sa_imm, sa_wd))
    }
    // MOVZ  <Xd>, #<imm>{, LSL #<shift>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isUimm16(v1) && (len(vv) == 0 || isSameMod(vv[0], LSL(0))) {
        var sa_shift_1 uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_imm := asUimm16(v1)
        if len(vv) == 1 {
            sa_shift_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(movewide(1, 2, sa_shift_1, sa_imm, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MOVZ")
}

// MRRS instruction have one single form:
//
//   * MRRS  <Xt>, <Xt+1>, (<systemreg>|S<op0>_<op1>_<Cn>_<Cm>_<op2>)
//
func (self *Program) MRRS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("MRRS", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXr(v1) && isNextReg(v1, v0, 1) && isSysReg(v2) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_systemreg := uint32(v2.(SystemRegister))
        return p.setins(systemmovepr(
            1,
            (sa_systemreg >> 6) & 0x1,
            (sa_systemreg >> 3) & 0x7,
            (sa_systemreg >> 7) & 0xf,
            (sa_systemreg >> 11) & 0xf,
            sa_systemreg & 0x7,
            sa_xt,
        ))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for MRRS")
}

// MRS instruction have one single form:
//
//   * MRS  <Xt>, (<systemreg>|S<op0>_<op1>_<Cn>_<Cm>_<op2>)
//
func (self *Program) MRS(v0, v1 interface{}) *Instruction {
    p := self.alloc("MRS", 2, Operands { v0, v1 })
    if isXr(v0) && isSysReg(v1) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_systemreg := uint32(v1.(SystemRegister))
        return p.setins(systemmove(
            1,
            (sa_systemreg >> 6) & 0x1,
            (sa_systemreg >> 3) & 0x7,
            (sa_systemreg >> 7) & 0xf,
            (sa_systemreg >> 11) & 0xf,
            sa_systemreg & 0x7,
            sa_xt,
        ))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for MRS")
}

// MSR instruction have 2 forms:
//
//   * MSR  <pstatefield>, #<imm>
//   * MSR  (<systemreg>|S<op0>_<op1>_<Cn>_<Cm>_<op2>), <Xt>
//
func (self *Program) MSR(v0, v1 interface{}) *Instruction {
    p := self.alloc("MSR", 2, Operands { v0, v1 })
    // MSR  <pstatefield>, #<imm>
    if isPState(v0) && isUimm4(v1) {
        sa_pstatefield := uint32(v0.(PStateField))
        sa_imm := asUimm4(v1)
        if sa_imm != sa_pstatefield & 0xf {
            panic("aarch64: invalid combination of operands for MSR")
        }
        return p.setins(pstate((sa_pstatefield >> 7) & 0x7, sa_imm, (sa_pstatefield >> 4) & 0x7, 31))
    }
    // MSR  (<systemreg>|S<op0>_<op1>_<Cn>_<Cm>_<op2>), <Xt>
    if isSysReg(v0) && isXr(v1) {
        sa_systemreg := uint32(v0.(SystemRegister))
        sa_xt := uint32(v1.(asm.Register).ID())
        return p.setins(systemmove(
            0,
            (sa_systemreg >> 6) & 0x1,
            (sa_systemreg >> 3) & 0x7,
            (sa_systemreg >> 7) & 0xf,
            (sa_systemreg >> 11) & 0xf,
            sa_systemreg & 0x7,
            sa_xt,
        ))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MSR")
}

// MSRR instruction have one single form:
//
//   * MSRR  (<systemreg>|S<op0>_<op1>_<Cn>_<Cm>_<op2>), <Xt>, <Xt+1>
//
func (self *Program) MSRR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("MSRR", 3, Operands { v0, v1, v2 })
    if isSysReg(v0) && isXr(v1) && isXr(v2) && isNextReg(v2, v1, 1) {
        sa_systemreg := uint32(v0.(SystemRegister))
        sa_xt := uint32(v1.(asm.Register).ID())
        return p.setins(systemmovepr(
            0,
            (sa_systemreg >> 6) & 0x1,
            (sa_systemreg >> 3) & 0x7,
            (sa_systemreg >> 7) & 0xf,
            (sa_systemreg >> 11) & 0xf,
            sa_systemreg & 0x7,
            sa_xt,
        ))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for MSRR")
}

// MSUB instruction have 2 forms:
//
//   * MSUB  <Wd>, <Wn>, <Wm>, <Wa>
//   * MSUB  <Xd>, <Xn>, <Xm>, <Xa>
//
func (self *Program) MSUB(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("MSUB", 4, Operands { v0, v1, v2, v3 })
    // MSUB  <Wd>, <Wn>, <Wm>, <Wa>
    if isWr(v0) && isWr(v1) && isWr(v2) && isWr(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_wa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(0, 0, 0, sa_wm, 1, sa_wa, sa_wn, sa_wd))
    }
    // MSUB  <Xd>, <Xn>, <Xm>, <Xa>
    if isXr(v0) && isXr(v1) && isXr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 0, sa_xm, 1, sa_xa, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for MSUB")
}

// NOP instruction have one single form:
//
//   * NOP
//
func (self *Program) NOP() *Instruction {
    p := self.alloc("NOP", 0, Operands {})
    return p.setins(hints(0, 0))
}

// ORN instruction have 2 forms:
//
//   * ORN  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ORN  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) ORN(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ORN", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ORN", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ORN takes 3 or 4 operands")
    }
    // ORN  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 1, sa_shift, 1, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ORN  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 1, sa_shift, 1, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ORN")
}

// ORR instruction have 4 forms:
//
//   * ORR  <Wd|WSP>, <Wn>, #<imm>
//   * ORR  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * ORR  <Xd|SP>, <Xn>, #<imm>
//   * ORR  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) ORR(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("ORR", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("ORR", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction ORR takes 3 or 4 operands")
    }
    // ORR  <Wd|WSP>, <Wn>, #<imm>
    if isWrOrWSP(v0) && isWr(v1) && isMask32(v2) {
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_imm := asMaskOp(v2)
        return p.setins(log_imm(0, 1, 0, (sa_imm >> 6) & 0x3f, sa_imm & 0x3f, sa_wn, sa_wd_wsp))
    }
    // ORR  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(0, 1, sa_shift, 0, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // ORR  <Xd|SP>, <Xn>, #<imm>
    if isXrOrSP(v0) && isXr(v1) && isMask64(v2) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_imm_1 := asMaskOp(v2)
        return p.setins(log_imm(1, 1, (sa_imm_1 >> 12) & 0x1, (sa_imm_1 >> 6) & 0x3f, sa_imm_1 & 0x3f, sa_xn, sa_xd_sp))
    }
    // ORR  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(log_shift(1, 1, sa_shift, 0, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ORR")
}

// PACDA instruction have one single form:
//
//   * PACDA  <Xd>, <Xn|SP>
//
func (self *Program) PACDA(v0, v1 interface{}) *Instruction {
    p := self.alloc("PACDA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 2, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACDA")
}

// PACDB instruction have one single form:
//
//   * PACDB  <Xd>, <Xn|SP>
//
func (self *Program) PACDB(v0, v1 interface{}) *Instruction {
    p := self.alloc("PACDB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 3, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACDB")
}

// PACDZA instruction have one single form:
//
//   * PACDZA  <Xd>
//
func (self *Program) PACDZA(v0 interface{}) *Instruction {
    p := self.alloc("PACDZA", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 10, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACDZA")
}

// PACDZB instruction have one single form:
//
//   * PACDZB  <Xd>
//
func (self *Program) PACDZB(v0 interface{}) *Instruction {
    p := self.alloc("PACDZB", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 11, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACDZB")
}

// PACGA instruction have one single form:
//
//   * PACGA  <Xd>, <Xn>, <Xm|SP>
//
func (self *Program) PACGA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("PACGA", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXr(v1) && isXrOrSP(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm_sp := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm_sp, 12, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACGA")
}

// PACIA instruction have one single form:
//
//   * PACIA  <Xd>, <Xn|SP>
//
func (self *Program) PACIA(v0, v1 interface{}) *Instruction {
    p := self.alloc("PACIA", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 0, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACIA")
}

// PACIA1716 instruction have one single form:
//
//   * PACIA1716
//
func (self *Program) PACIA1716() *Instruction {
    p := self.alloc("PACIA1716", 0, Operands {})
    return p.setins(hints(1, 0))
}

// PACIASP instruction have one single form:
//
//   * PACIASP
//
func (self *Program) PACIASP() *Instruction {
    p := self.alloc("PACIASP", 0, Operands {})
    return p.setins(hints(3, 1))
}

// PACIAZ instruction have one single form:
//
//   * PACIAZ
//
func (self *Program) PACIAZ() *Instruction {
    p := self.alloc("PACIAZ", 0, Operands {})
    return p.setins(hints(3, 0))
}

// PACIB instruction have one single form:
//
//   * PACIB  <Xd>, <Xn|SP>
//
func (self *Program) PACIB(v0, v1 interface{}) *Instruction {
    p := self.alloc("PACIB", 2, Operands { v0, v1 })
    if isXr(v0) && isXrOrSP(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 1, sa_xn_sp, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACIB")
}

// PACIB1716 instruction have one single form:
//
//   * PACIB1716
//
func (self *Program) PACIB1716() *Instruction {
    p := self.alloc("PACIB1716", 0, Operands {})
    return p.setins(hints(1, 2))
}

// PACIBSP instruction have one single form:
//
//   * PACIBSP
//
func (self *Program) PACIBSP() *Instruction {
    p := self.alloc("PACIBSP", 0, Operands {})
    return p.setins(hints(3, 3))
}

// PACIBZ instruction have one single form:
//
//   * PACIBZ
//
func (self *Program) PACIBZ() *Instruction {
    p := self.alloc("PACIBZ", 0, Operands {})
    return p.setins(hints(3, 2))
}

// PACIZA instruction have one single form:
//
//   * PACIZA  <Xd>
//
func (self *Program) PACIZA(v0 interface{}) *Instruction {
    p := self.alloc("PACIZA", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 8, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACIZA")
}

// PACIZB instruction have one single form:
//
//   * PACIZB  <Xd>
//
func (self *Program) PACIZB(v0 interface{}) *Instruction {
    p := self.alloc("PACIZB", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 9, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PACIZB")
}

// PRFM instruction have 3 forms:
//
//   * PRFM  (<prfop>|#<imm5>), [<Xn|SP>{, #<pimm>}]
//   * PRFM  (<prfop>|#<imm5>), [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * PRFM  (<prfop>|#<imm5>), <label>
//
func (self *Program) PRFM(v0, v1 interface{}) *Instruction {
    p := self.alloc("PRFM", 2, Operands { v0, v1 })
    // PRFM  (<prfop>|#<imm5>), [<Xn|SP>{, #<pimm>}]
    if isBasicPrf(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_prfop := v0.(PrefetchOp).encode()
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(3, 0, 2, sa_pimm, sa_xn_sp, sa_prfop))
    }
    // PRFM  (<prfop>|#<imm5>), [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isBasicPrf(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_prfop := v0.(PrefetchOp).encode()
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(3, 0, 2, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_prfop))
    }
    // PRFM  (<prfop>|#<imm5>), <label>
    if isBasicPrf(v0) && isLabel(v1) {
        sa_prfop := v0.(PrefetchOp).encode()
        sa_label := v1.(*asm.Label)
        return p.setenc(func(pc uintptr) uint32 { return loadlit(3, 0, uint32(sa_label.RelativeTo(pc)), sa_prfop) })
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for PRFM")
}

// PRFUM instruction have one single form:
//
//   * PRFUM (<prfop>|#<imm5>), [<Xn|SP>{, #<simm>}]
//
func (self *Program) PRFUM(v0, v1 interface{}) *Instruction {
    p := self.alloc("PRFUM", 2, Operands { v0, v1 })
    if isBasicPrf(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_prfop := v0.(PrefetchOp).encode()
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(3, 0, 2, sa_simm, sa_xn_sp, sa_prfop))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PRFUM")
}

// PSB instruction have one single form:
//
//   * PSB CSYNC
//
func (self *Program) PSB(v0 interface{}) *Instruction {
    p := self.alloc("PSB", 1, Operands { v0 })
    if v0 == CSYNC {
        return p.setins(hints(2, 1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for PSB")
}

// RBIT instruction have 2 forms:
//
//   * RBIT  <Wd>, <Wn>
//   * RBIT  <Xd>, <Xn>
//
func (self *Program) RBIT(v0, v1 interface{}) *Instruction {
    p := self.alloc("RBIT", 2, Operands { v0, v1 })
    // RBIT  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 0, sa_wn, sa_wd))
    }
    // RBIT  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 0, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for RBIT")
}

// RCWCAS instruction have one single form:
//
//   * RCWCAS  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCAS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCAS", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(0, 0, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCAS")
}

// RCWCASA instruction have one single form:
//
//   * RCWCASA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCASA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCASA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(0, 1, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASA")
}

// RCWCASAL instruction have one single form:
//
//   * RCWCASAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCASAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCASAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(0, 1, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASAL")
}

// RCWCASL instruction have one single form:
//
//   * RCWCASL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCASL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCASL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(0, 0, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASL")
}

// RCWCASP instruction have one single form:
//
//   * RCWCASP  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWCASP(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWCASP", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(0, 0, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASP")
}

// RCWCASPA instruction have one single form:
//
//   * RCWCASPA  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWCASPA(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWCASPA", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(0, 1, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASPA")
}

// RCWCASPAL instruction have one single form:
//
//   * RCWCASPAL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWCASPAL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWCASPAL", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(0, 1, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASPAL")
}

// RCWCASPL instruction have one single form:
//
//   * RCWCASPL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWCASPL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWCASPL", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(0, 0, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCASPL")
}

// RCWCLR instruction have one single form:
//
//   * RCWCLR  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCLR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLR", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLR")
}

// RCWCLRA instruction have one single form:
//
//   * RCWCLRA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCLRA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRA")
}

// RCWCLRAL instruction have one single form:
//
//   * RCWCLRAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCLRAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRAL")
}

// RCWCLRL instruction have one single form:
//
//   * RCWCLRL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWCLRL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRL")
}

// RCWCLRP instruction have one single form:
//
//   * RCWCLRP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWCLRP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 0, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRP")
}

// RCWCLRPA instruction have one single form:
//
//   * RCWCLRPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWCLRPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 0, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRPA")
}

// RCWCLRPAL instruction have one single form:
//
//   * RCWCLRPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWCLRPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 1, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRPAL")
}

// RCWCLRPL instruction have one single form:
//
//   * RCWCLRPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWCLRPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWCLRPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 1, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWCLRPL")
}

// RCWSCAS instruction have one single form:
//
//   * RCWSCAS  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCAS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCAS", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(1, 0, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCAS")
}

// RCWSCASA instruction have one single form:
//
//   * RCWSCASA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCASA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCASA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(1, 1, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASA")
}

// RCWSCASAL instruction have one single form:
//
//   * RCWSCASAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCASAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCASAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(1, 1, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASAL")
}

// RCWSCASL instruction have one single form:
//
//   * RCWSCASL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCASL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCASL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(rcwcomswap(1, 0, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASL")
}

// RCWSCASP instruction have one single form:
//
//   * RCWSCASP  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWSCASP(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWSCASP", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(1, 0, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASP")
}

// RCWSCASPA instruction have one single form:
//
//   * RCWSCASPA  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWSCASPA(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWSCASPA", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(1, 1, 0, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASPA")
}

// RCWSCASPAL instruction have one single form:
//
//   * RCWSCASPAL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWSCASPAL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWSCASPAL", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(1, 1, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASPAL")
}

// RCWSCASPL instruction have one single form:
//
//   * RCWSCASPL  <Xs>, <X(s+1)>, <Xt>, <X(t+1)>, [<Xn|SP>]
//
func (self *Program) RCWSCASPL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("RCWSCASPL", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) &&
       isXr(v1) &&
       isNextReg(v1, v0, 1) &&
       isXr(v2) &&
       isXr(v3) &&
       isNextReg(v3, v2, 1) &&
       isMem(v4) &&
       isXrOrSP(mbase(v4)) &&
       midx(v4) == nil &&
       moffs(v4) == 0 &&
       mext(v4) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v4).ID())
        return p.setins(rcwcomswappr(1, 0, 1, sa_xs, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCASPL")
}

// RCWSCLR instruction have one single form:
//
//   * RCWSCLR  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCLR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLR", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLR")
}

// RCWSCLRA instruction have one single form:
//
//   * RCWSCLRA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCLRA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRA")
}

// RCWSCLRAL instruction have one single form:
//
//   * RCWSCLRAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCLRAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRAL")
}

// RCWSCLRL instruction have one single form:
//
//   * RCWSCLRL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSCLRL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_xs, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRL")
}

// RCWSCLRP instruction have one single form:
//
//   * RCWSCLRP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSCLRP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 0, 0, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRP")
}

// RCWSCLRPA instruction have one single form:
//
//   * RCWSCLRPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSCLRPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 1, 0, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRPA")
}

// RCWSCLRPAL instruction have one single form:
//
//   * RCWSCLRPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSCLRPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 1, 1, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRPAL")
}

// RCWSCLRPL instruction have one single form:
//
//   * RCWSCLRPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSCLRPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSCLRPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 0, 1, sa_xt2, 1, 1, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSCLRPL")
}

// RCWSET instruction have one single form:
//
//   * RCWSET  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSET", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSET")
}

// RCWSETA instruction have one single form:
//
//   * RCWSETA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSETA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETA")
}

// RCWSETAL instruction have one single form:
//
//   * RCWSETAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSETAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETAL")
}

// RCWSETL instruction have one single form:
//
//   * RCWSETL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSETL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETL")
}

// RCWSETP instruction have one single form:
//
//   * RCWSETP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSETP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 0, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETP")
}

// RCWSETPA instruction have one single form:
//
//   * RCWSETPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSETPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 0, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETPA")
}

// RCWSETPAL instruction have one single form:
//
//   * RCWSETPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSETPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 1, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETPAL")
}

// RCWSETPL instruction have one single form:
//
//   * RCWSETPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSETPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSETPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 1, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSETPL")
}

// RCWSSET instruction have one single form:
//
//   * RCWSSET  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSET", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSET")
}

// RCWSSETA instruction have one single form:
//
//   * RCWSSETA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSETA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETA")
}

// RCWSSETAL instruction have one single form:
//
//   * RCWSSETAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSETAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETAL")
}

// RCWSSETL instruction have one single form:
//
//   * RCWSSETL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSETL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETL")
}

// RCWSSETP instruction have one single form:
//
//   * RCWSSETP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSETP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 0, 0, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETP")
}

// RCWSSETPA instruction have one single form:
//
//   * RCWSSETPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSETPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 1, 0, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETPA")
}

// RCWSSETPAL instruction have one single form:
//
//   * RCWSSETPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSETPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 1, 1, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETPAL")
}

// RCWSSETPL instruction have one single form:
//
//   * RCWSSETPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSETPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSETPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 0, 1, sa_xt2, 1, 3, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSETPL")
}

// RCWSSWP instruction have one single form:
//
//   * RCWSSWP  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSWP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWP")
}

// RCWSSWPA instruction have one single form:
//
//   * RCWSSWPA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSWPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPA")
}

// RCWSSWPAL instruction have one single form:
//
//   * RCWSSWPAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSWPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPAL")
}

// RCWSSWPL instruction have one single form:
//
//   * RCWSSWPL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSSWPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPL")
}

// RCWSSWPP instruction have one single form:
//
//   * RCWSSWPP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSWPP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 0, 0, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPP")
}

// RCWSSWPPA instruction have one single form:
//
//   * RCWSSWPPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSWPPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 1, 0, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPPA")
}

// RCWSSWPPAL instruction have one single form:
//
//   * RCWSSWPPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSWPPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 1, 1, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPPAL")
}

// RCWSSWPPL instruction have one single form:
//
//   * RCWSSWPPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSSWPPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSSWPPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(1, 0, 1, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSSWPPL")
}

// RCWSWP instruction have one single form:
//
//   * RCWSWP  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSWP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWP")
}

// RCWSWPA instruction have one single form:
//
//   * RCWSWPA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSWPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPA")
}

// RCWSWPAL instruction have one single form:
//
//   * RCWSWPAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSWPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPAL")
}

// RCWSWPL instruction have one single form:
//
//   * RCWSWPL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) RCWSWPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPL")
}

// RCWSWPP instruction have one single form:
//
//   * RCWSWPP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSWPP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 0, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPP")
}

// RCWSWPPA instruction have one single form:
//
//   * RCWSWPPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSWPPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 0, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPPA")
}

// RCWSWPPAL instruction have one single form:
//
//   * RCWSWPPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSWPPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 1, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPPAL")
}

// RCWSWPPL instruction have one single form:
//
//   * RCWSWPPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) RCWSWPPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RCWSWPPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 1, sa_xt2, 1, 2, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RCWSWPPL")
}

// RET instruction have one single form:
//
//   * RET  {<Xn>}
//
func (self *Program) RET(vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("RET", 0, Operands {})
        case 1  : p = self.alloc("RET", 1, Operands { vv[0] })
        default : panic("instruction RET takes 0 or 1 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) && (len(vv) == 0 || isXr(vv[0])) {
        var sa_xn uint32
        if len(vv) == 1 {
            sa_xn = uint32(vv[0].(asm.Register).ID())
        }
        return p.setins(branch_reg(2, 31, 0, sa_xn, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RET")
}

// RETAA instruction have one single form:
//
//   * RETAA
//
func (self *Program) RETAA() *Instruction {
    p := self.alloc("RETAA", 0, Operands {})
    return p.setins(branch_reg(2, 31, 2, 31, 31))
}

// RETAB instruction have one single form:
//
//   * RETAB
//
func (self *Program) RETAB() *Instruction {
    p := self.alloc("RETAB", 0, Operands {})
    return p.setins(branch_reg(2, 31, 3, 31, 31))
}

// REV instruction have 2 forms:
//
//   * REV  <Wd>, <Wn>
//   * REV  <Xd>, <Xn>
//
func (self *Program) REV(v0, v1 interface{}) *Instruction {
    p := self.alloc("REV", 2, Operands { v0, v1 })
    // REV  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 2, sa_wn, sa_wd))
    }
    // REV  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 3, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for REV")
}

// REV16 instruction have 2 forms:
//
//   * REV16  <Wd>, <Wn>
//   * REV16  <Xd>, <Xn>
//
func (self *Program) REV16(v0, v1 interface{}) *Instruction {
    p := self.alloc("REV16", 2, Operands { v0, v1 })
    // REV16  <Wd>, <Wn>
    if isWr(v0) && isWr(v1) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(0, 0, 0, 1, sa_wn, sa_wd))
    }
    // REV16  <Xd>, <Xn>
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for REV16")
}

// REV32 instruction have one single form:
//
//   * REV32  <Xd>, <Xn>
//
func (self *Program) REV32(v0, v1 interface{}) *Instruction {
    p := self.alloc("REV32", 2, Operands { v0, v1 })
    if isXr(v0) && isXr(v1) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 0, 2, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for REV32")
}

// RMIF instruction have one single form:
//
//   * RMIF  <Xn>, #<shift>, #<mask>
//
func (self *Program) RMIF(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RMIF", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isUimm6(v1) && isUimm4(v2) {
        sa_xn := uint32(v0.(asm.Register).ID())
        sa_shift := asUimm6(v1)
        sa_mask := asUimm4(v2)
        return p.setins(rmif(1, 0, 1, sa_shift, sa_xn, 0, sa_mask))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RMIF")
}

// RORV instruction have 2 forms:
//
//   * RORV  <Wd>, <Wn>, <Wm>
//   * RORV  <Xd>, <Xn>, <Xm>
//
func (self *Program) RORV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RORV", 3, Operands { v0, v1, v2 })
    // RORV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 11, sa_wn, sa_wd))
    }
    // RORV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 11, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for RORV")
}

// RPRFM instruction have one single form:
//
//   * RPRFM  (<rprfop>|#<imm6>), <Xm>, [<Xn|SP>]
//
func (self *Program) RPRFM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("RPRFM", 3, Operands { v0, v1, v2 })
    if isRangePrf(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_rprfop := v0.(RangePrefetchOp).encode()
        sa_xm := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldst_regoff(
            3,
            0,
            2,
            sa_xm,
            sa_rprfop & 0x7,
            (sa_rprfop >> 3) & 0x1,
            sa_xn_sp,
            (sa_rprfop >> 4) & 0x1f,
        ))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for RPRFM")
}

// SB instruction have one single form:
//
//   * SB
//
func (self *Program) SB() *Instruction {
    p := self.alloc("SB", 0, Operands {})
    return p.setins(barriers(0, 7, 31))
}

// SBC instruction have 2 forms:
//
//   * SBC  <Wd>, <Wn>, <Wm>
//   * SBC  <Xd>, <Xn>, <Xm>
//
func (self *Program) SBC(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SBC", 3, Operands { v0, v1, v2 })
    // SBC  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(0, 1, 0, sa_wm, sa_wn, sa_wd))
    }
    // SBC  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(1, 1, 0, sa_xm, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SBC")
}

// SBCS instruction have 2 forms:
//
//   * SBCS  <Wd>, <Wn>, <Wm>
//   * SBCS  <Xd>, <Xn>, <Xm>
//
func (self *Program) SBCS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SBCS", 3, Operands { v0, v1, v2 })
    // SBCS  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(0, 1, 1, sa_wm, sa_wn, sa_wd))
    }
    // SBCS  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(addsub_carry(1, 1, 1, sa_xm, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SBCS")
}

// SBFM instruction have 2 forms:
//
//   * SBFM  <Wd>, <Wn>, #<immr>, #<imms>
//   * SBFM  <Xd>, <Xn>, #<immr>, #<imms>
//
func (self *Program) SBFM(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("SBFM", 4, Operands { v0, v1, v2, v3 })
    // SBFM  <Wd>, <Wn>, #<immr>, #<imms>
    if isWr(v0) && isWr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_immr := asUimm6(v2)
        sa_imms := asUimm6(v3)
        return p.setins(bitfield(0, 0, 0, sa_immr, sa_imms, sa_wn, sa_wd))
    }
    // SBFM  <Xd>, <Xn>, #<immr>, #<imms>
    if isXr(v0) && isXr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_immr_1 := asUimm6(v2)
        sa_imms_1 := asUimm6(v3)
        return p.setins(bitfield(1, 0, 1, sa_immr_1, sa_imms_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SBFM")
}

// SCVTF instruction have 12 forms:
//
//   * SCVTF  <Dd>, <Wn>, #<fbits>
//   * SCVTF  <Dd>, <Wn>
//   * SCVTF  <Dd>, <Xn>, #<fbits>
//   * SCVTF  <Dd>, <Xn>
//   * SCVTF  <Hd>, <Wn>, #<fbits>
//   * SCVTF  <Hd>, <Wn>
//   * SCVTF  <Hd>, <Xn>, #<fbits>
//   * SCVTF  <Hd>, <Xn>
//   * SCVTF  <Sd>, <Wn>, #<fbits>
//   * SCVTF  <Sd>, <Wn>
//   * SCVTF  <Sd>, <Xn>, #<fbits>
//   * SCVTF  <Sd>, <Xn>
//
func (self *Program) SCVTF(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("SCVTF", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("SCVTF", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction SCVTF takes 2 or 3 operands")
    }
    // SCVTF  <Dd>, <Wn>, #<fbits>
    if len(vv) == 1 && isDr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 1, 0, 2, sa_fbits, sa_wn, sa_dd))
    }
    // SCVTF  <Dd>, <Wn>
    if isDr(v0) && isWr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 2, sa_wn, sa_dd))
    }
    // SCVTF  <Dd>, <Xn>, #<fbits>
    if len(vv) == 1 && isDr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 1, 0, 2, sa_fbits_1, sa_xn, sa_dd))
    }
    // SCVTF  <Dd>, <Xn>
    if isDr(v0) && isXr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 2, sa_xn, sa_dd))
    }
    // SCVTF  <Hd>, <Wn>, #<fbits>
    if len(vv) == 1 && isHr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 3, 0, 2, sa_fbits, sa_wn, sa_hd))
    }
    // SCVTF  <Hd>, <Wn>
    if isHr(v0) && isWr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 2, sa_wn, sa_hd))
    }
    // SCVTF  <Hd>, <Xn>, #<fbits>
    if len(vv) == 1 && isHr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 3, 0, 2, sa_fbits_1, sa_xn, sa_hd))
    }
    // SCVTF  <Hd>, <Xn>
    if isHr(v0) && isXr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 2, sa_xn, sa_hd))
    }
    // SCVTF  <Sd>, <Wn>, #<fbits>
    if len(vv) == 1 && isSr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 0, 0, 2, sa_fbits, sa_wn, sa_sd))
    }
    // SCVTF  <Sd>, <Wn>
    if isSr(v0) && isWr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 2, sa_wn, sa_sd))
    }
    // SCVTF  <Sd>, <Xn>, #<fbits>
    if len(vv) == 1 && isSr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 0, 0, 2, sa_fbits_1, sa_xn, sa_sd))
    }
    // SCVTF  <Sd>, <Xn>
    if isSr(v0) && isXr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 2, sa_xn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SCVTF")
}

// SDIV instruction have 2 forms:
//
//   * SDIV  <Wd>, <Wn>, <Wm>
//   * SDIV  <Xd>, <Xn>, <Xm>
//
func (self *Program) SDIV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SDIV", 3, Operands { v0, v1, v2 })
    // SDIV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 3, sa_wn, sa_wd))
    }
    // SDIV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 3, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SDIV")
}

// SETE instruction have one single form:
//
//   * SETE  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETE", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 8, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETE")
}

// SETEN instruction have one single form:
//
//   * SETEN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETEN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETEN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 10, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETEN")
}

// SETET instruction have one single form:
//
//   * SETET  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETET", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 9, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETET")
}

// SETETN instruction have one single form:
//
//   * SETETN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETETN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETETN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 11, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETETN")
}

// SETF16 instruction have one single form:
//
//   * SETF16  <Wn>
//
func (self *Program) SETF16(v0 interface{}) *Instruction {
    p := self.alloc("SETF16", 1, Operands { v0 })
    if isWr(v0) {
        sa_wn := uint32(v0.(asm.Register).ID())
        return p.setins(setf(0, 0, 1, 0, 1, sa_wn, 0, 13))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETF16")
}

// SETF8 instruction have one single form:
//
//   * SETF8  <Wn>
//
func (self *Program) SETF8(v0 interface{}) *Instruction {
    p := self.alloc("SETF8", 1, Operands { v0 })
    if isWr(v0) {
        sa_wn := uint32(v0.(asm.Register).ID())
        return p.setins(setf(0, 0, 1, 0, 0, sa_wn, 0, 13))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETF8")
}

// SETGE instruction have one single form:
//
//   * SETGE  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGE(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGE", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs_1, 8, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGE")
}

// SETGEN instruction have one single form:
//
//   * SETGEN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGEN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGEN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs_1, 10, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGEN")
}

// SETGET instruction have one single form:
//
//   * SETGET  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGET(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGET", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs_1, 9, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGET")
}

// SETGETN instruction have one single form:
//
//   * SETGETN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGETN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGETN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_2 := uint32(v1.(asm.Register).ID())
        sa_xs_1 := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs_1, 11, sa_xn_2, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGETN")
}

// SETGM instruction have one single form:
//
//   * SETGM  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGM", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 4, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGM")
}

// SETGMN instruction have one single form:
//
//   * SETGMN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGMN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGMN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 6, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGMN")
}

// SETGMT instruction have one single form:
//
//   * SETGMT  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGMT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGMT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 5, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGMT")
}

// SETGMTN instruction have one single form:
//
//   * SETGMTN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGMTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGMTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 7, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGMTN")
}

// SETGP instruction have one single form:
//
//   * SETGP  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGP", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 0, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGP")
}

// SETGPN instruction have one single form:
//
//   * SETGPN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGPN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGPN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 2, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGPN")
}

// SETGPT instruction have one single form:
//
//   * SETGPT  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGPT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGPT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 1, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGPT")
}

// SETGPTN instruction have one single form:
//
//   * SETGPTN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETGPTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETGPTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 1, 3, sa_xs, 3, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETGPTN")
}

// SETM instruction have one single form:
//
//   * SETM  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETM(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETM", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 4, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETM")
}

// SETMN instruction have one single form:
//
//   * SETMN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETMN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETMN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 6, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETMN")
}

// SETMT instruction have one single form:
//
//   * SETMT  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETMT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETMT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 5, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETMT")
}

// SETMTN instruction have one single form:
//
//   * SETMTN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETMTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETMTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd_1 := uint32(mbase(v0).ID())
        sa_xn_1 := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 7, sa_xn_1, sa_xd_1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETMTN")
}

// SETP instruction have one single form:
//
//   * SETP  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETP", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 0, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETP")
}

// SETPN instruction have one single form:
//
//   * SETPN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETPN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETPN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 2, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETPN")
}

// SETPT instruction have one single form:
//
//   * SETPT  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETPT(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETPT", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 1, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETPT")
}

// SETPTN instruction have one single form:
//
//   * SETPTN  [<Xd>]!, <Xn>!, <Xs>
//
func (self *Program) SETPTN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SETPTN", 3, Operands { v0, v1, v2 })
    if isMem(v0) &&
       isXr(mbase(v0)) &&
       midx(v0) == nil &&
       moffs(v0) == 0 &&
       mext(v0) == PreIndex &&
       isXr(v1) &&
       isXr(v2) {
        sa_xd := uint32(mbase(v0).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xs := uint32(v2.(asm.Register).ID())
        return p.setins(memcms(0, 0, 3, sa_xs, 3, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SETPTN")
}

// SEV instruction have one single form:
//
//   * SEV
//
func (self *Program) SEV() *Instruction {
    p := self.alloc("SEV", 0, Operands {})
    return p.setins(hints(0, 4))
}

// SEVL instruction have one single form:
//
//   * SEVL
//
func (self *Program) SEVL() *Instruction {
    p := self.alloc("SEVL", 0, Operands {})
    return p.setins(hints(0, 5))
}

// SMADDL instruction have one single form:
//
//   * SMADDL  <Xd>, <Wn>, <Wm>, <Xa>
//
func (self *Program) SMADDL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("SMADDL", 4, Operands { v0, v1, v2, v3 })
    if isXr(v0) && isWr(v1) && isWr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 1, sa_wm, 0, sa_xa, sa_wn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMADDL")
}

// SMAX instruction have 4 forms:
//
//   * SMAX  <Wd>, <Wn>, <Wm>
//   * SMAX  <Wd>, <Wn>, #<simm>
//   * SMAX  <Xd>, <Xn>, <Xm>
//   * SMAX  <Xd>, <Xn>, #<simm>
//
func (self *Program) SMAX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMAX", 3, Operands { v0, v1, v2 })
    // SMAX  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 24, sa_wn, sa_wd))
    }
    // SMAX  <Wd>, <Wn>, #<simm>
    if isWr(v0) && isWr(v1) && isFpImm8(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_simm := asFpImm8(v2)
        return p.setins(minmax_imm(0, 0, 0, 0, sa_simm, sa_wn, sa_wd))
    }
    // SMAX  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 24, sa_xn, sa_xd))
    }
    // SMAX  <Xd>, <Xn>, #<simm>
    if isXr(v0) && isXr(v1) && isFpImm8(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_simm := asFpImm8(v2)
        return p.setins(minmax_imm(1, 0, 0, 0, sa_simm, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SMAX")
}

// SMC instruction have one single form:
//
//   * SMC  #<imm>
//
func (self *Program) SMC(v0 interface{}) *Instruction {
    p := self.alloc("SMC", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(0, sa_imm, 0, 3))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMC")
}

// SMIN instruction have 4 forms:
//
//   * SMIN  <Wd>, <Wn>, <Wm>
//   * SMIN  <Wd>, <Wn>, #<simm>
//   * SMIN  <Xd>, <Xn>, <Xm>
//   * SMIN  <Xd>, <Xn>, #<simm>
//
func (self *Program) SMIN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMIN", 3, Operands { v0, v1, v2 })
    // SMIN  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 26, sa_wn, sa_wd))
    }
    // SMIN  <Wd>, <Wn>, #<simm>
    if isWr(v0) && isWr(v1) && isFpImm8(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_simm := asFpImm8(v2)
        return p.setins(minmax_imm(0, 0, 0, 2, sa_simm, sa_wn, sa_wd))
    }
    // SMIN  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 26, sa_xn, sa_xd))
    }
    // SMIN  <Xd>, <Xn>, #<simm>
    if isXr(v0) && isXr(v1) && isFpImm8(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_simm := asFpImm8(v2)
        return p.setins(minmax_imm(1, 0, 0, 2, sa_simm, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SMIN")
}

// SMSUBL instruction have one single form:
//
//   * SMSUBL  <Xd>, <Wn>, <Wm>, <Xa>
//
func (self *Program) SMSUBL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("SMSUBL", 4, Operands { v0, v1, v2, v3 })
    if isXr(v0) && isWr(v1) && isWr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 1, sa_wm, 1, sa_xa, sa_wn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMSUBL")
}

// SMULH instruction have one single form:
//
//   * SMULH  <Xd>, <Xn>, <Xm>
//
func (self *Program) SMULH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SMULH", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 2, sa_xm, 0, 31, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SMULH")
}

// ST2G instruction have 3 forms:
//
//   * ST2G  <Xt|SP>, [<Xn|SP>{, #<simm>}]
//   * ST2G  <Xt|SP>, [<Xn|SP>], #<simm>
//   * ST2G  <Xt|SP>, [<Xn|SP>, #<simm>]!
//
func (self *Program) ST2G(v0, v1 interface{}) *Instruction {
    p := self.alloc("ST2G", 2, Operands { v0, v1 })
    // ST2G  <Xt|SP>, [<Xn|SP>{, #<simm>}]
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(2, sa_simm, 2, Rn, Rt))
    }
    // ST2G  <Xt|SP>, [<Xn|SP>], #<simm>
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(2, sa_simm, 1, Rn, Rt))
    }
    // ST2G  <Xt|SP>, [<Xn|SP>, #<simm>]!
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(2, sa_simm, 3, Rn, Rt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for ST2G")
}

// ST64B instruction have one single form:
//
//   * ST64B  <Xt>, [<Xn|SP> {,#0}]
//
func (self *Program) ST64B(v0, v1 interface{}) *Instruction {
    p := self.alloc("ST64B", 2, Operands { v0, v1 })
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(memop(3, 0, 0, 0, 31, 1, 1, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ST64B")
}

// ST64BV instruction have one single form:
//
//   * ST64BV  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) ST64BV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ST64BV", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 1, 3, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ST64BV")
}

// ST64BV0 instruction have one single form:
//
//   * ST64BV0  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) ST64BV0(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("ST64BV0", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 1, 2, sa_xn_sp, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for ST64BV0")
}

// STG instruction have 3 forms:
//
//   * STG  <Xt|SP>, [<Xn|SP>{, #<simm>}]
//   * STG  <Xt|SP>, [<Xn|SP>], #<simm>
//   * STG  <Xt|SP>, [<Xn|SP>, #<simm>]!
//
func (self *Program) STG(v0, v1 interface{}) *Instruction {
    p := self.alloc("STG", 2, Operands { v0, v1 })
    // STG  <Xt|SP>, [<Xn|SP>{, #<simm>}]
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(0, sa_simm, 2, Rn, Rt))
    }
    // STG  <Xt|SP>, [<Xn|SP>], #<simm>
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(0, sa_simm, 1, Rn, Rt))
    }
    // STG  <Xt|SP>, [<Xn|SP>, #<simm>]!
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(0, sa_simm, 3, Rn, Rt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STG")
}

// STGM instruction have one single form:
//
//   * STGM  <Xt>, [<Xn|SP>]
//
func (self *Program) STGM(v0, v1 interface{}) *Instruction {
    p := self.alloc("STGM", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt
        return p.setins(ldsttags(2, 0, 0, Rn, Rt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STGM")
}

// STGP instruction have 3 forms:
//
//   * STGP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * STGP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
//   * STGP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
//
func (self *Program) STGP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STGP", 3, Operands { v0, v1, v2 })
    // STGP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        imm7 := uint32(0b0000000)
        imm7 |= sa_imm
        Rt2 := uint32(0b00000)
        Rt2 |= sa_xt2
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt1
        return p.setins(ldstpair_off(1, 0, 0, imm7, Rt2, Rn, Rt))
    }
    // STGP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        imm7 := uint32(0b0000000)
        imm7 |= sa_imm_1
        Rt2 := uint32(0b00000)
        Rt2 |= sa_xt2
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt1
        return p.setins(ldstpair_post(1, 0, 0, imm7, Rt2, Rn, Rt))
    }
    // STGP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        imm7 := uint32(0b0000000)
        imm7 |= sa_imm_1
        Rt2 := uint32(0b00000)
        Rt2 |= sa_xt2
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt1
        return p.setins(ldstpair_pre(1, 0, 0, imm7, Rt2, Rn, Rt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STGP")
}

// STILP instruction have 4 forms:
//
//   * STILP  <Wt1>, <Wt2>, [<Xn|SP>, #-8]!
//   * STILP  <Wt1>, <Wt2>, [<Xn|SP>]
//   * STILP  <Xt1>, <Xt2>, [<Xn|SP>, #-16]!
//   * STILP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) STILP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STILP", 3, Operands { v0, v1, v2 })
    // STILP  <Wt1>, <Wt2>, [<Xn|SP>, #-8]!
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == -8 &&
       mext(v2) == PreIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(2, 0, sa_wt2, 0, sa_xn_sp, sa_wt1))
    }
    // STILP  <Wt1>, <Wt2>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(2, 0, sa_wt2, 1, sa_xn_sp, sa_wt1))
    }
    // STILP  <Xt1>, <Xt2>, [<Xn|SP>, #-16]!
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == -16 &&
       mext(v2) == PreIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(3, 0, sa_xt2, 0, sa_xn_sp, sa_xt1))
    }
    // STILP  <Xt1>, <Xt2>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldiappstilp(3, 0, sa_xt2, 1, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STILP")
}

// STLLR instruction have 2 forms:
//
//   * STLLR  <Wt>, [<Xn|SP>{,#0}]
//   * STLLR  <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLLR(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLLR", 2, Operands { v0, v1 })
    // STLLR  <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(2, 0, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    // STLLR  <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(3, 0, 31, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STLLR")
}

// STLLRB instruction have one single form:
//
//   * STLLRB  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLLRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLLRB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(0, 0, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLLRB")
}

// STLLRH instruction have one single form:
//
//   * STLLRH  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLLRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLLRH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(1, 0, 31, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLLRH")
}

// STLR instruction have 4 forms:
//
//   * STLR  <Wt>, [<Xn|SP>, #-4]!
//   * STLR  <Xt>, [<Xn|SP>, #-8]!
//   * STLR  <Wt>, [<Xn|SP>{,#0}]
//   * STLR  <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLR(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLR", 2, Operands { v0, v1 })
    // STLR  <Wt>, [<Xn|SP>, #-4]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == -4 && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldapstl_writeback(2, 0, sa_xn_sp, sa_wt))
    }
    // STLR  <Xt>, [<Xn|SP>, #-8]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == -8 && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldapstl_writeback(3, 0, sa_xn_sp, sa_xt))
    }
    // STLR  <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(2, 0, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    // STLR  <Xt>, [<Xn|SP>{,#0}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(3, 0, 31, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STLR")
}

// STLRB instruction have one single form:
//
//   * STLRB  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLRB", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(0, 0, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLRB")
}

// STLRH instruction have one single form:
//
//   * STLRH  <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLRH", 2, Operands { v0, v1 })
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       midx(v1) == nil &&
       (moffs(v1) == 0 || moffs(v1) == 0) &&
       mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        return p.setins(ldstord(1, 0, 31, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLRH")
}

// STLUR instruction have 7 forms:
//
//   * STLUR  <Wt>, [<Xn|SP>{, #<simm>}]
//   * STLUR  <Xt>, [<Xn|SP>{, #<simm>}]
//   * STLUR  <Bt>, [<Xn|SP>{, #<simm>}]
//   * STLUR  <Dt>, [<Xn|SP>{, #<simm>}]
//   * STLUR  <Ht>, [<Xn|SP>{, #<simm>}]
//   * STLUR  <Qt>, [<Xn|SP>{, #<simm>}]
//   * STLUR  <St>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STLUR(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLUR", 2, Operands { v0, v1 })
    // STLUR  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(2, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STLUR  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(3, 0, sa_simm, sa_xn_sp, sa_xt))
    }
    // STLUR  <Bt>, [<Xn|SP>{, #<simm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(0, 0, sa_simm, sa_xn_sp, sa_bt))
    }
    // STLUR  <Dt>, [<Xn|SP>{, #<simm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(3, 0, sa_simm, sa_xn_sp, sa_dt))
    }
    // STLUR  <Ht>, [<Xn|SP>{, #<simm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(1, 0, sa_simm, sa_xn_sp, sa_ht))
    }
    // STLUR  <Qt>, [<Xn|SP>{, #<simm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(0, 2, sa_simm, sa_xn_sp, sa_qt))
    }
    // STLUR  <St>, [<Xn|SP>{, #<simm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_simd(2, 0, sa_simm, sa_xn_sp, sa_st))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STLUR")
}

// STLURB instruction have one single form:
//
//   * STLURB  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STLURB(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLURB", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLURB")
}

// STLURH instruction have one single form:
//
//   * STLURH  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STLURH(v0, v1 interface{}) *Instruction {
    p := self.alloc("STLURH", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldapstl_unscaled(1, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLURH")
}

// STLXP instruction have 2 forms:
//
//   * STLXP  <Ws>, <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
//   * STLXP  <Ws>, <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
//
func (self *Program) STLXP(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("STLXP", 4, Operands { v0, v1, v2, v3 })
    // STLXP  <Ws>, <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isWr(v2) &&
       isMem(v3) &&
       isXrOrSP(mbase(v3)) &&
       midx(v3) == nil &&
       (moffs(v3) == 0 || moffs(v3) == 0) &&
       mext(v3) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt1 := uint32(v1.(asm.Register).ID())
        sa_wt2 := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v3).ID())
        return p.setins(ldstexclp(0, 0, sa_ws, 1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // STLXP  <Ws>, <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isXr(v1) &&
       isXr(v2) &&
       isMem(v3) &&
       isXrOrSP(mbase(v3)) &&
       midx(v3) == nil &&
       (moffs(v3) == 0 || moffs(v3) == 0) &&
       mext(v3) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_xt1 := uint32(v1.(asm.Register).ID())
        sa_xt2 := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v3).ID())
        return p.setins(ldstexclp(1, 0, sa_ws, 1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STLXP")
}

// STLXR instruction have 2 forms:
//
//   * STLXR  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * STLXR  <Ws>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLXR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STLXR", 3, Operands { v0, v1, v2 })
    // STLXR  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(2, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    // STLXR  <Ws>, <Xt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(3, 0, sa_ws, 1, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STLXR")
}

// STLXRB instruction have one single form:
//
//   * STLXRB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLXRB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STLXRB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(0, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLXRB")
}

// STLXRH instruction have one single form:
//
//   * STLXRH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STLXRH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STLXRH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(1, 0, sa_ws, 1, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STLXRH")
}

// STNP instruction have 5 forms:
//
//   * STNP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
//   * STNP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * STNP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
//   * STNP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
//   * STNP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
//
func (self *Program) STNP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STNP", 3, Operands { v0, v1, v2 })
    // STNP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(0, 0, 0, sa_imm, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // STNP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(2, 0, 0, sa_imm_1, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // STNP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(1, 1, 0, sa_imm, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // STNP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(2, 1, 0, sa_imm_1, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // STNP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstnapair_offs(0, 1, 0, sa_imm_2, sa_st2, sa_xn_sp, sa_st1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STNP")
}

// STP instruction have 15 forms:
//
//   * STP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
//   * STP  <Wt1>, <Wt2>, [<Xn|SP>], #<imm>
//   * STP  <Wt1>, <Wt2>, [<Xn|SP>, #<imm>]!
//   * STP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
//   * STP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
//   * STP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
//   * STP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
//   * STP  <Dt1>, <Dt2>, [<Xn|SP>], #<imm>
//   * STP  <Dt1>, <Dt2>, [<Xn|SP>, #<imm>]!
//   * STP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
//   * STP  <Qt1>, <Qt2>, [<Xn|SP>], #<imm>
//   * STP  <Qt1>, <Qt2>, [<Xn|SP>, #<imm>]!
//   * STP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
//   * STP  <St1>, <St2>, [<Xn|SP>], #<imm>
//   * STP  <St1>, <St2>, [<Xn|SP>, #<imm>]!
//
func (self *Program) STP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STP", 3, Operands { v0, v1, v2 })
    // STP  <Wt1>, <Wt2>, [<Xn|SP>{, #<imm>}]
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(0, 0, 0, sa_imm, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // STP  <Wt1>, <Wt2>, [<Xn|SP>], #<imm>
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(0, 0, 0, sa_imm_1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // STP  <Wt1>, <Wt2>, [<Xn|SP>, #<imm>]!
    if isWr(v0) && isWr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_wt1 := uint32(v0.(asm.Register).ID())
        sa_wt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(0, 0, 0, sa_imm_1, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // STP  <Xt1>, <Xt2>, [<Xn|SP>{, #<imm>}]
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstpair_off(2, 0, 0, sa_imm_2, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // STP  <Xt1>, <Xt2>, [<Xn|SP>], #<imm>
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_post(2, 0, 0, sa_imm_3, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // STP  <Xt1>, <Xt2>, [<Xn|SP>, #<imm>]!
    if isXr(v0) && isXr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(2, 0, 0, sa_imm_3, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // STP  <Dt1>, <Dt2>, [<Xn|SP>{, #<imm>}]
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm := uint32(moffs(v2))
        return p.setins(ldstpair_off(1, 1, 0, sa_imm, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // STP  <Dt1>, <Dt2>, [<Xn|SP>], #<imm>
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_post(1, 1, 0, sa_imm_1, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // STP  <Dt1>, <Dt2>, [<Xn|SP>, #<imm>]!
    if isDr(v0) && isDr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_dt1 := uint32(v0.(asm.Register).ID())
        sa_dt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_1 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(1, 1, 0, sa_imm_1, sa_dt2, sa_xn_sp, sa_dt1))
    }
    // STP  <Qt1>, <Qt2>, [<Xn|SP>{, #<imm>}]
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_2 := uint32(moffs(v2))
        return p.setins(ldstpair_off(2, 1, 0, sa_imm_2, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // STP  <Qt1>, <Qt2>, [<Xn|SP>], #<imm>
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_post(2, 1, 0, sa_imm_3, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // STP  <Qt1>, <Qt2>, [<Xn|SP>, #<imm>]!
    if isQr(v0) && isQr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_qt1 := uint32(v0.(asm.Register).ID())
        sa_qt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_3 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(2, 1, 0, sa_imm_3, sa_qt2, sa_xn_sp, sa_qt1))
    }
    // STP  <St1>, <St2>, [<Xn|SP>{, #<imm>}]
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == nil {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_4 := uint32(moffs(v2))
        return p.setins(ldstpair_off(0, 1, 0, sa_imm_4, sa_st2, sa_xn_sp, sa_st1))
    }
    // STP  <St1>, <St2>, [<Xn|SP>], #<imm>
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PostIndex {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_5 := uint32(moffs(v2))
        return p.setins(ldstpair_post(0, 1, 0, sa_imm_5, sa_st2, sa_xn_sp, sa_st1))
    }
    // STP  <St1>, <St2>, [<Xn|SP>, #<imm>]!
    if isSr(v0) && isSr(v1) && isMem(v2) && isXrOrSP(mbase(v2)) && midx(v2) == nil && mext(v2) == PreIndex {
        sa_st1 := uint32(v0.(asm.Register).ID())
        sa_st2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        sa_imm_5 := uint32(moffs(v2))
        return p.setins(ldstpair_pre(0, 1, 0, sa_imm_5, sa_st2, sa_xn_sp, sa_st1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STP")
}

// STR instruction have 29 forms:
//
//   * STR  <Wt>, [<Xn|SP>], #<simm>
//   * STR  <Wt>, [<Xn|SP>, #<simm>]!
//   * STR  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * STR  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * STR  <Xt>, [<Xn|SP>], #<simm>
//   * STR  <Xt>, [<Xn|SP>, #<simm>]!
//   * STR  <Xt>, [<Xn|SP>{, #<pimm>}]
//   * STR  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * STR  <Bt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * STR  <Bt>, [<Xn|SP>], #<simm>
//   * STR  <Bt>, [<Xn|SP>, #<simm>]!
//   * STR  <Bt>, [<Xn|SP>{, #<pimm>}]
//   * STR  <Bt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * STR  <Dt>, [<Xn|SP>], #<simm>
//   * STR  <Dt>, [<Xn|SP>, #<simm>]!
//   * STR  <Dt>, [<Xn|SP>{, #<pimm>}]
//   * STR  <Dt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * STR  <Ht>, [<Xn|SP>], #<simm>
//   * STR  <Ht>, [<Xn|SP>, #<simm>]!
//   * STR  <Ht>, [<Xn|SP>{, #<pimm>}]
//   * STR  <Ht>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * STR  <Qt>, [<Xn|SP>], #<simm>
//   * STR  <Qt>, [<Xn|SP>, #<simm>]!
//   * STR  <Qt>, [<Xn|SP>{, #<pimm>}]
//   * STR  <Qt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//   * STR  <St>, [<Xn|SP>], #<simm>
//   * STR  <St>, [<Xn|SP>, #<simm>]!
//   * STR  <St>, [<Xn|SP>{, #<pimm>}]
//   * STR  <St>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//
func (self *Program) STR(v0, v1 interface{}) *Instruction {
    p := self.alloc("STR", 2, Operands { v0, v1 })
    // STR  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STR  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STR  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 0, 0, sa_pimm, sa_xn_sp, sa_wt))
    }
    // STR  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 0, 0, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // STR  <Xt>, [<Xn|SP>], #<simm>
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(3, 0, 0, sa_simm, sa_xn_sp, sa_xt))
    }
    // STR  <Xt>, [<Xn|SP>, #<simm>]!
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(3, 0, 0, sa_simm, sa_xn_sp, sa_xt))
    }
    // STR  <Xt>, [<Xn|SP>{, #<pimm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_1 := uint32(moffs(v1))
        return p.setins(ldst_pos(3, 0, 0, sa_pimm_1, sa_xn_sp, sa_xt))
    }
    // STR  <Xt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isXr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_1 uint32
        var sa_extend uint32
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount_1 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(3, 0, 0, sa_xm, sa_extend, sa_amount_1, sa_xn_sp, sa_xt))
    }
    // STR  <Bt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isBr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 1, 0, sa_xm, 3, sa_amount, sa_xn_sp, sa_bt))
    }
    // STR  <Bt>, [<Xn|SP>], #<simm>
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 1, 0, sa_simm, sa_xn_sp, sa_bt))
    }
    // STR  <Bt>, [<Xn|SP>, #<simm>]!
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 1, 0, sa_simm, sa_xn_sp, sa_bt))
    }
    // STR  <Bt>, [<Xn|SP>{, #<pimm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 1, 0, sa_pimm, sa_xn_sp, sa_bt))
    }
    // STR  <Bt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 1, 0, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_bt))
    }
    // STR  <Dt>, [<Xn|SP>], #<simm>
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(3, 1, 0, sa_simm, sa_xn_sp, sa_dt))
    }
    // STR  <Dt>, [<Xn|SP>, #<simm>]!
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(3, 1, 0, sa_simm, sa_xn_sp, sa_dt))
    }
    // STR  <Dt>, [<Xn|SP>{, #<pimm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_1 := uint32(moffs(v1))
        return p.setins(ldst_pos(3, 1, 0, sa_pimm_1, sa_xn_sp, sa_dt))
    }
    // STR  <Dt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isDr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_1 uint32
        var sa_extend_1 uint32
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_1 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(3, 1, 0, sa_xm, sa_extend_1, sa_amount_1, sa_xn_sp, sa_dt))
    }
    // STR  <Ht>, [<Xn|SP>], #<simm>
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 1, 0, sa_simm, sa_xn_sp, sa_ht))
    }
    // STR  <Ht>, [<Xn|SP>, #<simm>]!
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 1, 0, sa_simm, sa_xn_sp, sa_ht))
    }
    // STR  <Ht>, [<Xn|SP>{, #<pimm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_2 := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 1, 0, sa_pimm_2, sa_xn_sp, sa_ht))
    }
    // STR  <Ht>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isHr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_2 uint32
        var sa_extend_1 uint32
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_2 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 1, 0, sa_xm, sa_extend_1, sa_amount_2, sa_xn_sp, sa_ht))
    }
    // STR  <Qt>, [<Xn|SP>], #<simm>
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 1, 2, sa_simm, sa_xn_sp, sa_qt))
    }
    // STR  <Qt>, [<Xn|SP>, #<simm>]!
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 1, 2, sa_simm, sa_xn_sp, sa_qt))
    }
    // STR  <Qt>, [<Xn|SP>{, #<pimm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_3 := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 1, 2, sa_pimm_3, sa_xn_sp, sa_qt))
    }
    // STR  <Qt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isQr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_3 uint32
        var sa_extend_1 uint32
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_3 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 1, 2, sa_xm, sa_extend_1, sa_amount_3, sa_xn_sp, sa_qt))
    }
    // STR  <St>, [<Xn|SP>], #<simm>
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(2, 1, 0, sa_simm, sa_xn_sp, sa_st))
    }
    // STR  <St>, [<Xn|SP>, #<simm>]!
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(2, 1, 0, sa_simm, sa_xn_sp, sa_st))
    }
    // STR  <St>, [<Xn|SP>{, #<pimm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm_4 := uint32(moffs(v1))
        return p.setins(ldst_pos(2, 1, 0, sa_pimm_4, sa_xn_sp, sa_st))
    }
    // STR  <St>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isSr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount_4 uint32
        var sa_extend_1 uint32
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend_1 = uint32(mext(v1).(Extension).Extension())
            sa_amount_4 = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(2, 1, 0, sa_xm, sa_extend_1, sa_amount_4, sa_xn_sp, sa_st))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STR")
}

// STRB instruction have 5 forms:
//
//   * STRB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
//   * STRB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
//   * STRB  <Wt>, [<Xn|SP>], #<simm>
//   * STRB  <Wt>, [<Xn|SP>, #<simm>]!
//   * STRB  <Wt>, [<Xn|SP>{, #<pimm>}]
//
func (self *Program) STRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("STRB", 2, Operands { v0, v1 })
    // STRB  <Wt>, [<Xn|SP>, <Xm>{, LSL <amount>}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isXr(midx(v1)) &&
       (mext(v1) == nil || isSameMod(mext(v1), LSL(0))) {
        var sa_amount uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(0, 0, 0, sa_xm, 3, sa_amount, sa_xn_sp, sa_wt))
    }
    // STRB  <Wt>, [<Xn|SP>, (<Wm>|<Xm>), <extend> {<amount>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && moffs(v1) == 0 && isWrOrXr(midx(v1)) && isMod(mext(v1)) {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        sa_extend := uint32(mext(v1).(Extension).Extension())
        sa_amount := uint32(mext(v1).(Modifier).Amount())
        return p.setins(ldst_regoff(0, 0, 0, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // STRB  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(0, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STRB  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(0, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STRB  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(0, 0, 0, sa_pimm, sa_xn_sp, sa_wt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STRB")
}

// STRH instruction have 4 forms:
//
//   * STRH  <Wt>, [<Xn|SP>], #<simm>
//   * STRH  <Wt>, [<Xn|SP>, #<simm>]!
//   * STRH  <Wt>, [<Xn|SP>{, #<pimm>}]
//   * STRH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
//
func (self *Program) STRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("STRH", 2, Operands { v0, v1 })
    // STRH  <Wt>, [<Xn|SP>], #<simm>
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpost(1, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STRH  <Wt>, [<Xn|SP>, #<simm>]!
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_immpre(1, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STRH  <Wt>, [<Xn|SP>{, #<pimm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_pimm := uint32(moffs(v1))
        return p.setins(ldst_pos(1, 0, 0, sa_pimm, sa_xn_sp, sa_wt))
    }
    // STRH  <Wt>, [<Xn|SP>, (<Wm>|<Xm>){, <extend> {<amount>}}]
    if isWr(v0) &&
       isMem(v1) &&
       isXrOrSP(mbase(v1)) &&
       moffs(v1) == 0 &&
       isWrOrXr(midx(v1)) &&
       (mext(v1) == nil || isExtend(mext(v1))) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_xm := uint32(midx(v1).ID())
        if isMod(mext(v1)) {
            sa_extend = uint32(mext(v1).(Extension).Extension())
            sa_amount = uint32(mext(v1).(Modifier).Amount())
        }
        return p.setins(ldst_regoff(1, 0, 0, sa_xm, sa_extend, sa_amount, sa_xn_sp, sa_wt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STRH")
}

// STTR instruction have 2 forms:
//
//   * STTR  <Wt>, [<Xn|SP>{, #<simm>}]
//   * STTR  <Xt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STTR(v0, v1 interface{}) *Instruction {
    p := self.alloc("STTR", 2, Operands { v0, v1 })
    // STTR  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(2, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STTR  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(3, 0, 0, sa_simm, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STTR")
}

// STTRB instruction have one single form:
//
//   * STTRB  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STTRB(v0, v1 interface{}) *Instruction {
    p := self.alloc("STTRB", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(0, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STTRB")
}

// STTRH instruction have one single form:
//
//   * STTRH  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STTRH(v0, v1 interface{}) *Instruction {
    p := self.alloc("STTRH", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unpriv(1, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STTRH")
}

// STUR instruction have 7 forms:
//
//   * STUR  <Wt>, [<Xn|SP>{, #<simm>}]
//   * STUR  <Xt>, [<Xn|SP>{, #<simm>}]
//   * STUR  <Bt>, [<Xn|SP>{, #<simm>}]
//   * STUR  <Dt>, [<Xn|SP>{, #<simm>}]
//   * STUR  <Ht>, [<Xn|SP>{, #<simm>}]
//   * STUR  <Qt>, [<Xn|SP>{, #<simm>}]
//   * STUR  <St>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STUR(v0, v1 interface{}) *Instruction {
    p := self.alloc("STUR", 2, Operands { v0, v1 })
    // STUR  <Wt>, [<Xn|SP>{, #<simm>}]
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(2, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    // STUR  <Xt>, [<Xn|SP>{, #<simm>}]
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(3, 0, 0, sa_simm, sa_xn_sp, sa_xt))
    }
    // STUR  <Bt>, [<Xn|SP>{, #<simm>}]
    if isBr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_bt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 1, 0, sa_simm, sa_xn_sp, sa_bt))
    }
    // STUR  <Dt>, [<Xn|SP>{, #<simm>}]
    if isDr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_dt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(3, 1, 0, sa_simm, sa_xn_sp, sa_dt))
    }
    // STUR  <Ht>, [<Xn|SP>{, #<simm>}]
    if isHr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_ht := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(1, 1, 0, sa_simm, sa_xn_sp, sa_ht))
    }
    // STUR  <Qt>, [<Xn|SP>{, #<simm>}]
    if isQr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_qt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 1, 2, sa_simm, sa_xn_sp, sa_qt))
    }
    // STUR  <St>, [<Xn|SP>{, #<simm>}]
    if isSr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_st := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(2, 1, 0, sa_simm, sa_xn_sp, sa_st))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STUR")
}

// STURB instruction have one single form:
//
//   * STURB  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STURB(v0, v1 interface{}) *Instruction {
    p := self.alloc("STURB", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(0, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STURB")
}

// STURH instruction have one single form:
//
//   * STURH  <Wt>, [<Xn|SP>{, #<simm>}]
//
func (self *Program) STURH(v0, v1 interface{}) *Instruction {
    p := self.alloc("STURH", 2, Operands { v0, v1 })
    if isWr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_wt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        return p.setins(ldst_unscaled(1, 0, 0, sa_simm, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STURH")
}

// STXP instruction have 2 forms:
//
//   * STXP  <Ws>, <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
//   * STXP  <Ws>, <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
//
func (self *Program) STXP(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("STXP", 4, Operands { v0, v1, v2, v3 })
    // STXP  <Ws>, <Wt1>, <Wt2>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isWr(v2) &&
       isMem(v3) &&
       isXrOrSP(mbase(v3)) &&
       midx(v3) == nil &&
       (moffs(v3) == 0 || moffs(v3) == 0) &&
       mext(v3) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt1 := uint32(v1.(asm.Register).ID())
        sa_wt2 := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v3).ID())
        return p.setins(ldstexclp(0, 0, sa_ws, 0, sa_wt2, sa_xn_sp, sa_wt1))
    }
    // STXP  <Ws>, <Xt1>, <Xt2>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isXr(v1) &&
       isXr(v2) &&
       isMem(v3) &&
       isXrOrSP(mbase(v3)) &&
       midx(v3) == nil &&
       (moffs(v3) == 0 || moffs(v3) == 0) &&
       mext(v3) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_xt1 := uint32(v1.(asm.Register).ID())
        sa_xt2 := uint32(v2.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v3).ID())
        return p.setins(ldstexclp(1, 0, sa_ws, 0, sa_xt2, sa_xn_sp, sa_xt1))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STXP")
}

// STXR instruction have 2 forms:
//
//   * STXR  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//   * STXR  <Ws>, <Xt>, [<Xn|SP>{,#0}]
//
func (self *Program) STXR(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STXR", 3, Operands { v0, v1, v2 })
    // STXR  <Ws>, <Wt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(2, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    // STXR  <Ws>, <Xt>, [<Xn|SP>{,#0}]
    if isWr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(3, 0, sa_ws, 0, 31, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STXR")
}

// STXRB instruction have one single form:
//
//   * STXRB  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STXRB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STXRB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(0, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STXRB")
}

// STXRH instruction have one single form:
//
//   * STXRH  <Ws>, <Wt>, [<Xn|SP>{,#0}]
//
func (self *Program) STXRH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("STXRH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       (moffs(v2) == 0 || moffs(v2) == 0) &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(ldstexclr(1, 0, sa_ws, 0, 31, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STXRH")
}

// STZ2G instruction have 3 forms:
//
//   * STZ2G  <Xt|SP>, [<Xn|SP>{, #<simm>}]
//   * STZ2G  <Xt|SP>, [<Xn|SP>], #<simm>
//   * STZ2G  <Xt|SP>, [<Xn|SP>, #<simm>]!
//
func (self *Program) STZ2G(v0, v1 interface{}) *Instruction {
    p := self.alloc("STZ2G", 2, Operands { v0, v1 })
    // STZ2G  <Xt|SP>, [<Xn|SP>{, #<simm>}]
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(3, sa_simm, 2, Rn, Rt))
    }
    // STZ2G  <Xt|SP>, [<Xn|SP>], #<simm>
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(3, sa_simm, 1, Rn, Rt))
    }
    // STZ2G  <Xt|SP>, [<Xn|SP>, #<simm>]!
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(3, sa_simm, 3, Rn, Rt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STZ2G")
}

// STZG instruction have 3 forms:
//
//   * STZG  <Xt|SP>, [<Xn|SP>{, #<simm>}]
//   * STZG  <Xt|SP>, [<Xn|SP>], #<simm>
//   * STZG  <Xt|SP>, [<Xn|SP>, #<simm>]!
//
func (self *Program) STZG(v0, v1 interface{}) *Instruction {
    p := self.alloc("STZG", 2, Operands { v0, v1 })
    // STZG  <Xt|SP>, [<Xn|SP>{, #<simm>}]
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == nil {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(1, sa_simm, 2, Rn, Rt))
    }
    // STZG  <Xt|SP>, [<Xn|SP>], #<simm>
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PostIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(1, sa_simm, 1, Rn, Rt))
    }
    // STZG  <Xt|SP>, [<Xn|SP>, #<simm>]!
    if isXrOrSP(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && mext(v1) == PreIndex {
        sa_xt_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        sa_simm := uint32(moffs(v1))
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt_sp
        return p.setins(ldsttags(1, sa_simm, 3, Rn, Rt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for STZG")
}

// STZGM instruction have one single form:
//
//   * STZGM  <Xt>, [<Xn|SP>]
//
func (self *Program) STZGM(v0, v1 interface{}) *Instruction {
    p := self.alloc("STZGM", 2, Operands { v0, v1 })
    if isXr(v0) && isMem(v1) && isXrOrSP(mbase(v1)) && midx(v1) == nil && moffs(v1) == 0 && mext(v1) == nil {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v1).ID())
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rt := uint32(0b00000)
        Rt |= sa_xt
        return p.setins(ldsttags(0, 0, 0, Rn, Rt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for STZGM")
}

// SUB instruction have 6 forms:
//
//   * SUB  <Wd|WSP>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
//   * SUB  <Wd|WSP>, <Wn|WSP>, #<imm>{, <shift>}
//   * SUB  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * SUB  <Xd|SP>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
//   * SUB  <Xd|SP>, <Xn|SP>, #<imm>{, <shift>}
//   * SUB  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) SUB(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("SUB", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("SUB", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction SUB takes 3 or 4 operands")
    }
    // SUB  <Wd|WSP>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) &&
       isWrOrWSP(v0) &&
       isWrOrWSP(v1) &&
       isWr(v2) &&
       (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_extend = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_ext(0, 1, 0, 0, sa_wm, sa_extend, sa_amount, sa_wn_wsp, sa_wd_wsp))
    }
    // SUB  <Wd|WSP>, <Wn|WSP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isWrOrWSP(v0) &&
       isWrOrWSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_wd_wsp := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(0, 1, 0, sa_shift, sa_imm, sa_wn_wsp, sa_wd_wsp))
    }
    // SUB  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(0, 1, 0, sa_shift, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // SUB  <Xd|SP>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) &&
       isXrOrSP(v0) &&
       isXrOrSP(v1) &&
       isWrOrXr(v2) &&
       (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend_1 uint32
        var sa_r [4]uint32
        var sa_r__bit_mask [4]uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        switch true {
            case isWr(v2): sa_r = [4]uint32{0b000, 0b010, 0b100, 0b110}
            case isXr(v2): sa_r = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        switch true {
            case isWr(v2): sa_r__bit_mask = [4]uint32{0b110, 0b111, 0b110, 0b111}
            case isXr(v2): sa_r__bit_mask = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        if len(vv) == 1 {
            sa_extend_1 = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        if !matchany(sa_extend_1, &sa_r__bit_mask[0], &sa_r[0], 4) {
            panic("aarch64: invalid combination of operands for SUB")
        }
        return p.setins(addsub_ext(1, 1, 0, 0, sa_m, sa_extend_1, sa_amount, sa_xn_sp, sa_xd_sp))
    }
    // SUB  <Xd|SP>, <Xn|SP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isXrOrSP(v0) &&
       isXrOrSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(1, 1, 0, sa_shift, sa_imm, sa_xn_sp, sa_xd_sp))
    }
    // SUB  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(1, 1, 0, sa_shift, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SUB")
}

// SUBG instruction have one single form:
//
//   * SUBG  <Xd|SP>, <Xn|SP>, #<uimm6>, #<uimm4>
//
func (self *Program) SUBG(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("SUBG", 4, Operands { v0, v1, v2, v3 })
    if isXrOrSP(v0) && isXrOrSP(v1) && isUimm6(v2) && isUimm4(v3) {
        sa_xd_sp := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_uimm6 := asUimm6(v2)
        sa_uimm4 := asUimm4(v3)
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd_sp
        return p.setins(addsub_immtags(1, 1, 0, sa_uimm6, 0, sa_uimm4, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SUBG")
}

// SUBP instruction have one single form:
//
//   * SUBP  <Xd>, <Xn|SP>, <Xm|SP>
//
func (self *Program) SUBP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SUBP", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXrOrSP(v1) && isXrOrSP(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_xm_sp := uint32(v2.(asm.Register).ID())
        Rm := uint32(0b00000)
        Rm |= sa_xm_sp
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd
        return p.setins(dp_2src(1, 0, Rm, 0, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SUBP")
}

// SUBPS instruction have one single form:
//
//   * SUBPS  <Xd>, <Xn|SP>, <Xm|SP>
//
func (self *Program) SUBPS(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SUBPS", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXrOrSP(v1) && isXrOrSP(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_xm_sp := uint32(v2.(asm.Register).ID())
        Rm := uint32(0b00000)
        Rm |= sa_xm_sp
        Rn := uint32(0b00000)
        Rn |= sa_xn_sp
        Rd := uint32(0b00000)
        Rd |= sa_xd
        return p.setins(dp_2src(1, 1, Rm, 0, Rn, Rd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SUBPS")
}

// SUBS instruction have 6 forms:
//
//   * SUBS  <Wd>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
//   * SUBS  <Wd>, <Wn|WSP>, #<imm>{, <shift>}
//   * SUBS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
//   * SUBS  <Xd>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
//   * SUBS  <Xd>, <Xn|SP>, #<imm>{, <shift>}
//   * SUBS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
//
func (self *Program) SUBS(v0, v1, v2 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("SUBS", 3, Operands { v0, v1, v2 })
        case 1  : p = self.alloc("SUBS", 4, Operands { v0, v1, v2, vv[0] })
        default : panic("instruction SUBS takes 3 or 4 operands")
    }
    // SUBS  <Wd>, <Wn|WSP>, <Wm>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWrOrWSP(v1) && isWr(v2) && (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_extend = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_ext(0, 1, 1, 0, sa_wm, sa_extend, sa_amount, sa_wn_wsp, sa_wd))
    }
    // SUBS  <Wd>, <Wn|WSP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isWr(v0) &&
       isWrOrWSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn_wsp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(0, 1, 1, sa_shift, sa_imm, sa_wn_wsp, sa_wd))
    }
    // SUBS  <Wd>, <Wn>, <Wm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isWr(v0) && isWr(v1) && isWr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount uint32
        var sa_shift uint32
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(0, 1, 1, sa_shift, sa_wm, sa_amount, sa_wn, sa_wd))
    }
    // SUBS  <Xd>, <Xn|SP>, <R><m>{, <extend> {#<amount>}}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXrOrSP(v1) && isWrOrXr(v2) && (len(vv) == 0 || isExtend(vv[0])) {
        var sa_amount uint32
        var sa_extend_1 uint32
        var sa_r [4]uint32
        var sa_r__bit_mask [4]uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_m := uint32(v2.(asm.Register).ID())
        switch true {
            case isWr(v2): sa_r = [4]uint32{0b000, 0b010, 0b100, 0b110}
            case isXr(v2): sa_r = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        switch true {
            case isWr(v2): sa_r__bit_mask = [4]uint32{0b110, 0b111, 0b110, 0b111}
            case isXr(v2): sa_r__bit_mask = [4]uint32{0b011}
            default: panic("aarch64: unreachable")
        }
        if len(vv) == 1 {
            sa_extend_1 = uint32(vv[0].(Extension).Extension())
            sa_amount = uint32(vv[0].(Modifier).Amount())
        }
        if !matchany(sa_extend_1, &sa_r__bit_mask[0], &sa_r[0], 4) {
            panic("aarch64: invalid combination of operands for SUBS")
        }
        return p.setins(addsub_ext(1, 1, 1, 0, sa_m, sa_extend_1, sa_amount, sa_xn_sp, sa_xd))
    }
    // SUBS  <Xd>, <Xn|SP>, #<imm>{, <shift>}
    if (len(vv) == 0 || len(vv) == 1) &&
       isXr(v0) &&
       isXrOrSP(v1) &&
       isImm12(v2) &&
       (len(vv) == 0 || isShift(vv[0]) && modn(vv[0]) == 0) {
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn_sp := uint32(v1.(asm.Register).ID())
        sa_imm := asImm12(v2)
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
        }
        return p.setins(addsub_imm(1, 1, 1, sa_shift, sa_imm, sa_xn_sp, sa_xd))
    }
    // SUBS  <Xd>, <Xn>, <Xm>{, <shift> #<amount>}
    if (len(vv) == 0 || len(vv) == 1) && isXr(v0) && isXr(v1) && isXr(v2) && (len(vv) == 0 || isShift(vv[0])) {
        var sa_amount_1 uint32
        var sa_shift uint32
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        if len(vv) == 1 {
            sa_shift = uint32(vv[0].(ShiftType).ShiftType())
            sa_amount_1 = uint32(vv[0].(Modifier).Amount())
        }
        return p.setins(addsub_shift(1, 1, 1, sa_shift, sa_xm, sa_amount_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SUBS")
}

// SVC instruction have one single form:
//
//   * SVC  #<imm>
//
func (self *Program) SVC(v0 interface{}) *Instruction {
    p := self.alloc("SVC", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(0, sa_imm, 0, 1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SVC")
}

// SWP instruction have 2 forms:
//
//   * SWP  <Ws>, <Wt>, [<Xn|SP>]
//   * SWP  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) SWP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWP", 3, Operands { v0, v1, v2 })
    // SWP  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 0, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    // SWP  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 0, sa_xs, 1, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SWP")
}

// SWPA instruction have 2 forms:
//
//   * SWPA  <Ws>, <Wt>, [<Xn|SP>]
//   * SWPA  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) SWPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPA", 3, Operands { v0, v1, v2 })
    // SWPA  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 0, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    // SWPA  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 0, sa_xs, 1, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SWPA")
}

// SWPAB instruction have one single form:
//
//   * SWPAB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPAB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPAB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 0, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPAB")
}

// SWPAH instruction have one single form:
//
//   * SWPAH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPAH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPAH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 0, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPAH")
}

// SWPAL instruction have 2 forms:
//
//   * SWPAL  <Ws>, <Wt>, [<Xn|SP>]
//   * SWPAL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) SWPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPAL", 3, Operands { v0, v1, v2 })
    // SWPAL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 1, 1, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    // SWPAL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 1, 1, sa_xs, 1, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SWPAL")
}

// SWPALB instruction have one single form:
//
//   * SWPALB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPALB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPALB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 1, 1, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPALB")
}

// SWPALH instruction have one single form:
//
//   * SWPALH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPALH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPALH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 1, 1, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPALH")
}

// SWPB instruction have one single form:
//
//   * SWPB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 0, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPB")
}

// SWPH instruction have one single form:
//
//   * SWPH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 0, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPH")
}

// SWPL instruction have 2 forms:
//
//   * SWPL  <Ws>, <Wt>, [<Xn|SP>]
//   * SWPL  <Xs>, <Xt>, [<Xn|SP>]
//
func (self *Program) SWPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPL", 3, Operands { v0, v1, v2 })
    // SWPL  <Ws>, <Wt>, [<Xn|SP>]
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(2, 0, 0, 1, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    // SWPL  <Xs>, <Xt>, [<Xn|SP>]
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xs := uint32(v0.(asm.Register).ID())
        sa_xt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(3, 0, 0, 1, sa_xs, 1, 0, sa_xn_sp, sa_xt))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for SWPL")
}

// SWPLB instruction have one single form:
//
//   * SWPLB  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPLB(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPLB", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(0, 0, 0, 1, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPLB")
}

// SWPLH instruction have one single form:
//
//   * SWPLH  <Ws>, <Wt>, [<Xn|SP>]
//
func (self *Program) SWPLH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPLH", 3, Operands { v0, v1, v2 })
    if isWr(v0) &&
       isWr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_ws := uint32(v0.(asm.Register).ID())
        sa_wt := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop(1, 0, 0, 1, sa_ws, 1, 0, sa_xn_sp, sa_wt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPLH")
}

// SWPP instruction have one single form:
//
//   * SWPP  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) SWPP(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPP", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 0, sa_xt2, 1, 0, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPP")
}

// SWPPA instruction have one single form:
//
//   * SWPPA  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) SWPPA(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPPA", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 0, sa_xt2, 1, 0, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPPA")
}

// SWPPAL instruction have one single form:
//
//   * SWPPAL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) SWPPAL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPPAL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 1, 1, sa_xt2, 1, 0, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPPAL")
}

// SWPPL instruction have one single form:
//
//   * SWPPL  <Xt1>, <Xt2>, [<Xn|SP>]
//
func (self *Program) SWPPL(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("SWPPL", 3, Operands { v0, v1, v2 })
    if isXr(v0) &&
       isXr(v1) &&
       isMem(v2) &&
       isXrOrSP(mbase(v2)) &&
       midx(v2) == nil &&
       moffs(v2) == 0 &&
       mext(v2) == nil {
        sa_xt1 := uint32(v0.(asm.Register).ID())
        sa_xt2 := uint32(v1.(asm.Register).ID())
        sa_xn_sp := uint32(mbase(v2).ID())
        return p.setins(memop_128(0, 0, 1, sa_xt2, 1, 0, sa_xn_sp, sa_xt1))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SWPPL")
}

// SYS instruction have one single form:
//
//   * SYS  #<op1>, <Cn>, <Cm>, #<op2>{, <Xt>}
//
func (self *Program) SYS(v0, v1, v2, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("SYS", 4, Operands { v0, v1, v2, v3 })
        case 1  : p = self.alloc("SYS", 5, Operands { v0, v1, v2, v3, vv[0] })
        default : panic("instruction SYS takes 4 or 5 operands")
    }
    if (len(vv) == 0 || len(vv) == 1) &&
       isUimm3(v0) &&
       isUimm4(v1) &&
       isUimm4(v2) &&
       isUimm3(v3) &&
       (len(vv) == 0 || isXr(vv[0])) {
        var sa_xt uint32
        sa_op1 := asUimm3(v0)
        sa_cn := asUimm4(v1)
        sa_cm := asUimm4(v2)
        sa_op2 := asUimm3(v3)
        if len(vv) == 1 {
            sa_xt = uint32(vv[0].(asm.Register).ID())
        }
        return p.setins(systeminstrs(0, sa_op1, sa_cn, sa_cm, sa_op2, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SYS")
}

// SYSL instruction have one single form:
//
//   * SYSL  <Xt>, #<op1>, <Cn>, <Cm>, #<op2>
//
func (self *Program) SYSL(v0, v1, v2, v3, v4 interface{}) *Instruction {
    p := self.alloc("SYSL", 5, Operands { v0, v1, v2, v3, v4 })
    if isXr(v0) && isUimm3(v1) && isUimm4(v2) && isUimm4(v3) && isUimm3(v4) {
        sa_xt := uint32(v0.(asm.Register).ID())
        sa_op1 := asUimm3(v1)
        sa_cn := asUimm4(v2)
        sa_cm := asUimm4(v3)
        sa_op2 := asUimm3(v4)
        return p.setins(systeminstrs(1, sa_op1, sa_cn, sa_cm, sa_op2, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SYSL")
}

// SYSP instruction have one single form:
//
//   * SYSP  #<op1>, <Cn>, <Cm>, #<op2>{, <Xt1>, <Xt2>}
//
func (self *Program) SYSP(v0, v1, v2, v3 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("SYSP", 4, Operands { v0, v1, v2, v3 })
        case 2  : p = self.alloc("SYSP", 6, Operands { v0, v1, v2, v3, vv[0], vv[1] })
        default : panic("instruction SYSP takes 4 or 6 operands")
    }
    if (len(vv) == 0 || len(vv) == 2) &&
       isUimm3(v0) &&
       isUimm4(v1) &&
       isUimm4(v2) &&
       isUimm3(v3) &&
       (len(vv) == 0 || isXr(vv[0])) &&
       (len(vv) == 0 || isXr(vv[1])) {
        var sa_xt1 uint32
        var sa_xt2 uint32
        sa_op1 := asUimm3(v0)
        sa_cn := asUimm4(v1)
        sa_cm := asUimm4(v2)
        sa_op2 := asUimm3(v3)
        if len(vv) == 2 {
            sa_xt1 = uint32(vv[0].(asm.Register).ID())
            sa_xt2 = uint32(vv[1].(asm.Register).ID())
        }
        if sa_xt1 != sa_xt2 {
            panic("aarch64: invalid combination of operands for SYSP")
        }
        return p.setins(syspairinstrs(0, sa_op1, sa_cn, sa_cm, sa_op2, sa_xt2))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for SYSP")
}

// TBNZ instruction have one single form:
//
//   * TBNZ  <R><t>, #<imm>, <label>
//
func (self *Program) TBNZ(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("TBNZ", 3, Operands { v0, v1, v2 })
    if isWrOrXr(v0) && isUimm6(v1) && isLabel(v2) {
        var sa_r uint32
        sa_t := uint32(v0.(asm.Register).ID())
        switch true {
            case isWr(v0): sa_r = 0b0
            case isXr(v0): sa_r = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm := asUimm6(v1)
        sa_label := v2.(*asm.Label)
        if sa_imm & 0x1 != sa_r {
            panic("aarch64: invalid combination of operands for TBNZ")
        }
        return p.setenc(func(pc uintptr) uint32 {
            return testbranch(
                sa_imm & 0x1,
                1,
                (sa_imm >> 1) & 0x1f,
                uint32(sa_label.RelativeTo(pc)),
                sa_t,
            )
        })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TBNZ")
}

// TBZ instruction have one single form:
//
//   * TBZ  <R><t>, #<imm>, <label>
//
func (self *Program) TBZ(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("TBZ", 3, Operands { v0, v1, v2 })
    if isWrOrXr(v0) && isUimm6(v1) && isLabel(v2) {
        var sa_r uint32
        sa_t := uint32(v0.(asm.Register).ID())
        switch true {
            case isWr(v0): sa_r = 0b0
            case isXr(v0): sa_r = 0b1
            default: panic("aarch64: unreachable")
        }
        sa_imm := asUimm6(v1)
        sa_label := v2.(*asm.Label)
        if sa_imm & 0x1 != sa_r {
            panic("aarch64: invalid combination of operands for TBZ")
        }
        return p.setenc(func(pc uintptr) uint32 {
            return testbranch(
                sa_imm & 0x1,
                0,
                (sa_imm >> 1) & 0x1f,
                uint32(sa_label.RelativeTo(pc)),
                sa_t,
            )
        })
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TBZ")
}

// TCANCEL instruction have one single form:
//
//   * TCANCEL  #<imm>
//
func (self *Program) TCANCEL(v0 interface{}) *Instruction {
    p := self.alloc("TCANCEL", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(exception(3, sa_imm, 0, 0))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TCANCEL")
}

// TCOMMIT instruction have one single form:
//
//   * TCOMMIT
//
func (self *Program) TCOMMIT() *Instruction {
    p := self.alloc("TCOMMIT", 0, Operands {})
    return p.setins(barriers(0, 3, 31))
}

// TSB instruction have one single form:
//
//   * TSB CSYNC
//
func (self *Program) TSB(v0 interface{}) *Instruction {
    p := self.alloc("TSB", 1, Operands { v0 })
    if v0 == CSYNC {
        return p.setins(hints(2, 2))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TSB")
}

// TSTART instruction have one single form:
//
//   * TSTART  <Xt>
//
func (self *Program) TSTART(v0 interface{}) *Instruction {
    p := self.alloc("TSTART", 1, Operands { v0 })
    if isXr(v0) {
        sa_xt := uint32(v0.(asm.Register).ID())
        return p.setins(systemresult(3, 3, 0, 3, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TSTART")
}

// TTEST instruction have one single form:
//
//   * TTEST  <Xt>
//
func (self *Program) TTEST(v0 interface{}) *Instruction {
    p := self.alloc("TTEST", 1, Operands { v0 })
    if isXr(v0) {
        sa_xt := uint32(v0.(asm.Register).ID())
        return p.setins(systemresult(3, 3, 1, 3, sa_xt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for TTEST")
}

// UBFM instruction have 2 forms:
//
//   * UBFM  <Wd>, <Wn>, #<immr>, #<imms>
//   * UBFM  <Xd>, <Xn>, #<immr>, #<imms>
//
func (self *Program) UBFM(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("UBFM", 4, Operands { v0, v1, v2, v3 })
    // UBFM  <Wd>, <Wn>, #<immr>, #<imms>
    if isWr(v0) && isWr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_immr := asUimm6(v2)
        sa_imms := asUimm6(v3)
        return p.setins(bitfield(0, 2, 0, sa_immr, sa_imms, sa_wn, sa_wd))
    }
    // UBFM  <Xd>, <Xn>, #<immr>, #<imms>
    if isXr(v0) && isXr(v1) && isUimm6(v2) && isUimm6(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_immr_1 := asUimm6(v2)
        sa_imms_1 := asUimm6(v3)
        return p.setins(bitfield(1, 2, 1, sa_immr_1, sa_imms_1, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UBFM")
}

// UCVTF instruction have 12 forms:
//
//   * UCVTF  <Dd>, <Wn>, #<fbits>
//   * UCVTF  <Dd>, <Wn>
//   * UCVTF  <Dd>, <Xn>, #<fbits>
//   * UCVTF  <Dd>, <Xn>
//   * UCVTF  <Hd>, <Wn>, #<fbits>
//   * UCVTF  <Hd>, <Wn>
//   * UCVTF  <Hd>, <Xn>, #<fbits>
//   * UCVTF  <Hd>, <Xn>
//   * UCVTF  <Sd>, <Wn>, #<fbits>
//   * UCVTF  <Sd>, <Wn>
//   * UCVTF  <Sd>, <Xn>, #<fbits>
//   * UCVTF  <Sd>, <Xn>
//
func (self *Program) UCVTF(v0, v1 interface{}, vv ...interface{}) *Instruction {
    var p *Instruction
    switch len(vv) {
        case 0  : p = self.alloc("UCVTF", 2, Operands { v0, v1 })
        case 1  : p = self.alloc("UCVTF", 3, Operands { v0, v1, vv[0] })
        default : panic("instruction UCVTF takes 2 or 3 operands")
    }
    // UCVTF  <Dd>, <Wn>, #<fbits>
    if len(vv) == 1 && isDr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 1, 0, 3, sa_fbits, sa_wn, sa_dd))
    }
    // UCVTF  <Dd>, <Wn>
    if isDr(v0) && isWr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 1, 0, 3, sa_wn, sa_dd))
    }
    // UCVTF  <Dd>, <Xn>, #<fbits>
    if len(vv) == 1 && isDr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 1, 0, 3, sa_fbits_1, sa_xn, sa_dd))
    }
    // UCVTF  <Dd>, <Xn>
    if isDr(v0) && isXr(v1) {
        sa_dd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 1, 0, 3, sa_xn, sa_dd))
    }
    // UCVTF  <Hd>, <Wn>, #<fbits>
    if len(vv) == 1 && isHr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 3, 0, 3, sa_fbits, sa_wn, sa_hd))
    }
    // UCVTF  <Hd>, <Wn>
    if isHr(v0) && isWr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 3, 0, 3, sa_wn, sa_hd))
    }
    // UCVTF  <Hd>, <Xn>, #<fbits>
    if len(vv) == 1 && isHr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 3, 0, 3, sa_fbits_1, sa_xn, sa_hd))
    }
    // UCVTF  <Hd>, <Xn>
    if isHr(v0) && isXr(v1) {
        sa_hd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 3, 0, 3, sa_xn, sa_hd))
    }
    // UCVTF  <Sd>, <Wn>, #<fbits>
    if len(vv) == 1 && isSr(v0) && isWr(v1) && isFpBits(vv[0]) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_fbits := asFpScale(vv[0])
        return p.setins(float2fix(0, 0, 0, 0, 3, sa_fbits, sa_wn, sa_sd))
    }
    // UCVTF  <Sd>, <Wn>
    if isSr(v0) && isWr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(0, 0, 0, 0, 3, sa_wn, sa_sd))
    }
    // UCVTF  <Sd>, <Xn>, #<fbits>
    if len(vv) == 1 && isSr(v0) && isXr(v1) && isFpBits(vv[0]) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_fbits_1 := asFpScale(vv[0])
        return p.setins(float2fix(1, 0, 0, 0, 3, sa_fbits_1, sa_xn, sa_sd))
    }
    // UCVTF  <Sd>, <Xn>
    if isSr(v0) && isXr(v1) {
        sa_sd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        return p.setins(float2int(1, 0, 0, 0, 3, sa_xn, sa_sd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UCVTF")
}

// UDF instruction have one single form:
//
//   * UDF  #<imm>
//
func (self *Program) UDF(v0 interface{}) *Instruction {
    p := self.alloc("UDF", 1, Operands { v0 })
    if isUimm16(v0) {
        sa_imm := asUimm16(v0)
        return p.setins(perm_undef(sa_imm))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UDF")
}

// UDIV instruction have 2 forms:
//
//   * UDIV  <Wd>, <Wn>, <Wm>
//   * UDIV  <Xd>, <Xn>, <Xm>
//
func (self *Program) UDIV(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UDIV", 3, Operands { v0, v1, v2 })
    // UDIV  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 2, sa_wn, sa_wd))
    }
    // UDIV  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 2, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UDIV")
}

// UMADDL instruction have one single form:
//
//   * UMADDL  <Xd>, <Wn>, <Wm>, <Xa>
//
func (self *Program) UMADDL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("UMADDL", 4, Operands { v0, v1, v2, v3 })
    if isXr(v0) && isWr(v1) && isWr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 5, sa_wm, 0, sa_xa, sa_wn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UMADDL")
}

// UMAX instruction have 4 forms:
//
//   * UMAX  <Wd>, <Wn>, #<uimm>
//   * UMAX  <Wd>, <Wn>, <Wm>
//   * UMAX  <Xd>, <Xn>, #<uimm>
//   * UMAX  <Xd>, <Xn>, <Xm>
//
func (self *Program) UMAX(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMAX", 3, Operands { v0, v1, v2 })
    // UMAX  <Wd>, <Wn>, #<uimm>
    if isWr(v0) && isWr(v1) && isFpImm8(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_uimm := asFpImm8(v2)
        return p.setins(minmax_imm(0, 0, 0, 1, sa_uimm, sa_wn, sa_wd))
    }
    // UMAX  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 25, sa_wn, sa_wd))
    }
    // UMAX  <Xd>, <Xn>, #<uimm>
    if isXr(v0) && isXr(v1) && isFpImm8(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_uimm := asFpImm8(v2)
        return p.setins(minmax_imm(1, 0, 0, 1, sa_uimm, sa_xn, sa_xd))
    }
    // UMAX  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 25, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UMAX")
}

// UMIN instruction have 4 forms:
//
//   * UMIN  <Wd>, <Wn>, #<uimm>
//   * UMIN  <Wd>, <Wn>, <Wm>
//   * UMIN  <Xd>, <Xn>, #<uimm>
//   * UMIN  <Xd>, <Xn>, <Xm>
//
func (self *Program) UMIN(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMIN", 3, Operands { v0, v1, v2 })
    // UMIN  <Wd>, <Wn>, #<uimm>
    if isWr(v0) && isWr(v1) && isFpImm8(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_uimm := asFpImm8(v2)
        return p.setins(minmax_imm(0, 0, 0, 3, sa_uimm, sa_wn, sa_wd))
    }
    // UMIN  <Wd>, <Wn>, <Wm>
    if isWr(v0) && isWr(v1) && isWr(v2) {
        sa_wd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(0, 0, sa_wm, 27, sa_wn, sa_wd))
    }
    // UMIN  <Xd>, <Xn>, #<uimm>
    if isXr(v0) && isXr(v1) && isFpImm8(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_uimm := asFpImm8(v2)
        return p.setins(minmax_imm(1, 0, 0, 3, sa_uimm, sa_xn, sa_xd))
    }
    // UMIN  <Xd>, <Xn>, <Xm>
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_2src(1, 0, sa_xm, 27, sa_xn, sa_xd))
    }
    // none of above
    p.Free()
    panic("aarch64: invalid combination of operands for UMIN")
}

// UMSUBL instruction have one single form:
//
//   * UMSUBL  <Xd>, <Wn>, <Wm>, <Xa>
//
func (self *Program) UMSUBL(v0, v1, v2, v3 interface{}) *Instruction {
    p := self.alloc("UMSUBL", 4, Operands { v0, v1, v2, v3 })
    if isXr(v0) && isWr(v1) && isWr(v2) && isXr(v3) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_wn := uint32(v1.(asm.Register).ID())
        sa_wm := uint32(v2.(asm.Register).ID())
        sa_xa := uint32(v3.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 5, sa_wm, 1, sa_xa, sa_wn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UMSUBL")
}

// UMULH instruction have one single form:
//
//   * UMULH  <Xd>, <Xn>, <Xm>
//
func (self *Program) UMULH(v0, v1, v2 interface{}) *Instruction {
    p := self.alloc("UMULH", 3, Operands { v0, v1, v2 })
    if isXr(v0) && isXr(v1) && isXr(v2) {
        sa_xd := uint32(v0.(asm.Register).ID())
        sa_xn := uint32(v1.(asm.Register).ID())
        sa_xm := uint32(v2.(asm.Register).ID())
        return p.setins(dp_3src(1, 0, 6, sa_xm, 0, 31, sa_xn, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for UMULH")
}

// WFE instruction have one single form:
//
//   * WFE
//
func (self *Program) WFE() *Instruction {
    p := self.alloc("WFE", 0, Operands {})
    return p.setins(hints(0, 2))
}

// WFET instruction have one single form:
//
//   * WFET  <Xt>
//
func (self *Program) WFET(v0 interface{}) *Instruction {
    p := self.alloc("WFET", 1, Operands { v0 })
    if isXr(v0) {
        sa_xt := uint32(v0.(asm.Register).ID())
        Rt := uint32(0b00000)
        Rt |= sa_xt
        return p.setins(systeminstrswithreg(0, 0, Rt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for WFET")
}

// WFI instruction have one single form:
//
//   * WFI
//
func (self *Program) WFI() *Instruction {
    p := self.alloc("WFI", 0, Operands {})
    return p.setins(hints(0, 3))
}

// WFIT instruction have one single form:
//
//   * WFIT  <Xt>
//
func (self *Program) WFIT(v0 interface{}) *Instruction {
    p := self.alloc("WFIT", 1, Operands { v0 })
    if isXr(v0) {
        sa_xt := uint32(v0.(asm.Register).ID())
        Rt := uint32(0b00000)
        Rt |= sa_xt
        return p.setins(systeminstrswithreg(0, 1, Rt))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for WFIT")
}

// XAFLAG instruction have one single form:
//
//   * XAFLAG
//
func (self *Program) XAFLAG() *Instruction {
    p := self.alloc("XAFLAG", 0, Operands {})
    return p.setins(pstate(0, 0, 1, 31))
}

// XPACD instruction have one single form:
//
//   * XPACD  <Xd>
//
func (self *Program) XPACD(v0 interface{}) *Instruction {
    p := self.alloc("XPACD", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 17, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for XPACD")
}

// XPACI instruction have one single form:
//
//   * XPACI  <Xd>
//
func (self *Program) XPACI(v0 interface{}) *Instruction {
    p := self.alloc("XPACI", 1, Operands { v0 })
    if isXr(v0) {
        sa_xd := uint32(v0.(asm.Register).ID())
        return p.setins(dp_1src(1, 0, 1, 16, 31, sa_xd))
    }
    p.Free()
    panic("aarch64: invalid combination of operands for XPACI")
}

// XPACLRI instruction have one single form:
//
//   * XPACLRI
//
func (self *Program) XPACLRI() *Instruction {
    p := self.alloc("XPACLRI", 0, Operands {})
    return p.setins(hints(0, 7))
}

// YIELD instruction have one single form:
//
//   * YIELD
//
func (self *Program) YIELD() *Instruction {
    p := self.alloc("YIELD", 0, Operands {})
    return p.setins(hints(0, 1))
}
